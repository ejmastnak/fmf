\documentclass[11pt, a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage{mwe}
\usepackage[margin=3.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm} % for bold vectors in math mode
\usepackage{physics} % many useful physics commands
\usepackage[separate-uncertainty=true]{siunitx} % for scientific notation and units

\usepackage{graphicx}
\graphicspath{{"../figures/"}}
\usepackage[section]{placeins} % to keep figures in their sections
\usepackage[export]{adjustbox} % for subcaptionbox figures
\usepackage{subcaption}  % for figures with subcaptions

\usepackage[most, minted]{tcolorbox} % for displaying code

\usepackage{xcolor}  % to color hyperref links
\usepackage[colorlinks = true, allcolors=blue]{hyperref}

\setlength{\parindent}{0pt} % to stop indenting new paragraphs
\newcommand{\diff}{\mathop{}\!\mathrm{d}} % differential
\newcommand{\eqtext}[1]{\qquad \text{#1} \qquad}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\tmat}[1]{\tilde{\mathbf{#1}}}
\newcommand{\A}{\operatorname{A}}  % for elements of matrix A
\newcommand{\tA}{\tilde{\operatorname{A}}} % for elements of matrix \tidle{A}
\newcommand{\B}{\operatorname{B}}  % for beta function
\renewcommand{\t}[1]{\tilde{#1}}


\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\uvec}[1]{\hat{\vec{#1}}}

\renewcommand{\div}{\nabla \cdot}
\renewcommand{\curl}{\nabla \cross}
\renewcommand{\grad}{\nabla}
\renewcommand{\laplacian}{\nabla^{2}}

\newcommand{\Pois}{Poiseuille\xspace}  % short for Poiseuille

\newtcblisting{python}{%
	listing engine=minted,
	minted language=python,
	listing only,
	breakable,
	enhanced,
	minted options = {
		linenos, 
		breaklines=true, 
		tabsize=2,
		fontsize=\footnotesize, 
		numbersep=2mm
	},
	overlay={%
		\begin{tcbclipinterior}
			\fill[gray!25] (frame.south west) rectangle ([xshift=4mm]frame.north west);
		\end{tcbclipinterior}
	}   
}


\begin{document}
\title{The Galerkin Method for Partial Differential Equations}
\author{Elijan Jakob Mastnak\\[1mm]\small{Student ID: 28181157}}
\date{December 2020}
\maketitle

\tableofcontents

\newpage
\begin{center}
\textbf{Assignment}
\begin{enumerate}
	\item Use the Galerkin method to solve for the velocity profile of uniform, laminar fluid flow in a pipe with a semi-circular cross section using homogeneous boundary conditions.
	
	\item Use the Galerkin method to find the \Pois coefficient $ C $ (see the \hyperref[gal:s:theory]{theory section} for more detail) for the above velocity profile.
	
	\item Investigate the dependence of the method's accuracy on the size of the indices $ M $ and $ N $ used for the Galerkin method basis functions.
	
	\item \textit{Optional}: Use the Galerkin method to solve the first-order wave equation 
	\begin{equation*}
		\pdv{u}{t} - \pdv{u}{\xi} = 0
	\end{equation*}
	on the interval $ \xi \in [0, 2\pi] $ with the initial condition
	\begin{equation*}
		u(\xi, 0) = \sin\big[\pi \cos \xi \big]
	\end{equation*}
	and the periodic boundary conditions $ u(0, t) = u(2\pi, t) $.
\end{enumerate}
\end{center}

\vspace{2mm}

\rule{\textwidth}{0.2pt}

\section{Theory} \label{gal:s:theory}
\vspace{-2mm}
\textit{To jump right to the solution, see \hyperref[gal:s:solution]{Section \ref{gal:s:solution}}}.
\vspace{2mm}

\subsection{The Problem's Relevant Hydrodynamics}
Uniform, laminar flow of an incompressible fluid in a long, straight pipe under the influence of a pressure gradient $ p' $ is described by the Poisson equation
\begin{equation*}
	\laplacian v = - \frac{p'}{\eta},
\end{equation*}
where $ v $ is the component of fluid velocity along the pipe's longitudinal axis and $ \eta $ is the fluid's viscosity. Note that $ v $ depends only on the position in the pipe's cross section, which we will encode with the polar coordinates $ r $ and $ \phi $. The fluid flux in the pipe is governed by the \Pois equation
\begin{equation*}
	\Phi = \int_{S}v \diff S = C \frac{p' S^{2}}{8 \pi \eta},
\end{equation*}
where the coefficient $ C $ depends on the pipe's cross-sectional geometry. For a circular cross section $ C = 1 $, and the \Pois equation reduces to the familiar form
\begin{equation*}
	\Phi = \frac{p' S^{2}}{8 \pi \eta}.
\end{equation*}
For more complicated cross-sectional geometries $ C \neq 1 $---in this report, we determine $ C $ for a pipe with a semicircular cross section of radius $ R $.

\subsection{Transition to Dimensionless Variables}
For computational convenience, we replace the radius $ r $ and fluid velocity $ v $ with the dimensionless quantities
\begin{equation*}
	\xi \equiv \frac{r}{R} \eqtext{and} u \equiv \frac{v\eta}{p'R^{2}}.
\end{equation*}
In terms of $ \xi $ and $ u $, the problem's Poisson equation $ \laplacian v = -p'/ \eta $ and homogeneous boundary conditions simplify to
\begin{equation*}
	\laplacian u(\xi, \phi) = -1 \eqtext{and} \qquad u(1, \phi) = u(\xi, 0) = u(\xi, \pi) = 0.
\end{equation*}
Note that for a circular cross section, homogeneous boundary conditions are concisely written $ u(1, \phi) = 0 $, but for a semi-circular pipe we must explicitly specify the homogeneous behavior at $ \phi = 0 $ and $ \phi = \pi $ (along the pipe's flat edge). In terms of $ \xi $, $ \phi $, and $ u $, the semicircular pipe's \Pois coefficient $ C $ is
\begin{equation*}
	C = \frac{8\pi}{S^{2}} \int_{S}u \diff S = \frac{32}{\pi} \int_{\xi = 0}^{1}\int_{\phi=0}^{\pi} u(\xi, \phi)\xi \diff \xi \diff \phi,
\end{equation*}
where the pipe's cross-sectional area is $ S = \pi/2 $.

\subsection{The Galerkin Method}
We approximate the velocity profile $ u(\xi, \phi) $ with the ansatz
\begin{equation*}
	\t{u}(\xi, \phi) = \sum_{i=1}^{I} a_{i}\Psi_{i}(\xi, \phi),
\end{equation*}
where the basis functions\footnote{A more appropriate name might be \textit{trial functions}, since the $ \{\Psi_{i}\} $ comprise only a final set and thus do not form a complete basis of the generally infinite-dimensional solution space.} $ \Psi_{i} $ should satisfy the problem's boundary conditions---in our case the homogeneous conditions $ \Psi_{i}(1, \phi) = 0 $. The approximate solution $ \t{u} $ does not perfectly satisfy the Poisson equation---we're left with an error (or \textit{residual}) $ \epsilon $, defined by
\begin{equation*}
	\laplacian \t{u}(\xi, \phi) = -1 + \epsilon(\xi, \phi).
\end{equation*}
Our goal is to find weight coefficients $ a_{i} $ such that the basis functions $ \Psi_{i} $ minimize the residual $ \epsilon $. There are many possible approaches to minimizing the residual---the \textit{Galerkin method} takes a geometric approach. We assume the basis functions are orthogonal, with a well-defined inner product
\begin{equation*}
	\ip{\Psi_{i}}{\Psi_{k}} = \int_{V}\Psi_{i}\Psi_{k} \diff V = \delta_{ik},
\end{equation*}
and require the residual is orthogonal to the basis functions, i.e.
\begin{equation*}
	\ip{\epsilon}{\Psi_{i}} = 0 \qquad \text{for } i = 1, 2, \ldots, I.
\end{equation*}
Thus, at least in the scope of the $ I $-dimensional functional subspace spanned by the $ \{\Psi_{i} \} $, the approximate solution doesn't have any error. Next, we substitute the approximate solution $ \t{u} $ into the differential equation $ \laplacian \t{u} = -1 + \epsilon $ to get
\begin{equation*}
	\sum_{i = 1}^{I}a_{i}\laplacian \Psi_{i} = -1 + \epsilon.
\end{equation*}
We take the inner product of the equation from the right with $ \Psi_{i} $ and apply the Galerkin condition $ \ip{\epsilon}{\Psi_{i}} = 0 $ to get
\begin{equation*}
	\sum_{j = 1}^{I}a_{j} \ip{\laplacian\Psi_{j}}{\Psi_{i}} = \ip{-1}{\Psi_{i}}.
\end{equation*}
Finally, we write the above result as a system of equations for the coefficients $ a_{j} $:
\begin{equation*}
	\sum_{j=1}^{I} \A_{ij}a_{j} = b_{i}, \qquad i = 1, 2, \ldots, I,
\end{equation*}
where the matrix elements $ \A_{ij} $ are given by
\begin{equation*}
	\A_{ij} = \ip{\laplacian \Psi_{j}}{\Psi_{i}} \eqtext{and} b_{i} = \ip{-1}{\Psi_{i}}.
\end{equation*}
In terms of $ \A_{ij} $ and $ b_{i} $, the \Pois coefficient $ C $ reads
\begin{equation}
	C = - \frac{32}{\pi}\sum_{i}b_{ij}\A_{ij}^{-1}b_{j} = - \frac{32}{\pi}\sum_{i}b_{i}a_{i}. \label{gal:eq:C}
\end{equation}

\iffalse 
% Extra theory --- A is symmetric (probs overkill for the report)
Using vector calculus identities, we can show
\begin{align*}
	\A_{ij} &= (\Delta \psi_{j}, \psi_{i}) \equiv \int \Delta \psi_{j} \psi_{i} \diff V = \int_{V} \nabla \cdot \big[ \nabla \psi_{j} \psi_{i} \big] \diff V - \int_{V}\big[\nabla  \psi_{j} \cdot \nabla \psi_{i} \big] \diff V\\
	& = 0 - \int_{V}\big[\nabla  \psi_{j} \cdot \nabla \psi_{i} \big] \diff V
\end{align*}
where $ \int_{V} \nabla \cdot \big[ \nabla \psi_{j} \psi_{i} \big] \diff V = 0 $ in the last equality holds because of the homogeneous boundary conditions on $ \Psi_{i} $. We end up with
\begin{equation*}
	\A_{ij} = - \int_{V}\big[\nabla  \psi_{j} \cdot \nabla \psi_{i} \big] \diff V
\end{equation*}
Evidently, the matrix $ \mat{A} $ is symmetric, which invites the use of efficient numerical methods.
\fi

\subsection{Choosing Basis Functions}
All that remains is to choose an appropriate set of basis function $ \{\Psi_{i}\} $ suited to our problem's semicircular cross-sectional geometry. We take our inspiration from the analytic solution for $ u(\xi, \phi) $, which---assuming the axis of symmetry dividing the pipe's cross section in half occurs at $ \phi = \pi/2 $---reads
\begin{equation*}
	u(\xi, \phi) = \sum_{m=0}^{\infty}\sum_{s=1}^{\infty}c_{mn}J_{2m+1}\big(y_{(2m+1)_{s}}\xi\big)\sin \big[(2m+1)\phi\big],
\end{equation*}
where $ J_{2m+1} $ are Bessel functions of the first kind and $ y_{(2m+1)s} $ are the $ s $-th zeros of the $ (2m+1) $-th Bessel function $ J_{2m+1} $. We choose our basis functions to be
\begin{equation}
	\Psi_{mn}(\xi, \phi) \equiv \xi^{2m+1}(1-\xi)^{n} \sin\big[(2m+1)\phi\big], \label{gal:eq:basis1}
\end{equation}
where the indices take on the values $ m = 0, 1, \ldots, M $ and $ n = 1, 2, \ldots, N$. This choice of basis functions preserves exact solution's $ \phi $ dependence and replaces the radial Bessel functions with the simpler expression  $ \xi^{2m+1}(1-\xi)^{n} $. The functions also satisfy the problem's homogeneous boundary conditions $ \Psi_{i}(1, \phi) = 0 $.

\vspace{2mm}
\textbf{Caution:} The basis functions depend on two indices---$ m $ and $ n $. The index $ i $ in the expression $ \{\Psi_{i}\} $ is really a double index---it counts both $ m $ and $ n $. Establishing a well-defined conversion scheme between the single and double-index representations of the basis functions is key to solving this problem.


\section{Solution}  \label{gal:s:solution}

\subsection{Implementing The Galerkin Method}
%And introducing matrix equations.
The solution for both the velocity profile $ u(x, t) $ and the \Pois coefficient $ C $ rests on solving the system of equations
\begin{equation*}
	\sum_{j=1}^{I}A_{ij}a_{j} = b_{i}, \qquad i = 1, 2, \ldots I
\end{equation*}
for the weight coefficients $ a_{j} $. To solve the system of equations, we must first implement concrete expressions for the matrix elements and vector components
\begin{equation*}
	\A_{ij} = \ip{\laplacian \Psi_{j}}{\Psi_{i}} \eqtext{and} b_{i} = \ip{-1}{\Psi_{i}}.
\end{equation*}
For our choice of basis functions (Eq. \ref{gal:eq:basis1}), the inner product on the domain $ \xi \in [0, 1] $, $ \phi \in [0, \pi] $ can be written in terms of the beta function $ \B(x, y) $ as
\begin{equation*}
	\ip{\Psi_{m'n'}}{\Psi_{mn}} \equiv \int_{0}^{1}\int_{0}^{\pi} \Psi_{m'n'}\Psi_{mn}\xi \diff \xi \diff \phi = \frac{\pi}{2} \B(4 + 4m, 1 + n + n')\delta_{m'm}.
\end{equation*}
Note that the functions $ \Psi_{mn} $ are orthogonal with respect to the index $ m $. Again using the beta function, the matrix elements $ \A_{ij} $ (or, with double indices, $ \A_{(m'n')(mn)} $) are
\begin{equation}
	\A_{(m'n')(mn)} \equiv \ip{\laplacian \Psi_{mn}}{\Psi_{m'n'}} = -\frac{\pi}{2}\frac{nn'(3+4m)}{2+4m_n+n'}\B(n+n'-1,3+4m)\delta_{m'm}. \label{gal:eq:Aij}
\end{equation}
Finally, the vector components $ b_{i} \equiv b_{mn} $ are
\begin{equation*}
	b_{i} = \ip{-1}{\Psi_{m'n'}} = -\frac{2}{2m'+1}\B(2m'+3,n'+1).
\end{equation*}
The nonzero matrix elements $ \A_{ij} $ where $ m = m' $ can be found with the Python code
\begin{python}
import numpy as np
from scipy.special import beta
def get_Aij(m, n1, n2):
    """ Returns the nonzero (m' = m) matrix element A_{ij} = A_{(mn')(mn)}"""
    return -0.5*np.pi*beta(n1+n2-1, 3+4*m)*(n1*n2*(3+4*m))/(2+4*m+n1+n2)
\end{python}
Meanwhile, the vector components $ b_{i} $ are found in Python with the following code:
\begin{python}
def get_bi(m, n):
    """ Returns the vector components b_{i} = b_{mn} """
    return -2*beta(2*m + 3, n + 1)/(2*m + 1)
\end{python}


\subsection{Constructing Matrix and Vector Quantities}
Before going further, I rewrote the system of equations
\begin{equation*}
	\sum_{j=1}^{I}A_{ij}a_{j} = b_{i}, \qquad i = 1, 2, \ldots I
\end{equation*}
as the matrix equation $ \mat{A} \vec{a} = \vec{b} $, where, in terms of a single index, the vector $ \vec{a} $ is
\begin{equation*}
	\vec{a} = \big[a_{1}, a_{2}, \ldots, a_{I} \big]^{T} \in \mathbb{R}^{I}.	
\end{equation*}
Because of nontrivial re-indexing process, I discuss the vector $ \vec{b} $ and matrix $ \mat{A} $ in dedicated subsections below.

\subsubsection{The Vector \textit{b}}
 The vector $ \vec{b} $ has components
\begin{equation*}
	b_{i} \equiv b_{mn}, \qquad m = 0, 1, \ldots, M, \ n = 1, 2, \ldots, N.
\end{equation*}
There are $ M+1 $ values of $ m $ and $ N $ values of $ n $, so $ \vec{b} $ has $ N(M+1) $ elements. In a notation that makes the re-indexing from $ i $ to $ (mn) $ clear, the vector $ \vec{b} $ reads
\begin{equation*}
	\vec{b} = \Big[(b_{01}, b_{02}, \ldots, b_{0N})\bm{,} \ (b_{11}, b_{12}, \ldots, b_{1N})\bm{,} \ \ldots\bm{,}(b_{M1}, b_{M2}, \ldots, b_{MN}) \Big]^{T} \in \mathbb{R}^{N(M+1)}.
\end{equation*}
Using a loop approach, the vector $ \vec{b} $ is found in Python with
\begin{python}
def get_b(M, N):
    """ Returns the vector b used to find the weight coefficients a_i """
    b = np.zeros(N*(M+1))  # preallocate
    i = 0  # collective index to count both m and n
    for m in range(0, M+1, 1):
        for n in range(1, N+1, 1):
            b[i] = get_bi(m, n)  # calculate individual vector component
            i += 1               # increment total index
    return b
\end{python}
An efficient but perhaps less intuitive vectorized approach follows in \hyperref[gal:ss:Ab-vec]{Subsection \ref{gal:ss:Ab-vec}}.





\subsubsection{The Matrix A}
The matrix $ \mat{A} $ has elements
\begin{equation*}
	\A_{ij} \equiv \A_{(m'n')(mn)}.
\end{equation*}
There are $ M+1 $ values of $ m' $ and $ m $; and $ N $ values of $ n' $ and $ n $, so $ \mat{A} $ has $ N(M+1) $ rows and $ N(M+1) $ columns. Because of orthogonality with respect to $ m $, $ \mat{A} $ simplifies to a block-diagonal of the form
\begin{equation}
\begingroup
\setlength\arraycolsep{1.0pt}
\renewcommand*{\arraystretch}{0.0}
	\mat{A} = 
	\begin{pmatrix}
	 \tmat{A}^{(0)} & & & & & \\
	 & \tmat{A}^{(1)}  & & & & \\
	 & & \ddots         & & & \\
	 & & & \tmat{A}^{(m)} & & \\
	 & & & & \ddots         & \\
	 & & & & & \tmat{A}^{(M)} 
	\end{pmatrix}
	\in \mathbb{R}^{N(M+1) \cross N(M+1)} \label{gal:eq:A},
\endgroup
\end{equation}
where $ \tmat{A}^{(m)} $ is a $ N \cross N $ matrix of the form
\begin{equation}
	\tmat{A}^{(m)} = 
	\begin{pmatrix}
	 \tA^{(m)}_{11}  & \tA^{(m)}_{12} & \cdots & \tA^{(m)}_{1N} \\[2mm]
	 \tA^{(m)}_{21}  & \tA^{(m)}_{22} & \cdots &  \tA^{(m)}_{2N} \\[2mm]
  \vdots  & \vdots & \ddots & \vdots \\[2mm]
		 \tA^{(m)}_{N1}  & \tA^{(m)}_{N2} & \cdots & \tA^{(m)}_{NN}
	\end{pmatrix}
	\in \mathbb{R}^{N \cross N} \label{gal:eq:Am}.
\end{equation}
The matrix elements $ \tA^{(m)}_{n'n} $ are defined in Equation \ref{gal:eq:Aij}. Note that the orthogonality relation $ A_{(m'n')(mn)} \propto \delta_{mm'} $ allows the use of a single index $ m = m' $ for the nonzero block diagonals. I constructed $ \mat{A} $ as follows:
\begin{enumerate}
	\item Initialize a zero-filled $ (N\cdot(M+1))\cross(N\cdot(M+1))$ matrix---because of orthogonality with respect to $ m $, many elements will remain zero anyway. 
	
	\item For each $ m = 0, 1, \ldots, M $ construct the sub-matrix $ \tmat{A}^{(m)} $ given in Equation \ref{gal:eq:Am} by iterating over $ n' = 1, 2, \ldots, N $ in an outer loop and $ n = 1, 2, \ldots, N $ in an inner loop, and calculating the matrix element $ A_{(mn')(mn)} $ according to Equation \ref{gal:eq:Aij}. 
	
	Place the sub-matrix $ \tmat{A}^{(m)} $ in the appropriate block-diagonal position of $ \mat{A} $, as shown schematically in Equation \ref{gal:eq:A}.
\end{enumerate}
This procedure is implemented in the following Python code:
\begin{python}
def get_A(M, N):
    """ Returns the matrix A used to find the weight coefficients a_i """
    A = np.zeros((N*(M+1), N*(M+1)))  # preallocate main matrix
    for m in range(0, M+1, 1):
        A_m = np.zeros((N, N))        # preallocate mth submatrix
        for n1 in range(1, N+1, 1): 
            for n2 in range(1, N+1, 1): 
                A_m[n1-1, n2-1] = get_Aij(m, n1, n2)
        A[m*N:(m+1)*N, m*N:(m+1)*N] = A_m  # place A_m in A's block diagonal
    return A
\end{python}
A more efficient vectorized implementation follows in \hyperref[gal:ss:Ab-vec]{Subsection \ref{gal:ss:Ab-vec}}. 

\begin{figure}[htb!]
\centering
\includegraphics[width=0.49\linewidth]{matrix-2-3} \hfill
\includegraphics[width=0.49\linewidth]{matrix-9-5} \hfill
\caption{Visualizing the block-diagonal matrix $ \mat{A} $ for two values of $ M $ and $ N $---dark squares are larger in magnitude and white squares are zero. Note that the matrix elements $ \A_{ij} $ in the right image are raised to the $ 1/3 $ power for stronger color.}
\label{gal:fig:A}
\end{figure}



\subsection{Solutions}

\subsubsection{Finding the \Pois Coefficient \textit{C}}
To find the \Pois coefficient $ C $, I solved the matrix equation $ \mat{A}\vec{a} = \vec{b} $ for $ \vec{a} $,\footnote{In principle, one could also find $ C $ with $ C = - \frac{32}{\pi}\vec{b}\mat{A}^{-1}\vec{b}$, but solving for $ \vec{a} $---especially with the sparse-optimized \texttt{spsolve}---is more efficient than calculating the inverse matrix $ \mat{A}^{-1} $.} then calculated the coefficient $ C $ according to Equation \ref{gal:eq:C}, i.e.
\begin{equation*}
	C = - \frac{32}{\pi}\sum_{i}b_{i}a_{i} =  -\frac{32}{\pi} \vec{b} \cdot \vec{a}.
\end{equation*}
In Python, using SciPy's optimized sparse matrix equation function $ \texttt{spsolve} $ from the package \texttt{scipy.sparse.linalg}, the coefficient $ C $ is found as follows:
\begin{python}
from scipy.sparse.linalg import spsolve
from scipy.sparse import csr_matrix
# define values of M and N...
 b = get_b(M, N)
 A = get_A(M, N)
 a = spsolve(csr_matrix(A), b)  # convert A to CSR format to use spsolve
 C = - (32/np.pi)*np.dot(b, a)
\end{python}
Table \ref{gal:table:C} gives representative values of the semicircular pipe's coefficient $ C $ for various $ M $ and $ N $, while Figure \ref{gal:fig:C_err} shows the effect of $ M $ and $ N $ on the solution's accuracy.

\begin{table}[htb!]
	\begin{adjustbox}{center}

  	\begin{tabular}{c c}
  
	\begin{tabular}[t]{|c|c|c|}
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		$ M $ & $ N $ & $ C $\\
		\hline
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		1 & 1 		& 0.7461241 \\
		10 & 10 	& 0.7576178 \\
		100 & 100 	& 0.7577218 \\
		150 & 150  	& 0.7577220 \\
		\hline 
	\end{tabular}
	\hspace{5mm}
	&
	\hspace{5mm}
	\begin{tabular}[t]{|c|c|c|}
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		$ M $ & $ N $ & $ C $\\
		\hline
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		1 & 10 		& 0.7493260 \\
		1 & 150 	& 0.7493264 \\
		10 & 1 		& 0.7518211 \\
		150 & 1		& 0.7518413 \\
		\hline
	\end{tabular}
  	\end{tabular}
	\end{adjustbox}
	\caption{A few representative value of $ C $ for various $ M $ and $ N $. The value at $ (M, N) = (150, 150) $ is used as the reference value for estimating error in Figure \ref{gal:fig:C_err}.}
	\label{gal:table:C}
\end{table}


\begin{figure}[H]
\centering
\includegraphics[width=0.49\linewidth]{Cerr-10-10} \hfill
\includegraphics[width=0.49\linewidth]{Cerr-log-30-30} \hfill
\caption{Error in $ C $ with respect to the reference value at $ (M, N) = (150, 150) $ shown in Table \ref{gal:table:C} for various $ M $ and $ N $ in a linear (left) and log scale (right).}
\label{gal:fig:C_err}
\end{figure}

\subsubsection{Velocity Profile Solution}

First, we define a position grid spanning the pipe's semicircular cross section:
\begin{align*}
	& \xi_{k} = \xi_{0} + k \Delta \xi, \qquad k = 0, 1, \ldots, K, \qquad [\xi_{0}, \xi_{K}] = [0, 1]\\
	& \phi_{l} = \phi_{0} + l \Delta \phi, \qquad l = 0, 1, \ldots, L, \qquad \ [\phi_{0}, \phi_{L}] = [0, \pi].
\end{align*}
Next, for each coordinate pair $ (\xi_{k}, \phi_{l}) $, define the 1D vector $ \bm{\Psi}(\xi_{k}, \phi_{l}) $ according to
\begin{align*}
	\bm{\Psi}(\xi_{k}, \phi_{l}) &= \big[\Psi_{1}(\xi_{k}, \phi_{l}), \ldots, \Psi_{I}(\xi_{k}, \phi_{l}) \big]^{T} \in \mathbb{R}^{I}\\
	& \equiv \big[(\Psi_{01}, \Psi_{02}, \ldots, \Psi_{0N})\bm{,} \ldots (\Psi_{M1}, \Psi_{M2}, \ldots, \Psi_{MN})\big]^{T} \in \mathbb{R}^{N\cdot(M+1)},
\end{align*}
where in the second line $ \Psi_{mn} $ is implicitly evaluated at $ (\xi_{k}, \phi_{l}) $ for conciseness. In terms of $ \vec{\Psi} $, the approximate solution for velocity at the point $ (\xi_{k}, \phi_{l}) $ is found as 
\begin{equation*}
	\t{u}(\xi_{k}, \phi_{l}) = \sum_{i}^{I}a_{i}\Psi_{i}(\xi, \phi) = \bm{a} \cdot \bm{\Psi}(\xi_{k}, \phi_{l})
\end{equation*}
I'm leaving out the Python code for brevity---see the function \texttt{get\_u\_loop(K, L, M, N)} in the attached \texttt{galerkin.py} file for the exact implementation. Figures \ref{gal:fig:u-cartesian} and \ref{gal:fig:u-polar} show the semicircular pipe's velocity profile $ u(\xi, \phi) $ in a Cartesian and polar coordinate system, respectively, for $ K = L = 200 $ and $ M = N = 25 $. 

\vspace{2mm}
The attached animation \texttt{anim-velocity.mp4} shows how the velocity profile changes with increasing $ M $ and $ N $. I was surprised to find that even the ``rough'' approximation $ M = N = 1 $, using only two basis functions, produced a fairly accurate rendition of the result found for higher $ (M, N) $, and the effects of increasing $ (M, N) $ are essentially invisible beyond $ (M, N) = (10, 10) $. Meanwhile, for comparison, using a higher-order approximation in the previous Crank-Nicolson report hugely improved the basic $ r = M = 1 $ approximation. (Of course, the differential Crank-Nicolson method is not exactly comparable to the finite-element-style Galerkin method.)

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{velocity-cartesian-25-25}
\caption{The fluid's velocity profile $ u(\xi, \phi) $ in a Cartesian coordinate system.}
\label{gal:fig:u-cartesian}
\end{figure}

\begin{figure}[htb!]
\centering
\includegraphics[width=\linewidth]{velocity-polar-25-25}
\caption{The fluid's velocity profile $ u(\xi, \phi) $ in a polar coordinate system.}
\label{gal:fig:u-polar}
\end{figure}



\section{Vectorized Solutions}
I took this assignment as an opportunity to explore vectorization using Python's Numpy library, with which I had no previous experience. Vectorization is the process of rewriting algorithms involving element-by-element calculations inside Python loops into equivalent procedures acting on entire Numpy \texttt{ndarray} objects with Numpy functions, where operations are delegated to efficient, pre-compiled \texttt{C} code.\footnote{Looping directly in Python is undesirable because Python is a dynamically-typed language, meaning that, ``under the hood'', the Python interpreter must check variable type in each loop iteration, significantly slowing down code execution.} The following sections describe the vectorization process and the resulting performance improvement.

\subsection{The Vector \textit{b} and Matrix A, Vectorized} \label{gal:ss:Ab-vec}
Vectorization heavily involves using Numpy's \texttt{meshgrid} function to create multidimensional coordinate grids on which to evaluate Numpy functions. Using \texttt{meshgrid} to define a grid of $ m $ and $ n $ values, I found the vector $ \vec{b} $ with the following code:
\begin{python}
def get_b_vectorized(M, N):
    """ Vectorized implementation for finding the vector b """
    m = np.arange(0, M+1, 1)  # 0, 1, ..., M
    n = np.arange(1, N+1, 1)  # 1, 2, ..., N
    mm, nn = np.meshgrid(m, n, indexing="ij")  # 2D grids
    B = get_bi(mm, nn)             # returns a 2D (M+1) by N grid
    return np.reshape(B, N*(M+1))  # reshape into a 1D N*(M+1)-element vector
\end{python}
To find the matrix $ \mat{A} $, I calculated only the non-zero block diagonal sub-matrices $ \mat{A}^{(m)} $ (Eq. \ref{gal:eq:Am}) and assembled $ \mat{A} $ from the $ \mat{A}^{(m)} $ using Scipy's \texttt{scipy.linalg.block\_diag} function, as shown in the code block below.
\begin{python}
from scipy.linalg import block_diag  # to construct block diagonal matrix A
def get_A_vectorized(M, N):
    """ Vectorized implementation for finding the matrix A """
    n = np.arange(1, N+1, 1)
    m = np.arange(0, M+1, 1)
    mm, nn1, nn2 = np.meshgrid(m, n, n, indexing="ij")  # 3D grids
    A_vec = get_Aij(mm, nn1, nn2)  # returns a 3D grid of M+1 (N by N) block-diagonal sub-matrices (A^{(m)} in the report's notation)
    return block_diag(*A_vec)      # unpack submatrices and construct A
\end{python}
Figure \ref{gal:fig:times-A} shows the computation time required to assemble the matrix $ \mat{A} $ as a function of the single index $ I = N(M+1) $, where I used $ M = N $ for simplicity. Note the 40-fold improvement in performance thanks to vectorization.

\begin{figure}[htb!]
\centering
\includegraphics[width=\linewidth]{times-A}
\caption{Computation time, averaged over 10 runs, required to calculate the matrix $ \mat{A} $ using ``vanilla'' Python loops compared to an optimized vectorized approach.}
\label{gal:fig:times-A}
\end{figure}


\subsection{Defining Higher-Dimensional Grids for Vectorization}
This section, besides taking advantage of optimized Numpy's functions, also serves as a good exercise in manipulating two and three-dimensional grids. As in Section \ref{gal:s:solution}, we partition the pipe's cross section into the two-dimensional grid
\begin{align*}
	& \xi_{k} = \xi_{0} + k \Delta \xi, \qquad k = 0, 1, \ldots, K, \qquad [\xi_{0}, \xi_{K}] = [0, 1]\\
	& \phi_{l} = \phi_{0} + l \Delta \phi, \qquad l = 0, 1, \ldots, L, \qquad \ [\phi_{0}, \phi_{L}] = [0, \pi].
\end{align*}
As a reference for the code that will follow, we create the position grid in Python via
\begin{python}
# define K, L...
xi0, xiK = 0.0, 1.0                 # max and min scaled radius
xi = np.linspace(xi0, xiK, K+1)
phi0, phiL = 0.0, np.pi             # max and min polar angle
phi = np.linspace(phi0, phiL, L+1)
\end{python}

For review, recall that for each $ (\xi_{k}, \phi_{l}) $, we defined the 1D vector $ \bm{\Psi}(\xi_{k}, \phi_{l}) $ as
\begin{equation*}
	\bm{\Psi}(\xi_{k}, \phi_{l}) = \big[\Psi_{1}(\xi_{k}, \phi_{l}), \ldots, \Psi_{I}(\xi_{k}, \phi_{l}) \big]^{T} \in \mathbb{R}^{I} \equiv \mathbb{R}^{N\cdot(M+1)}.
\end{equation*}
In the vectorized approach, we generalize the vector $ \vec{\Psi} $ to a three-index object 
\begin{equation*}
	\t{\bm{\Psi}} \in \mathbb{R}^{(K+1)\cross(L+1)\cross N\cdot(M+1)},
\end{equation*}
which holds the one-dimensional vector $ \bm{\Psi}(\xi_{k}, \phi_{l}) \in  \mathbb{R}^{N\cdot(M+1)} $ at each two-dimensional coordinate $ (\xi_{k}, \phi_{l}) $. I found $ \t{\bm{\Psi}} $ with the following code:
\begin{python}
""" Purely vectorized approach for finding the 3D grid Psi (slower) """
# define M, N...
m = np.arange(0, M+1, 1)
n = np.arange(1, N+1, 1)
XI, PHI, mm, nn = np.meshgrid(xi, phi, m, n, indexing="ij")
Psi = get_Psi_mn(XI, PHI, mm, nn)  # 4D grid
Psi = np.reshape(Psi_vec, (K+1, L+1, N*(M+1)))  # reshape to 3D
\end{python}
The function \texttt{get\_Psi\_mn} is a direction implementation of Equation \ref{gal:eq:basis1}. I assume creating four 4D coordinate grids in the above code (understandably) outweighed vectorization benefits, because I found the hybrid approach below, which combines vectorization and Python loops, performed faster by a factor of roughly 2.
\begin{python}
""" Hybrid (and faster) vectorized approach for finding the 3D grid Psi """
mm, nn = np.meshgrid(m, n, indexing="ij")      # 2D grid
Psi = np.zeros((K+1, L+1, N*(M+1)))            # preallocate
for k, xi_k in enumerate(xi):                  # loop over xi
    for l, phi_l in enumerate(phi):            # loop over phi
        psi = get_Psi_mn(mm, nn, xi_k, phi_l)  # vectorized
        psi = psi_vec.flatten()                # convert 2D to 1D
        Psi[k][l] = psi_vec  # place 1D psi in 3D Psi
\end{python}
Finally, we generalize the scalar solution $ \t{u}(\xi, \phi) $ to a two-dimensional solution matrix $ \mat{U} \in \mathbb{R}^{(K+1)\cross (L+1)} $, which takes the form
\begin{equation*}
	\mat{U} = 
	\begin{pmatrix}
		\t{u}(\xi_{0}, \phi_{0})  & \t{u}(\xi_{0}, \phi_{1}) & \cdots &  \t{u}(\xi_{0}, \phi_{L}) \\
		\t{u}(\xi_{1}, \phi_{0})  & \t{u}(\xi_{1}, \phi_{1}) & \cdots &  \t{u}(\xi_{1}, \phi_{L}) \\
		\vdots & \vdots & \ddots & \vdots \\
		\t{u}(\xi_{K}, \phi_{0})  & \t{u}(\xi_{K}, \phi_{1}) & \cdots &  \t{u}(\xi_{K}, \phi_{L}) 
	\end{pmatrix}
	\in \mathbb{R}^{(K+1) \cross (L+1)},
\end{equation*}
where $ \t{u} $ is the approximate solution for fluid velocity $ u $ at a point $ (\xi_{k}, \phi_{l}) $.

\subsection{Vectorized Velocity Profile Solution}
In terms of $ \t{\bm{\Psi}}  $ and the weight coefficient vector $ \vec{a} $, we can concisely find $ \mat{U} $ with
\begin{equation*}
	\mat{U} =  \t{\bm{\Psi}} \bullet \bm{a}
\end{equation*}
where $ \bullet $ denotes the multiplication of the 3D object $ \t{\bm{\Psi}} \in \mathbb{R}^{(K+1)\cross(L+1)\cross N\cdot(M+1)} $ with the 1D vector $ \vec{a} \in \mathbb{R}^{N\cdot(M+1)} $ to yield the two-dimensional solution matrix $ \mat{U} \in \mathbb{R}^{(K+1) \cross (L+1)} $. Having found $ \vec{b} $, $ \mat{A} $, and $ \vec{a} $, we can use Numpy's \texttt{np.dot} function to calculate the solution matrix $ \mat{U} $ in a single line of code:
\begin{python}
b = get_b_vectorized(M, N)
A = get_A_vectorized_full(M, N)
a = spsolve(csr_matrix(A), b)   # convert A to CSR for use with spsolve
U = np.dot(Psi, a)              # nice "one-liner"!
\end{python}
Figure \ref{gal:fig:times-u} show the performance improvement resulting from a vectorized computation of $ u(\xi, \phi) $, again using the single index $ I = N(M+1) $ with $ M = N $. Table \ref{gal:table:times} shows the same results in tabular form, to better show the concrete numerical values. 

\begin{figure}[htb!]
\centering
\includegraphics[width=0.95\linewidth]{times-u}
\vspace{-2mm}
\caption{Computation time, averaged over 10 runs, required to calculate the velocity profile $ u(\xi, \phi) $ using ``vanilla'' Python loops compared to a vectorized approach.}
\label{gal:fig:times-u}
\end{figure}

\begin{table}[H]
\begin{subtable}[h]{0.45\textwidth}
    \centering
	\begin{tabular}[t]{|c|c|c|c|}
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		$ M $ & $ N $ & $ t_{\mathrm{vec}} $ & $ t_{\mathrm{loop}} $ \\
		\hline
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		1   & 1   & \SI{3.12}{\milli \second} & \SI{0.30}{\milli \second}\\
		10  & 10  & \SI{5.57}{\milli \second} & \SI{77.1}{\milli \second}\\
		50  & 50  & \SI{0.36}{\second}        & \SI{9.50}{\second} \\
		100 & 100 & \SI{2.41}{\second}        & \SI{96.9}{\second} \\
		\hline 
	\end{tabular}
    \caption{Times when finding $ \mat{A} $}
\end{subtable}
\hspace{10mm}
\begin{subtable}[h]{0.45\textwidth}
    \centering
	\begin{tabular}[t]{|c|c|c|c|}
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		$ M $ & $ N $ & $ t_{\mathrm{vec}} $ & $ t_{\mathrm{loop}} $ \\
		\hline
		\hline {\rule{0pt}{2.3ex}} \hspace{-7pt}
		1 & 1   & \SI{1.22}{\second} & \SI{0.66}{\second}\\
		5 & 5   & \SI{1.25}{\second} & \SI{6.13}{\second}\\
		10 & 10 & \SI{1.52}{\second} & \SI{23.7}{\second}\\
		15 & 15 & \SI{2.31}{\second} & \SI{74.4}{\second}\\
		\hline 
	\end{tabular}
    \caption{Times when finding $ u $}
\end{subtable}
\vspace{-2mm}
\caption{A few representative values of the computation time $ t $ required to calculate the matrix $ \mat{A} $ (left) and velocity profile $ u $ (right) with a vectorized and loop approach. Note that the loop approach is actually faster for small $ (M, N) $.}
\label{gal:table:times}
\end{table}

\section{First-Order Wave Equation}
\subsection{Implementing the Galerkin Method}
For the first-order wave equation, we use the Galerkin ansatz
\begin{equation*}
	\t{u}(x, t) = \sum_{k=-N/2}^{N/2} a_{k}(t) \Psi_{k}(x) = \frac{1}{\sqrt{2\pi}}\sum_{k=-N/2}^{N/2}, a_{k}(t)e^{ikx}
\end{equation*}
where the basis functions are plane waves of the form
\begin{equation}
	\Psi_{k}(x) = \frac{e^{ikx}}{\sqrt{2\pi}}. \label{gal:eq:wave-basis}
\end{equation}
The inner product between basis functions is
\begin{equation*}
	\ip{\Psi_{k}}{\Psi_{j}} = \int_{0}^{2\pi}\Psi_{k}^{*}(x)\Psi_{j}(x)\diff x = \frac{1}{2\pi} \int_{0}^{2\pi}e^{-ikx}e^{ijx}\diff x = \delta_{jk}.
\end{equation*}
Substituting the Galerkin ansatz into the differential equation and taking the inner product of the equation leads to the Galerkin condition
\begin{equation*}
	\int_{0}^{2\pi}\left[\pdv{\t{u}}{t} - \pdv{\t{u}}{x}\right]\Psi_{k}^{*}(x)\diff x = 0, \qquad \text{for } k = - \frac{N}{2}, \ldots, \frac{N}{2}.
\end{equation*}
We then use the Galerkin condition and orthonormality of the basis functions to get a differential equation for the coefficients $ a_{k}(t) $:
\begin{equation}
	\dv{a_{k}}{t} - ika_{k} = 0, \qquad \text{for } k = - \frac{N}{2}, \ldots, \frac{N}{2}. \label{gal:eq:wave-dadt}
\end{equation}
Here we have two options:
\begin{enumerate}
	\item Find the coefficients $ a_{k}(t) $ with the analytic solution
	\begin{equation}
		a_{k}(t) = \sin \left(\frac{k\pi}{2}\right)J_{k}(\pi)e^{ikt}, \label{gal:eq:wave-ak}
	\end{equation}
	where $ J_{k} $ is the Bessel function of the first kind.
	
	\item Find the initial coefficients $ a_{k}(0) $ with the Fourier transform
	\begin{equation*}
		a_{k}(0) = \int_{0}^{2\pi}u(x, 0)\Psi_{k}^{*}(x) \diff x = \frac{1}{\sqrt{2\pi}}\int_{0}^{2\pi}u(x, 0)e^{-ikx}\diff x,
	\end{equation*}
	where $ u(x, 0) = \sin\big[\pi \cos x \big] $ is the known initial condition (shown in Figure \ref{gal:fig:wave-ic}), and propagate the initial coefficients through time using either the analytic solution $ a_{k}(t) = a_{k}(0)e^{ikt} $ or numerically using Equation \ref{gal:eq:wave-dadt}.
\end{enumerate}
Out of laziness, I opted for the analytic solution in Equation \ref{gal:eq:wave-ak}. With the $ a_{k}(t) $ known, we find the solution $ u(x, t) $ with 
\begin{equation*}
	\t{u}(x, t) = \frac{1}{\sqrt{2\pi}}\sum_{k=-N/2}^{N/2} a_{k}(t)e^{ikx}.
\end{equation*}	

\begin{figure}
\centering
\includegraphics[width=\linewidth]{wave-ic}
\caption{The initial condition used when solving the first-order wave equation.}
\label{gal:fig:wave-ic}
\end{figure}

\subsection{Position, Time, and ``Frequency'' Grids}
First, we define a position grid of $ N+1 $ points spanning $ x \in [0, 2\pi] $ and a time grid of $ M+1 $ points spanning $ t $ from 0 to some arbitrary final simulation time, e.g. an integer multiple of $ \pi $.
\begin{align*}
	&x_{n} = x_{0} + n \Delta x, \qquad n = 0, 1, \ldots, N, \quad \ [x_{0}, x_{N}] = [0, 2\pi]\\
	&t_{m} = t_{0} + m \Delta t, \qquad m = 0, 1, \ldots, M, \quad [t_{0}, t_{M}] = [0, \mathcal{N}\pi], \quad  \mathcal{N} \in \mathbb{N}.
\end{align*}
Although this report does not use a purely spectral approach, it is nonetheless worth noting that the term $ k $ is analogous to a ``frequency'' for sampling position $ x $. The corresponding sample rate $ f_{s} $ and Nyquist frequency $ f_{c} $ are
\begin{equation*}
	f_{s} = \frac{N}{x_{N} - x_{0}} = \frac{1}{\Delta x} \eqtext{and} f_{c} = \frac{f_{s}}{2} = \frac{N}{2(x_{N} - x_{0})}.
\end{equation*}
With the Nyquist frequency in mind, I generate a grid of $ N+1 $ values of $ k $ spanning
\begin{equation*}
	k \in \left\{ -\tfrac{N}{2}, -\tfrac{N}{2} + 1, \ldots,  \tfrac{N}{2}\right\}.
\end{equation*}

\subsection{Vector and Matrix Quantities}
I used a vectorized approach for this problem. First, define the vector $ \bm{a}(t) $ as
\begin{equation*}
		\bm{a}(x) = \big[a_{-N/2}(t), \ldots, a_{N/2}(t)\big]^{T} \in \mathbb{C}^{N+1}.
\end{equation*}
We then generalize $ \vec{a} $ to the coefficient matrix $ \mat{A} \in \mathbb{C}^{M \cross N} $, which holds $ \bm{a}^{T}(t) $ as a row at each time $ t $. The matrix $ \mat{A} $ is defined as 
\begin{equation*}
	\mat{A} = 
	\begin{pmatrix}
		a_{-N/2}(t_{0}) & \cdots & a_{N/2}(t_{0}) \\
		a_{-N/2}(t_{1}) & \cdots & a_{N/2}(t_{1}) \\
		\vdots & \ddots & \vdots\\
		a_{-N/2}(t_{M}) & \cdots & a_{N/2}(t_{M})
	\end{pmatrix}
	\in \mathbb{C}^{(M+1) \cross (N+1)},
\end{equation*}
where the matrix elements are found with Equation \ref{gal:eq:wave-ak}. Figure \ref{gal:fig:wave-A} shows the matrix $ \mat{A} $ for $ M = 30 $ and $ N = 50 $. Next, define the vector $ \Psi_{k} $, given by
\begin{equation*}
	\bm{\psi}(x) = \big[\Psi_{-N/2}(x), \ldots, \Psi_{N/2}(x)\big]^{T} \in \mathbb{C}^{N+1},
\end{equation*}
and the associated matrix quantity
\begin{equation*}
	\mat{\Psi} = 
	\begin{pmatrix}
		\Psi_{-N/2}(x_{0}) & \cdots & \Psi_{-N/2}(x_{N}) \\
		\vdots & \ddots & \vdots \\
		\Psi_{N/2}(x_{0}) & \cdots & \Psi_{N/2}(x_{N}) 
	\end{pmatrix}
	\in \mathbb{C}^{(N+1) \cross (N+1)}.
\end{equation*}
The matrix elements are found with the known basis functions in Equation \ref{gal:eq:wave-basis}. Note that within $ \mat{\Psi} $, the index $ k $ changes across rows, while $ k $ changes across columns in the matrix $ \mat{A} $. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{matrix-wave-A-30-50}
\caption{Visualizing the complex matrix $ \mat{A} $ used with the first-order wave equation. Only central $ k $ values are significant, an effect of the $ J_{k}(\pi) $ term in Equation \ref{gal:eq:wave-ak}.}
\label{gal:fig:wave-A}
\end{figure}





\subsection{Solution}
First, we define the solution vector $ \bm{u}(t) $, which holds $ u(x) $ at a fixed time $ t $, as
\begin{equation*}
		\bm{u}(t) = \big[u(x_{0}, t), u(x_{1}, t), \ldots, u(x_{N}, t)\big]^{T} \in \mathbb{C}^{N+1}.
\end{equation*}
The associated solution matrix $ \mat{U} \in \mathbb{C}^{(M+1) \cross (N+1)} $ is 
\begin{equation*}
	\mat{U} = 
	\begin{pmatrix}
		u(x_{0}, t_{0}) & \cdots & u(x_{N}, t_{0}) \\
		u(x_{0}, t_{1}) & \cdots & u(x_{N}, t_{1}) \\
		\vdots & \ddots & \vdots \\
		u(x_{0}, t_{M}) & \cdots & u(x_{N}, t_{M})
	\end{pmatrix}
	\in \mathbb{C}^{(M+1) \cross (N+1)}.
\end{equation*}
The matrix $ \mat{U} $ holds the solution $ \bm{u}^{T}(t) $ as a row at each time $ t $. In terms of $ \mat{A} $ and $ \Psi $, the solution matrix is found concisely with the matrix product
\begin{equation*}
	\mat{U} = \mat{A} \mat{\Psi}.
\end{equation*}
Figures \ref{gal:fig:wave-2d} and \ref{gal:fig:wave-3d} show the time-dependent solution $ u(x, t) $ in two and three dimensions, respectively. In my opinion, neither does a satisfactory job of conveying the actual behavior, which is best visualized with the attached animation \texttt{anim-wave.mp4}. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{wave-2d}
\caption{The solution to the first-order wave equation over a half-period. The wave travels left along the $ x $ axis and preserves the shape of the initial condition in Figure \ref{gal:fig:wave-ic}. This is best visualized in the attached animation \texttt{anim-wave.mp4}.}
\label{gal:fig:wave-2d}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{wave-3d}
\caption{A questionably successful attempt at visualizing the solution to the first-order wave equation in 3D---see the attached \texttt{anim-wave.mp4} for a better result.}
\label{gal:fig:wave-3d}
\end{figure}


\end{document}
