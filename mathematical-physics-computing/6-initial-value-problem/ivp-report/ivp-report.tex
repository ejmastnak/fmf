\documentclass[11pt, a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage{mwe}
\usepackage[margin=3.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm} % for bold vectors in math mode
\usepackage{physics} % many useful physics commands
\usepackage[separate-uncertainty=true]{siunitx} % for scientific notation and units

\usepackage{graphicx}
\graphicspath{{"../figures/"}}
\usepackage[section]{placeins} % to keep figures in their sections
\usepackage[export]{adjustbox} % for subcaptionbox figures

\usepackage[most, minted]{tcolorbox} % for displaying code

\usepackage{xcolor}  % to color hyperref links
\usepackage[colorlinks = true, allcolors=blue]{hyperref}

\setlength{\parindent}{0pt} % to stop indenting new paragraphs
\newcommand{\diff}{\mathop{}\!\mathrm{d}} % differential
\newcommand{\eqtext}[1]{\qquad \text{#1} \qquad}
\renewcommand{\O}{\mathcal{O}}  % for order of DE methods

\newtcblisting{python}{%
	listing engine=minted,
	minted language=python,
	listing only,
	breakable,
	enhanced,
	minted options = {
		linenos, 
		breaklines=true, 
		tabsize=2,
		fontsize=\footnotesize, 
		numbersep=2mm
	},
	overlay={%
		\begin{tcbclipinterior}
			\fill[gray!25] (frame.south west) rectangle ([xshift=4mm]frame.north west);
		\end{tcbclipinterior}
	}   
}



\begin{document}
\title{The First-Order Linear Initial Value Problem}
\author{Elijan Jakob Mastnak\\[1mm]\small{Student ID: 28181157}}
\date{November 2020}
\maketitle

\tableofcontents

\newpage
\begin{center}
\textbf{Assignment}
\begin{enumerate}
	\item Investigate the numerical solutions of the first-order differential equation (\ref{ivp:eq:dTdt-simple}) with the initial temperatures $ T(0) = 21 $ or $ T(0) = -15 $ and an external temperature $ T_{\text{ext}} = -5 $ with the parameter $ k = 0.1 $ using as many different numerical methods as possible. How large a step size is needed for each method? Compare efficiency, accuracy and stability of the methods among each other, using the analytic solution (\ref{ivp:eq:simple-solution}) for reference. 
	
%	\item Vary the parameter $ k $ and determine an appropriate method and step size for each value of $ k $. 
	
	\item \textit{Optional}: Perform a similar analysis of Equation \ref{ivp:eq:dTdt-osc}, which includes a sinusoidal term to model temperature variation across the day, using the parameters $ k = 0.1 $ and $ \delta = 10 $. Experiment with varying the periodic term's amplitude $ A $. Which differential equation method is best suited to accurately determining the value and time of the maximum daily temperature? 
\end{enumerate}
\end{center}

\vspace{2mm}

\rule{\textwidth}{0.2pt}

\section{Theory} \label{ivp:s:theory}
\vspace{-2mm}
\textit{To jump right to the solution, see \hyperref[ivp:s:solution]{Section \ref{ivp:s:solution}}}.

Note that this section is rather long, largely to solidify my own understanding.

\subsection{The Temperature Equation}
For this report we consider the first-order linear differential equation
\begin{equation}
	\dv{T}{t} = -k(T - T_{\text{ext}}) \label{ivp:eq:dTdt-simple}
\end{equation}
which is a simple model of the time dependence of the temperature in a room whose walls have thermal conductivity modeled by $ k $, with an external temperature $ T_{\text{ext}} $. The equation's analytic solution is 
\begin{equation*}
	T(t) = T_{\text{ext}} + e^{-kt}\big[T(0) - T_{\text{ext}}\big]  \label{ivp:eq:simple-solution}
\end{equation*}
A better model for the temperature in a room might include a periodic term to model change in temperature throughout the day, of the form
\begin{equation}
	\dv{T}{t} = -k(T - T_{\text{ext}}) + A \sin\left[\frac{2\pi}{24}(t - \delta)\right] \label{ivp:eq:dTdt-osc}
\end{equation}

\subsection{Overview of Numerical Methods for Linear ODEs}
This report focuses on numerically solving the first-order initial value problem
\begin{equation*}
	y' \equiv \dv{y}{t} = f(y, t)
\end{equation*}
with the initial condition $ y(t_{0}) = y_{0} $ on the interval $ t \in [a, b] $. This is done by first dividing the time interval $ [a, b] $ into $ n $ partition points $ \{t_{i}\}_{i=0}^{n-1} $ where
\begin{equation*}
	a = t_0 < t_1 < \dots < t_{n-1} < t_{n} = b
\end{equation*}
In theory, the spacing between individual points could be arbitrary, but in practice we choose uniformly spaced points separated by a \textit{time step} $ h = t_{i+1} - t_{i} $. The problem reduces to finding $ n $ numerical approximations $ y_{i} $ to the analytic solution $ y $ at each of the $ n $ partition points $ t_{i} $. We then approximate the solution $ y $ by the values $ y_{i} $ at the partition points.


\subsubsection{Explicit and Implicit Methods}
The approximations $ y_{i} $ are found either with \textit{explicit} or \textit{implicit} methods. An explicit method starts with the initial value $ y_{0} $ and calculates successive approximations directly on the basis of the previous approximations, for example
\begin{equation*}
	y_{i+1} = \Phi(h, x_{i}, y_{i-k}, \ldots, y_{i-1}, y_{i})
\end{equation*}
An implicit method calculates approximations by solving a (generally non-linear) equation involving previous approximations and the next approximation, e.g.
\begin{equation*}
	y_{i+1} = \Phi(h, x_{i}, y_{i-k}, \ldots, y_{i}, y_{i+1})
\end{equation*}
Both explicit and implicit methods can be either \textit{single step} or \textit{multi-step}. In single step methods, the next approximation $ y_{i+1} $ is found only from the previous approximation $ y_{i} $. In a multi-step, e.g. $ k $-step method, the next approximation $ y_{i+1} $ is found with the $ k $  previous approximations $ y_{i}, y_{i-1}, \ldots y_{i-k-1} $.

\subsubsection{Error}
Estimates for local truncation error come from comparing the method to the function's Taylor expansion. Error is expressed in powers of the step size $ h $, e.g. $ \O_{\text{l}}(h^{p})$. Higher order methods have lower error. \textit{Local error} is the difference between the numerical approximation and analytic solution at a single point while \textit{global error} is sum of all local errors. Since we make $ N $ steps $ N = \frac{b-a}{h} $ which scale as $ \sim h^{-1} $, the global error of a $ \O_{\text{l}}(h^{p}) $ method scales as
\begin{equation*}
	\O_{\text{g}} = N \cdot \O_{\text{l}} \sim \frac{1}{h} \cdot h^{p} = \O_{\text{g}}(h^{p-1})
\end{equation*}
The global error is one order less than the local truncation error. When error is given without reference to local or global, it is usually refers to global error. 

\subsection{Descriptions of a Few Common Methods}
In all cases below, the function $ f $ gives the value of the derivative $ y' $. 

\subsubsection{Explicit Euler Method}
The simplest and most primitive method. A nice way to remember it is as a rearranged version of the first-order Taylor series approximation for the derivative:
\begin{equation*}
	y' \approx \frac{1}{h}\big[y(x+h) - y(x)\big] \implies y(x + h) = y(x) + h \dv{y}{x}\bigg |_{x}
\end{equation*}
A numerical implementation starts with the initial condition $ y(t_{0}) = y_{0} $ and calculates successive points using
\begin{equation*}
	y_{i+1} = y_{i} + hf(t_{i}, y_{i}) \eqtext{for} i = 0, 1, \ldots, n-1
\end{equation*}
The explicit Euler method has local order $ \O(h^{2}) $ and global order $ \O(h) $. 


\subsubsection{Trapezoid Method}
The trapezoid method reads
\begin{equation*}
	y_{i+1} = y_{i} + \frac{h}{2}\big[f(t_{i}, y_{i}), f(t_{i+1}, y_{i+1})\big]
\end{equation*}
It is an implicit, linear multistep method. The method comes from the trapezoid method for integration; integrating the differential equation from $ t_{i} $ to $ t_{i+1} $ and applying the trapezoid rule gives
\begin{equation*}
	y(t_{i+1}) - y(t_{i}) = \int_{t_{i}}^{t_{i+1}}f\big(t, y(t)\big)\diff t  \approx \frac{h}{2}\big[f(t_{i}, y(t_{i})), f(t_{i+1}, y(t_{i+1}))\big]
\end{equation*}
Using the approximations $ y_{i} \approx y(t_{i}) $ and solving for $ y_{i+1} $ gives
\begin{equation*}
		y_{i+1} = y_{i} + \frac{h}{2}\big[f(t_{i}, y_{i}), f(t_{i+1}, y_{i+1})\big]
\end{equation*}

\subsubsection{Heun's Method}
Heun's method is a 2nd order explicit method and reads
\begin{align*}
	&\tilde{y}_{i+1} = y_{i} + h f(t_{i}, y_{i})\\
	&y_{i+1} = y_{i} + \frac{h}{2}\big[f(t_{i}, y_{i}) + f(t_{i+1}, \tilde{y}_{i+1})\big]
\end{align*}
This is a simple \textit{predictor-corrector} method. The first step uses Euler's method to predict the value $ \tilde{y}_{i+1} $. This intermediate value is then used with the trapezoid method to generate an improved guess $ y_{i+1} $. Two evaluations of $ f $ are required per step, once for $ f(t_{i}, y_{i}) $ (used twice) and once for $ f(t_{i+1}, \tilde{y}_{i+1}) $. 







\subsubsection{Explicit Midpoint Method}
The explicit midpoint method reads
\begin{equation*}
	y_{i+1} = y_{i} + h f\left(t_{i} + \tfrac{h}{2}, y_{i} + \tfrac{h}{2}f(t_{i}, y_{i})\right)
\end{equation*}
The method uses the value of the derivative at the midpoint $ t_{i} + \frac{h}{2} $ between $ t_{i} $ and $ t_{i+1} $. In equivalent, Runge-Kutta form, the midpoint method reads
\begin{align*}
	&k_{1} = f(x_{i}, y_{i}), \qquad k_{2} = f\left(x_{i} + \tfrac{h}{2}, y_{i} + \tfrac{h}{2}k_{1}\right)\\
	&y_{i+1} = y_{i} + hk_{2}
\end{align*}
In this form the midpoint method is often called the RK2 method. The method has global order $ \O(h^{2}) $. Two function evaluations are required per step. The midpoint method is a single-step method with an intermediate half-step stage, which is discarded after each complete step.

\subsubsection{Implicit Midpoint Method}
The implicit midpoint method reads
\begin{equation*}
	y_{i+1} = y_{i} + h f\left(t_{i} + \tfrac{h}{2}, \tfrac{1}{2}(y_{i} + y_{i+1})\right)
\end{equation*}
The derivative is evaluated at the $ t $ coordinate corresponding to the midpoint $ t_{i} + \frac{h}{2} $ and the $ y $ coordinate corresponding to the average of the $ y $ values at the start and end of the step.


\subsubsection{Runge-Kutta RK4}
A canonical choice for its combination of efficiency and accuracy. Starting with $ y(t_{0}) = y_{0}$, we calculate the coefficients 
\begin{align*}
 & k_{1} = f(t_{i}, y_{i}) && k_{2} = f\left (t_{i} + \tfrac{h}{2}, y_{i} + \tfrac{h}{2}k_{1}\right ) \\
 & k_{3} = f\left (t_{i} + \tfrac{h}{2}, y_{i} + \tfrac{h}{2}k_{2} \right ) && k_{4} = f(t_{i} + h, y_{i} + hk_{3})
\end{align*}
and calculate the next term with the formula
\begin{equation*}
	y_{i+1} = y_{i} + \frac{h}{6}\big(k_{1} + 2k_{2} + 2k_{3} + k_{4}\big) + \mathcal{O}(h^{5}), \quad i = 0, 1, \ldots, n - 1
\end{equation*}
The method has global order $ \O(h^{4}) $ and requires four function evaluations per step. It is a single-step method with four intermediate stages; these are discarded after each complete step. 


\section{Solution}  \label{ivp:s:solution}

\subsection{Methods Used in This Report}
As encouraged in the instructions, I tried to test many methods, so this list is rather long. I include a brief description and the \texttt{function\_name} used in this report. 

Since the methods are all well-known, I chose not to include code in the report itself, but each function appears in the attached source code file \texttt{ivp.py}. 

\vspace{2mm}
\underline{Single-step methods from the Runge-Kutta family with fixed step size:}
\begin{itemize}
	\item Euler method \texttt{euler}
	
	\item Heun's method \texttt{heun}
	
	\item Explicit midpoint method \texttt{rk2a}
	
	\item Alternate 2nd order RK2 method \texttt{rk2b}
	
	\item Ralston's 3rd Runge-Kutta method \texttt{rk3r}
	
	\item 3rd order strong stability preserving Runge-Kutta  \texttt{rk3ssp}
	
	\item Classic 4th order Runge-Kutta \texttt{rk4}
	
	\item The 3/8 4th order Runge-Kutta method \texttt{rk438} (which was included along with the canonical RK4 in Kutta's original 1901 paper)
	
	\item Ralston's 4th order Runge-Kutta method \texttt{rk4r} (supposedly minimizes local truncation error)
	
	\item Fixed-step embedded 4-5th order Runge-Kutta with error estimate \texttt{rk45} 

\end{itemize}

\vspace{2mm}
\underline{Single-step, embedded adaptive-step Runge-Kutta methods:}
\begin{itemize}
	\item Adaptive 2-3rd order Bogacki–Shampine method \texttt{bs23} (supposedly used in Matlab's \texttt{ode23})
	
	\item Adaptive 4-5th order Runge-Kutta-Fehlberg method \texttt{rkf45}
	
	\item Adaptive 4-5th order Cash-Karp method \texttt{ck45}

	\item Adaptive 4-5th order Dormand–Prince method \texttt{dp45} (supposedly used in Matlab's \texttt{ode45})
\end{itemize}

\vspace{2mm}
\underline{Multistep methods}
\begin{itemize}
	\item Alas, only the single 4th order multi-step predictor-corrector Adams-Bashforth-Moulton method \texttt{pc4}. I ran out of time and energy to implement more of my own, despite the importance of multistep methods in scientific computing.
\end{itemize}

\vspace{2mm}
\underline{Interesting ideas for another day}
\begin{itemize}
	\item Investigate the backward differentiation family of multistep methods; use more multistep methods in general.
	\item Investigate the behavior of explicit versus implicit methods when applied to stiff equations.
	\item Solving implicit equations with fixed-point iteration.
\end{itemize}

%\pagebreak

\subsection{The Solution to the Differential Equations}
Before wading knee-deep in the details of numerical methods, it feels appropriate to first show the solution to the temperature in the room! Figure \ref{ivp:fig:solution} shows both the simple exponentially decaying solution and the oscillating solution for positive and negative initial temperature $ T_{0} $. The plotted solutions were found with the Adams-Bashforth-Moulton predictor-corrector \texttt{pc4}, but on the macroscopic scale on display in Figure \ref{ivp:fig:solution} all methods give essentially indistinguishable results. 

As seen in Figure \ref{ivp:fig:solution}, the basic exponential model decays to the external temperature $ T_{\text{ext}} $ on a time scale determined by the parameter $ k $. The oscillating model also approaches the external temperature $ T_{\text{ext}} $, about which it oscillates with amplitude $ A $ over a period of 24 hours upon reaching the steady state at large $ t $. 

\begin{figure}[htb!]
\centering

{\includegraphics[width=\linewidth]{solution-21}}
\vfill \vspace{2mm}
{\includegraphics[width=\linewidth]{solution--15}} \vfil

\caption{Exponentially decaying and oscillating solutions with initial conditions $ T_{0} = \SI{21}{\degreeCelsius} $ and $ T_{0} = \SI{-15}{\degreeCelsius} $ found with the 4th order Adams-Bashforth-Moulton predictor-corrector \texttt{pc4}. Tested with $ k = 0.1 $, $ A = 1 $ and $ \delta = 10 $.} 

\label{ivp:fig:solution}

\end{figure} 

\section{Accuracy}
I used the absolute error of each numerical solution relative to the analytic solution in Equation \ref{ivp:eq:simple-solution} as a measure of each method's accuracy---I worked with the simpler model in Equation \ref{ivp:eq:dTdt-simple} when testing accuracy because this method has a convenient analytic solution as a reference for error. I focused on the following:
\begin{enumerate}

	\item Dependence of error on time at a fixed time step $ h $ (for fixed time step methods).
	
	\item Agreement (or disagreement) between the error predicted by embedded methods and error with respect to the analytic solution.
	
	\item The reliability of the tolerance parameter for adaptive-step methods.
	
	\item Dependence of error on step size for fixed time step methods.
\end{enumerate}

\textit{First Note}: I encountered a dilemma when plotting error: I could either present error in a linear scale and preserve the sign (which could be either positive or negative with respect to the analytic solution, depending on the method) or plot the absolute value of error on a better-suited logarithmic scale (naturally, I couldn't plot negative values on a logarithmic scale). I opted for the latter, so keep in mind: all error quantities are absolute values, which hides information about the error's sign.

\vspace{2mm}
\textit{Second Note}:  I was motivated by the Airy function report to also investigate relative error with respect to the analytic solution and found an understandably large spike whenever temperature crossed zero. Since this spike completed dominated the error in the rest of the time domain, I focused only on absolute error, which naturally avoids the risk of divergence near zero. Absolute error would be more interesting in most practical applications anyway.


\begin{figure}[htb!]
\centering
\includegraphics[width=\linewidth]{error-time-fixed-step}

\caption{Error of fixed-step methods (progressing vertically from 1st order Euler to 4th order Runge-Kutta) in the time domain over the course of a 48 hour simulation for various time steps $ h $. Tested with $ k = 0.1 $ and $ T_{0} = \SI{21}{\degreeCelsius} $.} 

\label{ivp:fig:error-time-fixed-step}

\end{figure} 


\subsection{Error in the Time Domain for Fixed-Step Methods}
Figure \ref{ivp:fig:error-time-fixed-step} shows the error of solutions found with the fixed-step methods in the time domain for various time steps $ h $. Accuracy improves in roughly uniform steps of two to three orders of magnitude with each method order. Note the vast numerical range across the various methods---over 10 orders of magnitude for $ h = 0.01 $, from the 1st order Euler method with $ \epsilon \sim 10^{-3} $ to the 4th order \texttt{rk4} and \texttt{pc4} methods with $ \epsilon \sim 10^{-14} $. I was surprised to find the 4th order Runge-Kutta methods slightly but consistently outperformed the multistep \texttt{pc4} on a basis of accuracy.


\subsection{Reliability of Embedded Error Estimates}
Figure \ref{ivp:fig:error-embedded-rk45} compares the error estimated by the \texttt{rk45} method---a 4-5th order embedded method---to the true error with respect to the analytic result. I was surprised by onset of local oscillations in error for small step sizes (see the $ h = 0.01 $ and $ h = 0.05 $ subplots). Conspicuously, the embedded method underestimates its error across a range of step sizes, and, although I only plotted the \texttt{rk45}  method, other embedded methods show similar behavior. 

\begin{figure}
\centering
\includegraphics[width=\linewidth]{error-embedded-rk45}

\caption{Error estimated by the embedded \texttt{rk45} method compared to the true error with respect to the analytic solution. Note that the embedded method consistently underestimates its error! Tested with $ k = 0.1 $ and $ T_{0} = \SI{21}{\degreeCelsius} $ over 72 hours.} 

\label{ivp:fig:error-embedded-rk45}

\end{figure} 

\subsection{Reliability of the Tolerance in Adaptive Step Methods} \label{ivp:ss:adaptive}
\textbf{Note:} I implemented three adaptive-step methods by hand (the 2-3rd order Bogacki–Shampine method \texttt{bs23}, the 4-5th order Cash-Karp method \texttt{ck45} and the 4-5th order Dormand–Prince method \texttt{dp45}), in addition to the provided \texttt{rkf45}. Of these, it appears only \texttt{ck45} works as it should. Although both \texttt{bs23} and \texttt{dp45} give ``macroscopically'' acceptable results (they appear identical to the analytical solution on a scale of e.g. $ T \sim \SI{1}{\degreeCelsius} $), \textit{I have strong reason to believe my implementations contain bugs}, which remained stubbornly hidden over the course of my unsuccessful debugging process. Rather than hide this semi-failure by leaving the offending methods out, I included \texttt{bs23} and \texttt{dp45} to show what could go wrong.

Figure \ref{ivp:fig:error-adaptive-step} shows error in the time domain for solutions found with the four adaptive-step methods for various tolerances $ \epsilon $. Note how \texttt{rkf45} and \texttt{ck45} work properly; \texttt{bs23} drastically underestimates its error with respect to the inputted tolerance $ \epsilon $; \texttt{dp45} both underestimates its error and computes \textit{far} too many than should be necessary, e.g. compare \texttt{dp45}'s densely-spaced points in the $ \epsilon = 0.001 $ subplot to the other methods' sparse solutions. These extremely dense points imply my \texttt{dp45} needs a much smaller step size than it should to meet the inputted tolerance; evidently the partial steps don't converge properly to a correct approximation for the next point. Perhaps a coefficient in my Butcher tableau is slightly off, but I could not for the life of me find which one.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{error-adaptive-step}

\caption{Error for four adaptive-step methods for various tolerances $ \epsilon $, shown with a red line for reference. \texttt{rkf45} and \texttt{ck45} work as intended, while my implementations of \texttt{bs23} and \texttt{dp45} are likely ``buggy'', i.e. the poor performance is my responsibility and not the fault of method in principle. Tested with $ k = 0.1 $ and $ T_{0} = \SI{21}{\degreeCelsius} $.} 

\label{ivp:fig:error-adaptive-step}

\end{figure} 


\subsection{Case Study: Small Step Size Is Not Always Better}
Intuition suggests that an arbitrarily small step size could produce an arbitrarily accurate numerical solution. Unfortunately, floating point arithmetic has other thoughts. As the step size (and thus the solution increments $ T_{i+1} $) grow very small, the increments $ T_{i+1} $ become comparable to and eventually overshadowed by the computer's limit of floating point precision. Worse yet, a small step size generates an enormous number of approximation points and thus an accelerated accumulation of numerical error. The end result, at least for higher-order methods, is a an increase in error for excessively small step size, as shown in Figure \ref{ivp:fig:error-vs-step-size}.




\begin{figure}
\centering

{\includegraphics[width=\linewidth]{error-vs-step-size-euler}}
\vfill
{\includegraphics[width=\linewidth]{error-vs-step-size-rk3r}} \vfill
{\includegraphics[width=\linewidth]{error-vs-step-size-rk4}} \vfill

\caption{Dependence of global and maximum local error on step size $ h $ for \texttt{euler}, \texttt{rk3r} and \texttt{rk4}---note that $ h $ decreases from left to right. The 1st order \texttt{euler} does not reach an optimal step size, although the global error fails to improve appreciably beyond $ h \sim 10^{-2} $. The higher-order \texttt{rk3r} and \texttt{rk4} methods reach an optimal step size near $ 5\cdot 10^{-4} $ and $ 5 \cdot 10^{-3} $. Tested over 48 hours with $ k = 0.1 $ and $ T_{0} = \SI{21}{\degreeCelsius}$.} 

\label{ivp:fig:error-vs-step-size}

\end{figure} 


\section{Efficiency}
I evaluated efficiency by measuring the time required by each method to produce a solution with a given error tolerance $ \epsilon $ (for adaptive-step methods) or a given upper bound on maximum local error (for fixed-step methods). Another metric for efficiency might be computation time needed to find a solution for a given step size $ h $. Since, in practice, we are more interested in the accuracy of the solution than the step size needed to achieve it, I decided it made more sense to measure computation time versus tolerance/error. Since the computation times for this problem were short even for small step sizes, I measured each computation time over an average of 10 runs to get a more precise result than a single run could give.


\begin{figure}[htb!]
\centering
\includegraphics[width=\linewidth]{time-fixed}

\caption{Computation time as a function of maximum permissible local error for each fixed-step method. Note the decrease in computation time with method order for a given tolerance.} 

\label{ivp:fig:time-fixed}

\end{figure} 

\subsection{Computation Time for Fixed Step Size Methods}
I found computation time versus max local error for fixed-step methods in two steps: 
\begin{enumerate}
	\item Calculate solutions with each fixed-step method over a large range of step sizes $ h $ and record the maximum local error in each solution.
	
	\item Use the above data to create a bijective map between step size and maximum local error for each method. With the step sizes spaced closely enough, this map gives a good estimate of the step size needed for a given tolerance. 
\end{enumerate}
I then used this map to measure computation time $ t $ as a function of step size $ h $ as a function of tolerance $ \epsilon $, i.e. $ t\big(h(\epsilon)\big) $. Figure \ref{ivp:fig:time-fixed} shows the results; naturally, smaller error corresponds to longer computation time. Computation time improved with method order for each method tested;  Although higher-order methods would take longer for a fixed step size $ h $ because of the larger number of calculations per step, they nonetheless come out more efficient than low-order methods on a basis of tolerance or maximum local error.


\subsection{Computation Time for Adaptive Step Size Methods}
Adaptive-step methods allow a direct specification of tolerance, so measuring computation time versus tolerance is straightforward. Figure \ref{ivp:fig:time-adaptive} shows the results with the fixed-step \texttt{rk4} for reference. Note the (relatively) terrible performance of my \texttt{dp45} implementation, which is another good indication of the unresolved bugs mentioned in \hyperref[ivp:ss:adaptive]{Subsection \ref{ivp:ss:adaptive}}). The ill-fated \texttt{dp45} aside, all adaptive-step methods perform on-par, if not better than, the fastest fixed-step methods in Figure \ref{ivp:fig:time-fixed}.

\begin{figure}
\centering
\includegraphics[width=\linewidth]{time-adaptive}

\caption{Computation time as a function of tolerance for each adaptive-step method. Aside from the ``buggy'' \texttt{dp45} (see the discussion in \hyperref[ivp:ss:adaptive]{Subsection \ref{ivp:ss:adaptive}}) all adaptive-step methods are comparably efficient and stack up favorably against even the fastest fixed-step methods (\texttt{rk4} shown for reference).} 

\label{ivp:fig:time-adaptive}

\end{figure} 


\section{Stability}
I focused on the following when measuring stability:
\begin{enumerate}
	\item Convergence (or divergence) of the fixed-step methods for progressively increasing step size $ h $
	
	\item Convergence (or divergence) of the adaptive-step methods for progressively increasing tolerance $ \epsilon $
	
	\item The stability of both method families for varying values of the parameter $ k $ and initial temperature $ T_{0} $. 
	
\end{enumerate}
I used both the simple model in Equation \ref{ivp:eq:dTdt-simple} and the oscillating model in Equation \ref{ivp:eq:dTdt-osc} to test stability and ran measurements over a 10 day (240 hour) period.

\subsection{Stability of Fixed-Step Method for Large Step Sizes}
Figures \ref{ivp:fig:stability-fixed-osc} and \ref{ivp:fig:stability-fixed-exp} show the solutions of selected fixed-step methods for increasing step sizes. Naturally, as $ h $ increased the methods began to diverge. Some observations:
\begin{itemize}
	\item Of the fixed-step methods tested, Euler's method impressed me with its stability even for moderate step sizes of $ h = 15 $ hours, while the multistep predictor-corrector \texttt{pc4} surprised me with its relative instability. 
	
	\item  Second-order methods, e.g. \texttt{rk2a} and \texttt{rk2b} were relatively unstable, while the 4th order Runge-Kutta methods \texttt{rk4}, \texttt{rk4r} and \texttt{rk438} performed best out of the fixed-step methods I tested, with \texttt{euler} also reliable.
	
	\item Small changes in initial temperature $ T_{0} $ did not noticeably affect the methods' stability. Meanwhile, all fixed-step methods grew unstable with an increasing exponential parameter $ k $, which leads to a rapidly-changing initial part of the solution. The fixed-step methods tended to fail for $ k > 0.3 $ for step sizes larger than five hours and remained stable for $ k > 1.0 $ only for step sizes below one hour, which is quite small relative to the 240 hour simulation time.
\end{itemize}


\begin{figure}[htb!]
\centering
{\includegraphics[width=\linewidth]{{stability-fixed-0.2}.png}} \vfill

\caption{Stability of fixed-step methods for various values of $ h $ with the oscillating model. \texttt{euler} and the fourth-order methods \texttt{rk4}, \texttt{rk4r} and \texttt{rk438} performed best.} 

\label{ivp:fig:stability-fixed-osc}

\end{figure}

\begin{figure}[H]
\centering
{\includegraphics[width=\linewidth]{{stability-fixed-0.3}.png}} \vfill

\caption{Testing stability of fixed-step methods for various values of $ h $ with the basic exponential model. All methods fail for $ h > 5 $ at $ k = 0.3 $.} 

\label{ivp:fig:stability-fixed-exp}

\end{figure} 



\subsection{Stability of Adaptive-Step Methods for Large Tolerances}
Figure \ref{ivp:fig:stability-adaptive} shows the solutions of selected adaptive-step methods for progressively larger tolerances---I only tested \texttt{rkf45} and \texttt{ck45}, which I was fairly certain worked properly. My takeaways:
\begin{itemize}

	\item As for fixed-step methods, changes in initial temperature $ T_{0} $ did not noticeably affect the methods' stability, so I did not include graphs.
	
	\item Although step-size $ h $ and tolerance $ \epsilon $ cannot be directly compared, the results in Figures \ref{ivp:fig:stability-adaptive} suggest that adaptive-step methods outperform fixed-step methods on grounds of stability. Even for absurdly large tolerances (e.g. $ \epsilon = \SI{50}{\degreeCelsius} $ on a temperature scale with amplitudes $ A = \SI{1}{\degreeCelsius} $), the adaptive-step methods remain relatively stable for $ k $ values at which the fixed-step methods consistently diverged.
	
	\item Note that both adaptive-step understandably suffer in accuracy for the solutions shown in Figure \ref{ivp:fig:stability-adaptive}. This is understandable, since the tolerance is intentionally made excessively large, and I did not count this inaccuracy against the methods, since the purpose of this section was to study stability. Despite considerable oscillations about the correct value, both adaptive-step methods remain consistently stable throughout the 10-day simulation.
	
\end{itemize}





\begin{figure}[htb!]
\centering
{\includegraphics[width=\linewidth]{{stability-adaptive-0.5-osc}.png}} \vfill
{\includegraphics[width=\linewidth]{{stability-adaptive-1.0-osc}.png}} \vfill
{\includegraphics[width=\linewidth]{{stability-adaptive-1.0-simple}.png}} \vfill

\caption{Testing stability of adaptive-step methods for various values of $ \epsilon $ and $ k $; the figure shows solutions for both the oscillating model (top two graphs) and simple exponential model (bottom graph). Although both methods understandably suffer in accuracy as $ \epsilon $ increases, they remain stable even for large $ k $.} 

\label{ivp:fig:stability-adaptive}

\end{figure} 

\clearpage 
\newpage

\begin{thebibliography}{}
\setlength{\itemsep}{.2\itemsep}\setlength{\parsep}{.5\parsep}

\bibitem{rk-methods} 

Wikipedia contributors. ``List of Runge–Kutta methods.'' \textit{Wikipedia, The Free Encyclopedia}. 28 August 2020.  \url{https://en.wikipedia.org/wiki/List_of_Runge-Kutta_methods}

\bibitem{press} Press, William and Teukolsky, Saul. ``Adaptive Stepsize Runge-Kutta Integration''. \textit{Computers in Physics} \textbf{6}, 188 (1992); doi: 10.1063/1.4823060. \url{https://aip.scitation.org/doi/pdf/10.1063/1.4823060}.  % adaptive step size


\bibitem{duturck} Deturck, Dennis and Wilf, Herbert. \textit{Lectures on Numerical Analysis}. Department of Mathematics, University of Pennsylvania. Philadelphia, 2002. \url{https://www.math.upenn.edu/~wilf/DeturckWilf.pdf}.

\end{thebibliography}

\end{document}



