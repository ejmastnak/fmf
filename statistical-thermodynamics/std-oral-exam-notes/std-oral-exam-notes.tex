\documentclass[11pt, a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage{mwe}
\usepackage[margin=3.5cm]{geometry}
\usepackage{fancyhdr}
\usepackage{truncate}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bm} % for bold vectors in math mode
\usepackage{physics} % many useful physics commands
\usepackage[separate-uncertainty=true]{siunitx} % for scientific notation and units
\usepackage{xcolor}  % to color hyperref links
\usepackage[colorlinks = true, linkcolor = blue, urlcolor  = blue, citecolor = blue, anchorcolor = blue]{hyperref}

\setlength{\parindent}{0pt} % to stop indenting new paragraphs

\newcommand{\eqtext}[1]{\qquad \text{#1} \qquad}
\newcommand{\diff}{\mathop{}\!\mathrm{d}} % differential
\newcommand{\dbar}{{\mathchar '26\mkern -11mu \mathrm{d}}} % for improper differentials
\newcommand{\pdveval}[3]{\left(\pdv{#1}{#2}\right)_{#3}}
\newcommand{\heff}{H_{\text{eff}}}

\pdfinfo{
	/Title (Statistical Thermodynamics Oral Exam Notes)
	/Author (Elijan Mastnak)
	/Subject (Physics)
}

% begin header configuration
\fancypagestyle{headerstyle}{
\fancyhf{}  % clear default settings from header and footer; create blank slate
\fancyhead[R]{\href{https://github.com/ejmastnak/fmf}{\small{\texttt{github.com/ejmastnak/fmf}}}}  % link in upper right
\fancyhead[L]{\textit{\truncate{0.65\headwidth}{\rightmark}}}  % subsection name; truncate long names
\fancyfoot[C]{\thepage}  % centered page number in footer
\renewcommand{\headrulewidth}{0.1pt}
}
% end header configuration

\begin{document}

\vspace{-10mm}
\title{Statistical Thermodynamics Oral Exam Notes}
\author{Elijan Mastnak}
\date{2019-2020 Academic Year}
\maketitle

\begin{center}
\textbf{About These Notes}
\end{center}
These notes provide answers to typical questions\footnote{The questions come directly from a list of 60 sample theoretical exam questions from a document compiled in the academic year 2008-2009. The original questions, in Slovene, are included in the \href{https://github.com/ejmastnak/fmf/tree/main/statistical-thermodynamics/std-oral-exam-notes}{\underline{GitHub directory}} containing these notes.} from the oral exam needed to pass the course \textit{Statisti\v{c}na Termodinamika} (Statistical Thermodynamics), a required course in thermodynamics and statistical physics for second-year physics students at the Faculty of Math and Physics in Ljubljana, Slovenia. I wrote the notes while studying for the exam and am making them publicly available in the hope that they might help others learning similar material. The most recent version of the these notes can be found on \href{https://github.com/ejmastnak/fmf/tree/main/statistical-thermodynamics}{\underline{GitHub}}.

\vspace{2mm}
\textit{Navigation}: For easier document navigation, the table of contents is ``clickable'', meaning you can jump directly to a section by clicking the colored section names in the table of contents. Unfortunately, \textit{the clickable links do not work in most online or mobile PDF viewers}; you have to download the file first.

\vspace{2mm}
\textit{On Authorship}: The material herein is far from original---it is primarily a combination of Professor David Tong's \href{http://www.damtp.cam.ac.uk/user/tong/statphys.html}{\underline{lectures on Statistical Physics}} and Professor Primo\v{z} Ziherl's lectures at the University of Ljubljana. I take credit only for compiling the common exam material in one place and for typesetting the notes.

\vspace{2mm}
\textit{Disclaimer:} Mistakes---both trivial typos and legitimate errors---are likely. Keep in mind that these are the notes of an undergraduate student in the process of learning the material himself---take what you read with a grain of salt. If you find mistakes and feel like telling me, by \href{https://github.com/ejmastnak/fmf}{Github} pull request, \href{mailto:ejmastnak@gmail.com}{email} or some other means, I'll be happy to hear from you, even for the most trivial of errors.

\vspace{5mm}
\textbf{How to use these notes}
\begin{itemize}
    \item The sections and subsections follow the order in which the course material was taught.

    \item Each subsection begins with a \texttt{Common Questions} subsubsection, which lists the common questions for the material in the parent subsection.

    \item  Each question is answered in a dedicated subsubsection, where the question is written in italics at the beginning of the subsubsection.
\end{itemize}

\newpage

\tableofcontents	


\iffalse
\section{List of Course Topics and Exam Materal}
\subsection{Thermodynamics Course Material}
\subsubsection{Equations of State in Thermodynamics}
Equilibrium states, thermodynamic variables, temperature.\\
Equations of state; integral and differential form.\\
Equations of state for an ideal gas, real gas, paramagnet, superconductor. Black-body radiation.

\subsubsection{First Law of Thermodynamics}
The first law, work, heat, internal energy. \\
Specific heat and specific enthalpy.\\
The first law for gases, Hirn's experiment, Joule-Kelvin effect.

\subsubsection{Second Law of Thermodynamics}
The second law, reversible and irreversible processes, substitute reversible process.\\ Entropy as a function of state, entropy as a thermodynamic potential. \\
Consequences of the first law\\
Entropy of an ideal gas

\subsubsection{Thermodynamic Potentials}
Enthalpy, Legendre transform, free energy, free enthalpy, chemical potential.\\
Maxwell's relations\\
Adiabatic compressibility, difference of heat capacities, Joule-Kelvin coefficient.

\subsubsection{Phase Changes}
Phase diagrams, the critical point, continuous and non-continuous phase changes.\\
Latent heat\\
Clausius-Clapeyron relation\\
Phase change from fluid to gas: thermodynamic and mechanical equilibrium, Maxwell's rule for fluid-gas phase change.

\subsubsection{Transport Phenomena}
Diffusion of matter and heat.\\
Heat conduction\\
Viscosity\\
Cross transport and thermoelectric effects


\subsection{Statistical Physics Course Material}

\subsubsection{Introduction to Statistical Physics}
Microscopic coordinates, statistical ensemble, stationary distribution.\\
Microcanonical ensemble.\\
Heat exchange, classical canonical ensemble, temperature.\\
Partition function, average energy, equipartition principle.

\subsubsection{Equations of State in Statistical Physics}
Pressure in statistical physics, ideal gas equation, thermodynamic beta.\\
Virial expansion, second virial coefficient, van der Waals equation of state

\subsubsection{Entropy in Statistical Physics}
Gibbs entropy formula\\
Boltzmann entropy formula\\
Separation of energy levels in a macroscopic system\\
The two-state system

\subsubsection{Quantum Statistical Physics}
Pauli exclusion principle, fermions and bosons\\
Bohr-Sommerfeld rule
Normalization of partition function
Quantum canonical ensemble\\
Specific heat of a diatomic ideal gas, translational, rotational and vibrational degrees of freedom.
Quantum rotator\\
Quantum harmonic oscillator at finite temperature and its average occupation number

\subsubsection{Magnetism in Statistical Physics}
Average magnetization, Curie's law\\
Specific heat\\
Ising model of magnetism: mean field approximation, ferromagnetic and paramagnetic phases

\subsubsection{Grand Canonical Ensemble}
Grand canonical potential\\
Classical monatomic gas, chemical constant\\
Pauli exclusion principle, fermions and bosons\\
Fermi-Dirac distribution\\
Bose-Einstein distribution\\
Free electron gas in metals at $ T = 0 $ and specific heat\\
Debye model of specific heat in solids\\
Sound, phonons, low-temperature and high-temperature limits of heat capacity

\subsubsection{Kinetic Theory of Gases}
Pressure of an ideal gas, ideal gas equation\\
Mean speed of gas molecules, mean free path\\
Diffusion of matter and heat\\
Viscosity and transport coefficients


\fi
\newpage

\pagestyle{headerstyle}
\section{Thermodynamics}

\subsection{Equations of State in Thermodynamics}
\subsubsection{Common Questions}
\begin{enumerate}
    \item Discuss thermodynamic equilibrium, the zeroth law of thermodynamics and the relationship between the two. Discuss the role of temperature in this dynamic. 
    \item Explain the gas thermometer temperature scale. What empirical gas law does the scale rely on?

    \item Explain how the Carnot cycle provides a canonical choice for a thermodynamic temperature scale.

    \item What are thermodynamic state variables? What is a function of state? What are intensive and extensive variables? What is an equation of state? Include examples of each.

    \item State and discuss the ideal gas equation of state. In what regime is the ideal gas equation valid, and what is the microscopic interpretation of an ideal gas? Discuss the isotherms of an ideal gas, mentioning their shape and temperature dependence. Which empirical gas laws are combined to form the ideal gas equation?

    \item Give an example of an equation of state for a real gas and discuss the corresponding isotherms. What region of the isotherm corresponds to a gaseous state and which to a fluid state? Discuss which parts of the isotherm represent meta-stable states and which parts represent non-physical states.
\end{enumerate}

\subsubsection{Thermodynamic Equilibrium and the Zeroth Law}
\textit{Discuss thermodynamic equilibrium, the zeroth law of thermodynamics and the relationship between the two. Discuss the role of temperature in this dynamic.}

\begin{itemize}
	\item A system whose energy, momentum, and all other relevant quantities are stable (i.e. do not change with time) from a \textit{macroscopic} viewpoint is in thermodynamic equilibrium. In this case, the probability distribution of the system's microstates, and thus expectation values of operators, are time-independent. 

	\item The zeroth law states:
	\begin{quote}
		If two systems $ A $ and $ B $ are both in equilibrium with a third system $ C $, then $ A $ and $ B $ are also in equilibrium with each other.
	\end{quote}
	Essentially, the zeroth law states that equilibrium is a transitive property.
	
	\item  The zeroth law forms the basis for the definition of a temperature as a function of state. Here's how it works:
	
	Assume $ A $ and $ B $ are both separately in equilibrium with $ C $. By the zeroth law, $ A $ and $ B $ are also in equilibrium with each other. Before we go farther, assume the system is the classic ideal gas (so we're working with something concrete). The system's state variables are then $ (p, V) $. Suppose system $ A $ is in state $ (p_{A}, V_{A}) $ and system $ C $ in state $ (p_{C}, V_{C}) $.
	
	\item First, we consider $ A $ and $ C $: If we fix the variables $ p_{A}, V_{A} $ and $ p_{C} $, there is a particular value of $ V_{C} $ for which $ A $ and $ C $ remain in equilibrium when placed in contact.
	
	We can introduce a constraint $ F $ on systems $ A $ and $ C $ that determines when $ A $ and $ C $ are in equilibrium:
	\begin{equation*}
		F_{AC}(p_{A}, V_{A}; p_{C}, V_{C}) = 0
	\end{equation*}
	With the fixed variables $ p_{A}, V_{A} $ and $ p_{C} $, we can solve this constraint for the equilibrium volume $ V_{C} $ to get
	\begin{equation*}
		V_{C} = f_{AC}(p_{A}, V_{A}; p_{C})
	\end{equation*} 
	for some function $ f_{AC} $; the exact form of the function is not important.
	
	\item We apply an analogous argument to systems $ B $ and $ C $, which are also in equilibrium, and get an analogous equilibrium constraint
	\begin{equation*}
		F_{BC}(p_{B}, V_{B}; p_{C}, V_{C}) = 0
	\end{equation*}
	If we fix $ p_{B}, V_{B} $ and $ p_{C} $, we can solve for the equilibrium volume $ V_{C} $:
	\begin{equation*}
		V_{C} = f_{BC}(p_{B}, V_{B}; p_{C})
	\end{equation*}
	
	\item Equating the two expressions for volume $ V_{C} $ gives
	\begin{equation*}
		f_{AC}(p_{A}, V_{A}; p_{C}) = f_{BC}(p_{B}, V_{B}; p_{C})
	\end{equation*}
	Next, we apply the zeroth law, which tells us that $ A $ and $ B $ are also in equilibrium with each other and set an analogous equilibrium constraint for systems $ A $ and $ B $. 
	\begin{equation*}
		F_{AB}(p_{A}, V_{A}; p_{B}, V_{B}) = 0
	\end{equation*}
	
	\item Here's the key step: transitive nature of equilibrium from the zeroth law tells us the equilibrium between the combinations of $ A $, $ B $ and $ C $ are equivalent; all the systems are in equilibrium with each other. This means that the earlier equation 
	\begin{equation*}
		f_{AC}(p_{A}, V_{A}; p_{C}) = f_{BC}(p_{B}, V_{B}; p_{C})
	\end{equation*}
	encoding the equilibrium between $ (A, C) $ and $ (B,C) $ is equivalent to the constraint $ F_{AB}(p_{A}, V_{A}; p_{B}, V_{B}) = 0 $ for equilibrium between $ A $ and $ B $.
	
	The equivalence of these equations by the zeroth law gives the important result. Because $ p_{C} $ does not appear anywhere in $ F_{AB}(p_{A}, V_{A}; p_{B}, V_{B}) = 0 $, it must occur in $ f_{AC}(p_{A}, V_{A}; p_{C}) = f_{BC}(p_{B}, V_{B}; p_{C}) $ in such a way that it cancels from both sides of the equality. In this case (once $ p_{C} $ is canceled), all trace of system $ C $ disappears from both $ f_{AC} $ and $ f_{BC} $. This means that (as long as we have equilibrium) there is a relationship between $ A $ and $ B $ of the form
	\begin{equation*}
		\theta_{A}(p_{A}, V_{A}) = \theta_{B}(p_{B}, V_{B})
	\end{equation*}
	The value of $ \theta(p, V) $ is called the \textit{temperature} of the system, and the function $ T = \theta(p, V) $ is the corresponding equation of state.
	
	The zeroth law tells us that every system has a well-defined function of state $ \theta(p, V) $ that takes the same value for any two systems in equilibrium. We call this quantity temperature.
	
	\item Note that the zeroth law does not define the specific form of the temperature $ T = \theta(p, V) $; it just says that the quantity exists. In the section on the second law of thermodynamics, we will see there is a canonical choice for the exact form of temperature defined by the Carnot cycle. For now, we will simply pick a reference system to define temperature: the ideal gas, from which we get
	\begin{equation*}
		T(p, V) = \frac{pV}{Nk_{B}}
	\end{equation*}

\end{itemize}

\subsubsection{Temperature Scales: Gas Thermometer}
\textit{Explain the gas thermometer temperature scale. What empirical gas law does the scale rely on?}
\begin{itemize}
	\item We can define a temperature scale from the equation of state for an ideal gas. The ideal gas equation $ T = \frac{pV}{Nk_{B}} $ is perhaps the most natural choice, but the simpler Gay-Lussac law was known earlier and defined the first gas temperature scale. Let's use the Gay-Lussac law.
	
	The Gay-Lussac law relates the temperature and volume of two gases at constant pressure with the equation
	\begin{equation*}
		\frac{V_{1}}{T_{1}} = \frac{V_{2}}{T_{2}}
	\end{equation*}
	It's basically a special case of the ideal gas law at constant pressure.
	
	\item The actual experiment goes something like this: start with a highly sparse (low density) gas (low density so the gas behaves ideally) at constant pressure. Slowly increasing and decreasing the temperature, measure temperature dependence of the gas's volume. Repeating this procedure for many different gases reveals a similar behavior: volume is linearly proportional to temperature for all ideal gases. Of course, we can only play the volume-decreasing game so far: at low volumes and high densities the gas departs from ideal behavior, and the gas eventually condenses and solidifies. And in any case the volume cannot decrease beyond the volume of the constituent gas particles.
	
	\item However, we see something interesting in the $ V(T) $ dependence. If we fit an equation to the data points in the linear regime and extrapolate to low temperatures, \textit{all ideal gases appear to cross zero volume at the same temperature $ T_{0} $}. We call this temperature absolute zero, and it serves as a well-defined reference point for an ideal gas temperature scale.
	

\end{itemize}


\subsubsection{Temperature Scales: Carnot Cycle}
\textit{Explain how the Carnot cycle provides a canonical choice for a thermodynamic temperature scale.}
\smallskip

\textbf{Note:} This question is grouped with the introductory lecture at FMF, and only a brief answer is expected. I'm moving the topic to the entropy chapter, where we'll have enough background to give a more thorough answer.



\subsubsection{State Variables and Equation of State} \label{sss:td:eq_state}
\textit{What are thermodynamic state variables? What is a function of state? What are intensive and extensive variables? What is an equation of state? Include examples of each.}
\begin{itemize}
	\item State variables are the quantities needed to uniquely specify the state of a system. For example, the state variables for an ideal gas are pressure and volume; once we have specified an ideal gas's pressure and volume we can determine its state and all other relevant quantities. 
	
	\item A system's function of state is a function of the system's state variables. The function corresponds to a property of the system that can be derived from the state variables. For example, energy and entropy are typical functions of state. You can think of state variables as holding priority over functions of state; once we know the state variables, we can also find the values of all the function's of state.
	
	In some interpretations, temperature is viewed as a state variable common to all thermodynamic systems. Alternatively, the zeroth law of thermodynamics lets us define temperature as a function of state with the property that any two systems in equilibrium have the same temperature.
	
	\item The adjectives extensive and extensive classify how the value of a system's property scales as the size of the system increases: extensive quantities increase proportionally to the size of the system and intensive quantities are independent of the size of the system. Everyday examples are clearest:
	\begin{itemize}
		\item Mass and volume are extensive quantities: a system twice as big has twice the mass. Volume is analogous: a system twice as big takes up twice the volume.
		
		\item Energy, entropy, and number of particles are other important extensive quantities.
		
		\item Density, temperature, pressure and chemical potential are examples of intensive quantities. For example, if we double the size of a system both its mass and volume will double, so the density stays the same.
	\end{itemize}
	In fact, as the density example suggests, intensive quantities are always ratios or derivatives of extensive quantities: both extensive quantities scale equally, so their ratio remains the same.
	
	\item There is a more formal way of looking at extensive and intensive quantities. Suppose we increase our system by the factor $ \lambda $, which means we scale all relevant ``size'' variables, e.g. volume $ V $ and particle number $ N $ by $ \lambda $. The quantity $ f(V, N) $ is extensive if it scales as 
	\begin{equation*}
		f(\lambda V, \lambda N) = \lambda f(V, N)
	\end{equation*}
	In other words, doubling the system doubled the value of $ f $. It is straightforward to show that intensive quantities stay the same under scaling. Suppose $ f $ and $ g $ are both extensive quantities. In this case their ratio, e.g. $ h = \frac{f}{g} $ scales as
	\begin{equation*}
		h(\lambda V, \lambda N) = \frac{f(\lambda V, \lambda N)}{g(\lambda V, \lambda N)} = \frac{\lambda f(V, N)}{\lambda g(V, N)} = \frac{f(V, N)}{g(V, N)} = h(V, N)
	\end{equation*}
	We see $ h(\lambda V, \lambda N) = h(V, N) $ and the intensive quantity $ h $ doesn't change. By construction, any ratio of extensive quantities stays the same under scaling because the $ \lambda $ factor cancels in the ratio.
	
	\item An equation of state is an equation relating a system's state variables. A classic example is the ideal gas equation of state
	\begin{equation*}
		pV = Nk_{B}T
	\end{equation*}
	which relates the ideal gas state variables $ p $ and $ V $ (and $ T $, depending on your interpretation). 
	
	\item The equation of state for a $ (p, V) $ system can be written in the general differential form
	\begin{equation*}
		\frac{\diff V}{V} = \alpha \diff T - \chi_{T} \diff p
	\end{equation*}
	where the \textit{coefficient of thermal expansion} $ \alpha $ describes the change in volume in response to temperature (at constant pressure) and the \textit{coefficient of isothermal compressibility} $ \chi_{T} $ represents the change in volume in response to pressure at fixed temperature. For physical systems $ \chi_{T} $ is always negative, meaning that volume decreases in response to applied pressure.
\end{itemize}
 
\subsubsection{Ideal Gas Equation of State}
\textit{State and discuss the ideal gas equation of state. In what regime is the ideal gas equation valid, and what is the microscopic interpretation of an ideal gas? Discuss the isotherms of an ideal gas, mentioning their shape and temperature dependence. Which empirical gas laws are combined to form the ideal gas equation?}
\begin{itemize}
	\item The ideal gas equation of state is
	\begin{equation*}
		pV = N k_{B} T
	\end{equation*}
	where $ k_{B} = \SI{1.38e-23}{\joule \, \kelvin^{-1}} $ is the Boltzmann constant. The ideal gas equation relates the state variables $ p $ and $ V $ to the temperature $ T $ and number of particles $ N $. 
	
	Alternatively, the ideal gas equation can be written
	\begin{equation*}
		pV = \frac{m}{M}RT
	\end{equation*}
	where $ R = \SI{8.314}{\joule \, \mole \, \kelvin^{-1}} $ is the universal gas constant. In this form, the equation relates $ p $ and $ V $ to the gas's mass $ m $ and molar mass $ M $. This version is more favored by chemists, who tend to work in moles $ n = \frac{m}{M} $ rather than number of particles $ N $.
	
	\item Microscopically, an ideal gas is a system of non-interacting particles contained in a volume $ V $, where non-interacting means there is not potential between the particles, and all collisions are completely elastic. In fact, all real gas particles have some mutual interaction, but this interaction is negligible at low enough densities when the particles are far apart on average. As such, the ideal gas equation holds for low-density gases.
	
	\item Isotherms are curves of pressure versus volume at fixed temperature $ T $. For an ideal gas, the isotherms are monotonically decreasing functions $ p \sim \frac{1}{V} $ of the form
	\begin{equation*}
		p(V) = \frac{Nk_{B}T}{V}
	\end{equation*}
	where temperature is viewed as a constant parameter. Isotherms at higher temperature temperatures have higher pressures, and occur above lower temperature isotherms in the $ (V, p) $ plane.
	
	For an ideal gas, the coefficients of thermal expansion and isothermal compressibility are
	\begin{align*}
		&\alpha = \frac{1}{V} \pdveval{V}{T}{p} = \frac{1}{V} \frac{Nk_{B}}{p} = \frac{1}{T}\\
		&\chi_{T} =  - \frac{1}{V} \pdveval{V}{p}{T} = +\frac{1}{V} \frac{Nk_{B}T}{p^{2}} = \frac{1}{p}
	\end{align*}
	
	\item The ideal gas equation is a combination of Boyle's law and Gay-Lussac's laws, which are, respectively
	\begin{equation*}
		pV = const. \eqtext{and} \frac{V}{T} = const.
	\end{equation*}
	Combining the two equations gives the ideal gas law $ \frac{pV}{T} = const.$ where the constant is $ Nk_{B} $. Intuitively, the ratio should be proportional to the number of particles $ N $, while the Boltzmann constant ensures dimensions match. 
	
	\item Finally, an interesting note on the numerical value of the Boltzmann constant: We customarily measure physical quantities in units designed so the values of the quantities are everyday numbers of order e.g. $ \sim 1 $. For example, it is much more natural to say a person is two meters tall, rather than some absurd number like $ \SI{3.78e10}{} $ Bohr radii, even though both mean the same thing. And indeed, typical values of $ \frac{pV}{T} $ measured in SI units are relatively everyday numbers of order e.g. $ 10^{-3}$ to $ \SI{e3}{\joule \, \kelvin^{-1}}$. Because typical values of $ N $ are of the order of Avogadro's constant $ \sim 10^{23} $, the Boltzmann constant should be of the order $ \sim 10^{-23} $ in SI units so the product $ Nk_{B} = \frac{pV}{T} $ takes on roughly everyday numerical values. And indeed we find $  k_{B} = \SI{1.38e-23}{\joule \, \kelvin^{-1}}  $.
\end{itemize}


\subsubsection{Real Gas Equation of State}
\textit{Give an example of an equation of state for a real gas (van der Waals equation!) and discuss the corresponding isotherms. What region of the isotherm corresponds to a gaseous state and which to a fluid state? Discuss which parts of the isotherm represent meta-stable states and which parts represent non-physical states.}

\vspace{2mm}
\textit{Note}: the van der Waals equation is discussed in more detail in the section on statistical mechanics and interacting gases.
\begin{itemize}
	\item Real gases account for the interaction between gas particles, so the behavior of real gases deviates from ideal gases; this effect is most prominent at high densities where interactions are more frequent. The classic example of a real gas equation of state is the van der Waals equation
	\begin{equation*}
		k_{B}T = \left(p + \frac{N^{2}}{V^{2}}a\right)\left(\frac{V}{N} - b\right) \equiv \left(p + \frac{a}{v^{2}}\right)\left(v - b\right)
	\end{equation*}
	where $ v = \frac{V}{N} $ is the volume per particle. The equation can also be written in terms of the universal gas constant $ R $ 
	\begin{equation*}
		RT = \left(p + \frac{a}{V_{M}^{2}}\right)\left(V_{M} - b\right)
	\end{equation*}
	where $ V_{V} = \frac{V}{n}$ is the volume per mole of gas. 
	
	A derivation of the van der Waals equation using the Mayer $ f $ function is given in Subsection \ref{sss:vdW_derivation} in the chapter on statistical mechanics and interacting gases. The van der Waals equation is valid only at low densities and at high temperatures (this is clear in the derivation, but for now you just have have to take my word for it.)
	
	\item A note on the parameters. They are
	\begin{equation*}
	 a = \frac{2\pi r_{0}^{3}U_{0}}{3} \qquad \text{and} \qquad b = \frac{2\pi r_{0}^{3}}{3} 
	\end{equation*}
	where $ U_{0} $ models the depth of the potential well in the inter-particle interactions and $ r_{0} $ is the particles' characteristic radius.
	
	$ a $ (which contains potential well factor $ U_{0} $) models the attractive interaction between particles that takes effect at large distances. $ a $ reduces the pressure of the real gas compared to an ideal gas.
		
	The parameter $ b $ contains $ r_{0}^{3} $ and essentially represents the volume of a gas particle. $ b $ serves to parameterize hard-core repulsion at short distances. $ b $ reduces the effective volume of the interacting gas compared to an ideal gas; more volume is taken up by the finite-size particles, so the gas's effective volume (the ``empty'' space) decreases. 
	
	\item The van der Waals isotherms are best analyzed with the equation in the form
	\begin{equation*}
		p = \frac{k_{B}T}{v-b} - \frac{a}{v^{2}}
	\end{equation*}
	\begin{itemize}
		\item First, we see that $ v $ is always larger than the parameter $ b = \frac{2\pi r_{0}^{3}}{3} $ and pressure approaches infinity as $ v $ approaches $ b $ from the left. This makes sense; $ b $ represents the volume of a particle, and the volume per particle $ v $ cannot be smaller than the volume of each particle.
			
		\item At large $ v $ (and thus low density $ \rho = \frac{1}{v} $) the $ p $ versus $ v $ isotherms gently decrease monotonically as $ p \sim \frac{1}{v} $, just like the isotherms of an ideal gas. (This makes sense; we expect a real gas to behave ideally at low densities). The gentle slope $ \abs{\dv{p}{v}} $ means only a small amount of pressure is needed for an appreciable change in volume: the substance is highly compressible, like a gas. The region of high $ v $ represents a gaseous state
			
		\item As $ v $ decreases and and approaches $ b $ from the right, pressure steeply increases as $ p(v) \sim \frac{1}{v - b} $. The steep slope $ \abs{\dv{p}{v}} $ means a great deal of pressure is required for only a small change in volume: the material is nearly incompressible, like a liquid. This region of low $ v $ corresponds to a liquid state.
			
	\end{itemize}

	\item The van der Waals isotherms fall into three regimes classified by the value of temperature $ T $. These are
	\begin{enumerate}
		\item For very high temperatures the isotherms are monotonically decreasing functions scaling approximately as $ p \sim \frac{1}{v} $, just like the isotherms of an ideal gas. This is expected---most interacting gases begin to behave like an ideal gas in the high temperature regime.
				
		\item At very low temperatures (when both terms in the van der Waals equation contribute comparably, roughly for temperatures where $ k_{B}T \sim \frac{a}{v} $), the $ (v, p) $ curve has a wiggle. You basically need an actual graph in front of you to visualize this. The wiggle represents an highly unstable, non-physical state; we will discuss it more detail shortly.
				
		\item At an intermediate \textit{critical temperature}  $ T = T_{c} $, the maximum and minimum of the wiggle meet, and the wiggle flattens out to form an inflection point. This occurs where $ \dv{p}{v} = \dv[2]{p}{v} = 0 $ and is satisfied only for $ T = T_{c} $ given by
		\begin{equation*}
			k_{B}T_{c} = \frac{8a}{27b}
		\end{equation*}
		Along the way, we also get the critical volume $ v_{c} $ and pressure $ p_{c} $; these are
		\begin{equation*}
			v_{c} = 3b \qquad \text{and} \qquad p_{c} = \frac{a}{27b^{2}}
		\end{equation*}
	\end{enumerate}
	
	\item The non-physical and metastable states occur in the low temperature $ T < T_{c} $ isotherm. 	
	
	The region with $ \pdv{p}{v} > 0 $ is non-physical. Positive slope $ \pdv{p}{v} $ means that if we increase the gas's volume, the pressure increases. Alternatively, if we decrease the gas's volume (e.g. by compression) the pressure decreases! Such a system has negative compressibility, which is non-physical. 
	
	I'll discuss meta-stable states in the discussion, where I think the context is better. But to give a quick answer, meta-stable state are the regions of the van der Waals isotherm between the equilibrium pressure at which the gaseous and liquid phases coexist and the non-physical region with $ \pdv{p}{v} > 0 $. The equilibrium pressure comes from the Maxwell construction of the liquid-gas phase transition, discussed in the section on phase transitions.
	
\end{itemize}
 
\subsubsection{Liquid-Gas Phase Change}
\textit{The University of Ljubljana course has a question about the liquid-gas phase change, but I am moving that to the section on phase changes, where I feel it is better suited.}


\subsection{The First Law of Thermodynamics}

\subsubsection{Common Questions}
\begin{enumerate}
    \item Discuss the first law of thermodynamics and state its associated equation in macroscopic and differential form. State the differential form of work for a gas, rod, magnetic, and dielectric thermodynamic system.

    \item What is enthalpy? Explain the mathematics behind the derivation process, making sure to discuss the role of the Legendre transformation. In which situations is enthalpy a preferred quantity to internal energy? What is the difference in internal energy and enthalpy in the evaporation of water?

    \item What are heat capacity and specific heat capacity? What are the definitions of heat capacity for a gas at constant volume and at constant temperature?

    \item Explain the process of Joule expansion (aka Hirn's experiment at UL FMF). What does the process tell us about the properties of an ideal gas?

    \item Explain the Joule-Kelvin (Joule-Thomson) effect and the associated experiment. Derive the Joule-Kelvin coefficient.
\end{enumerate}


\subsubsection{Statement of the First Law}
\textit{Discuss the first law of thermodynamics and state its associated equation in macroscopic and differential form. State the differential form of work for a gas, rod, magnetic, and dielectric thermodynamic system.}
\begin{itemize}
	\item The first law is usually stated in the form 
	\begin{equation*}
		\Delta E = Q + W 
	\end{equation*}
	meaning the change a system's internal energy during a thermodynamic process equals the sum of the work $ W $ done on the system and the heat $ Q $ added to the system during the process. However, the equation itself is really a consequence of the first law. The first law is best understood as a statement of conservation of energy allowing for multiple modes energy transfer (i.e. heat and work).
	
	\item For an \textit{isolated} system we can formulate the first law as
	\begin{quote}
		The amount of work required to change an isolated system between two states is independent of how the work is performed.
	\end{quote}
	By the first law, when we perform work $ W $ on an \textit{isolated} system, the change in energy $ \Delta E $ is the same regardless of how the work is performed, which allows us to define energy as a function of state.
	
	If the system is \textit{not} isolated, the change in the system's energy in a thermodynamic process is general not equal to the amount of work done. A simple example is heating a pot of water with a gas flame: the flame does not do work on the water, but the water's energy increases nonetheless. We allow for the additional (non-work) mode of energy transfer by defining \textit{heat}, a mode of energy transfer that arises from temperature differences. This leads to the familiar equation form of the first law
	\begin{equation*}
		\Delta E = Q + W
	\end{equation*}
	The first law links two state of a system in terms of the change in internal energy and modes of energy transfer between the states.
	
	Note that heat, like work, is not a \textit{type} of energy, like potential or kinetic energy, and a system cannot ``contain'' heat. Instead, heat is a mode of energy transfer. Because they are modes of energy \textit{transfer} and not properties of a system itself, neither heat nor work are functions of state (while energy is).
	
	\item In differential form, the first law reads
	\begin{equation*}
		\diff E = \dbar Q + \dbar W
	\end{equation*}
	Where $ \dbar Q $ and $ \dbar W $ are infinitesimal quantities of heat and work. The $ \dbar $ notation stresses that $ Q $ and $ W $ are \textit{improper differentials}. Because neither $ Q $ nor $ W $ are functions of state, an infinitesimal amount of work or heat has no interpretation as a total differential and is simply a small quantity. Meanwhile, energy $ E $ is a function of state, so $ \diff E $ can be viewed as both a small amount of energy and a total derivative. For example, for an ideal gas with $ E = E(p, V) $, an infinitesimal change in energy is the total derivative
	\begin{equation*}
		\diff E = \pdv{E}{p}\diff p + \pdv{E}{V}\diff V
	\end{equation*}
	
	\item By focusing on a particular system we can be more specific about the form of work. Following are some examples.
	\begin{itemize}
		\item For an ideal gas, the work done on the system is $ \dbar W = - p \diff V $ and the first law reads
		\begin{equation*}
			\diff E = \dbar Q - p\diff V
		\end{equation*}
		The negative sign ensures that when the system is compressed (i.e. $ \diff V < 0 $ ) the work done on the system is positive. 	
		
		\item For thin film we have $ \dbar W = - \gamma \diff A $ and
		\begin{equation*}
			\diff E = \dbar Q - \gamma \diff A
		\end{equation*}
		where $ \gamma $ is the surface tension and $ A $ is the surface area.
		
		\item For a thin rod or wire we have $ \dbar W = - F \diff l $ and
		\begin{equation*}
			\diff E = \dbar Q - F \diff l
		\end{equation*}
		where $ F $ is the force on the rod and $ l $ is the rod's length.
		
		\item For a magnetic system with described by magnetic flux density $ B $ and magnetic moment $ m $ the first law is
		\begin{equation*}
			\diff E = \dbar Q + B \diff m
		\end{equation*}
		
		Alternatively, for a magnetic system with described by magnetic field strength $ H $ and volume magnetization $ \mathcal{M} = MV $ the first law is
		\begin{equation*}
			\diff E = \dbar Q + \mu H \diff \mathcal{M}
		\end{equation*}
		
		\item For an electric system described by electric field strength $ E $ and volume electric polarization and $ \mathcal{P} = VP $ the first law is
		\begin{equation*}
			\diff E = \dbar Q + E \diff \mathcal{P}
		\end{equation*}
	\end{itemize}
	
\end{itemize}

\subsubsection{Enthalpy}
\textit{What is enthalpy? Explain the mathematics behind the derivation process, making sure to discuss the role of the Legendre transformation. In which situations is enthalpy a preferred quantity to internal energy? What is the difference in internal energy and enthalpy in the evaporation of water?}
\begin{itemize}
	\item Enthalpy is defined as
	\begin{equation*}
		H = E + p V
	\end{equation*}
	The total derivative of enthalpy is 
	\begin{equation*}
		\diff H = T \diff S + V \diff p
	\end{equation*}
	so enthalpy is best thought of as function of entropy and pressure. Mathematically, enthalpy is a Legendre transform of internal energy, changing from the variable $ V $ to the variable $ p $. Enthalpy is best suited for describing systems at fixed energy and pressure.
	
	\item Suppose we completely evaporate a mass $ m $ of liquid into gas at constant pressure. When we study phase changes, we'll learn that changing a liquid into a gas requires the addition of heat equal to $ Q = mL $ where $ L $ is the specific latent heat associated with the transition; for now you'll just have to take my word that the transition requires heat $ Q = mL $. 
	
	Because pressure is constant, the work done on the gas simplifies to $ W = - \int p\diff V = - p \Delta V $. With $  W = - p \Delta V $ and $ Q = mL $, the first law applied to the evaporation process reads
	\begin{equation*}
		\Delta E = Q + W = mL - p \Delta V = mL - p(V_{\text{gas}} - V_{\text{liquid}})
	\end{equation*}
	Meanwhile, the change in enthalpy during the process is 
	\begin{equation*}
		\Delta H = \Delta E + \Delta[pv] = mL - p \Delta V + p \Delta V = mL
	\end{equation*}
	where $ \Delta[pV] = p \Delta V$ at constant pressure. Notice the changes in internal energy and enthalpy differ by the factor
	\begin{equation*}
		\Delta H - \Delta E = mL - (mL - p \Delta V) = p \Delta V
	\end{equation*}
	So the change in enthalpy is larger by the amount $ p \Delta V $, which is the work done by the gas during the evaporation process. Because a given mass of gas takes up much more volume than a liquid, $ \Delta V = V_{\text{gas}} - V_{\text{liquid}} \approx V_{\text{gas}} $. For an ideal gas we can use the ideal gas equation to get
	\begin{equation*}
		\Delta H - \Delta E = p \Delta V \approx p V_{\text{gas}} = Nk_{B}T_{\text{gas}}
	\end{equation*}
	To get a feel for the associated magnitudes, the change in enthalpy and internal energy per unit mass when evaporating water at room temperature and pressure are roughly
	\begin{equation*}
		\Delta E \approx \SI{2.09}{\mega \joule \cdot \kilogram^{-1}} \eqtext{and} \Delta H \approx \SI{2.26}{\mega \joule \cdot \kilogram^{-1}}
	\end{equation*}
	The change in enthalpy is larger by an amount equal to the work done in the evaporation process.
\end{itemize}

\subsubsection{Heat Capacity}
\textit{What are heat capacity and specific heat capacity? What are the definitions of heat capacity for a gas at constant volume and at constant temperature?}
\begin{itemize}
	\item A thermodynamic system's heat capacity $ C $ is defined as
	\begin{equation*}
		C = \pdv{Q}{T}
	\end{equation*}
	where $ Q $ is the energy added to system as heat. Heat capacity measures the amount of heat energy needed to produce a given change in temperature. Specific heat capacity $ c $ is generally defined as heat capacity per unit mass:
	\begin{equation*}
		c = \frac{1}{m}\pdv{Q}{T}
	\end{equation*}
	In statistical mechanics, specific heat capacity is usually defined as heat capacity per particle $ c = \frac{1}{N}\pdv{Q}{T} $ (instead of per mass). In either case, heat capacity is an extensive quantity while specific heat capacity is intensive.
	
	In practice, we usually measure heat capacity at either constant volume or constant pressure. The corresponding quantities are
	\begin{equation*}
		C_{V} = \pdveval{Q}{T}{V} \eqtext{and} C_{p} = \pdveval{Q}{T}{p}
	\end{equation*}
	
	\item For an ideal gas, we can use the first law of thermodynamics $ \diff E = \dbar Q - p \diff V $ to derive more concrete expressions for $ C_{V} $ and $ C_{p} $. At constant volume $ \diff V = 0 $, so $ \dbar Q = \diff E $ and we have
	\begin{equation*}
		C_{V} \equiv \pdveval{Q}{T}{V} = \pdveval{E}{T}{V}
	\end{equation*}
	Heat capacity of an ideal gas at constant pressure is defined in terms of enthalpy $ H = E + pV $, which we define more thoroughly in the next bullet point. The total differential is $ \diff H = \dbar Q + V \diff p $, so at constant pressure $ \diff p = 0 $ and $ \dbar Q = \diff H $. 
	\begin{equation*}
		C_{p} \equiv \pdveval{Q}{T}{p} = \pdveval{H}{T}{p}
	\end{equation*}
	
	\item The difference of heat capacities $ C_{p} $ and $ C_{V} $ can be written
	\begin{equation*}
		C_{p} - C_{V} = T \pdveval{V}{T}{p}\pdveval{p}{T}{V}
	\end{equation*}
	The relationship is derived in Subsection \ref{sss:applications_max_rel} with the help of Maxwell's relations. Evaluating the derivatives for the ideal gas equation of state $ pV = Nk_{B}T $ gives 
	\begin{equation*}
		C_{p} - C_{V} = T \left(\frac{Nk_{B}}{p}\right) \left(\frac{Nk_{B}}{V}\right) = Nk_{B}
	\end{equation*}
	An alternate path to the same result uses the definition of enthalpy, the identity $ \diff E = C_{v} \diff T $ for an ideal gas (derived in Subsection \ref{sss:joule_expansion}), and the ideal gas law $  pV = Nk_{B}T $. 
	\begin{equation*}
		\diff H = \diff E + \diff [pV] = C_{V} \diff T + \diff [Nk_{B}T] = (C_{V} + Nk_{B}) \Delta T 
	\end{equation*}
	We then take the partial derivative with respect to $ T $ and get
	\begin{equation*}
		C_{V} + Nk_{B}  = \pdveval{H}{T}{p} \equiv C_{p} \implies C_{p} - C_{V} = Nk_{B}
	\end{equation*}
	In other words, $ C_{p} $ is larger than $ C_{V} $ for an ideal gas by a factor of $ Nk_{B} $. This makes sense: at constant volume the system can't do work and all heat goes into raising the system's internal energy, and thus temperature. At constant pressure at least some heat added to a system goes into work, so there is less energy available to raise the system's temperature. More heat is needed for a given temperature increase, so $ C_{p} > C_{V} $. This basically means we have to add more heat to a constant pressure system than to a constant volume system for a given increase in temperature.
		
\end{itemize}



\subsubsection{Joule Expansion} \label{sss:joule_expansion}
\textit{Explain the process of Joule expansion (aka Hirn's experiment at UL FMF). What does the process tell us about the properties of an ideal gas?}
\begin{itemize}
	\item Joule expansion in its purest form is a theoretical process: the expansion of an ideal gas in vacuum. This is an idealization, but we can come pretty close in practice as follows: consider two isolated chambers, one filled with an ideal gas and one evacuated, and a removable partition between the two chambers. We rapidly remove the partition observe the resulting expansion of the gas into the evacuated chamber. The two chambers are otherwise isolated from their surroundings. Experiment shows something interesting: the temperature of the gas is constant throughout the process.
	
	\item Because it is isolated, the gas does not exchange heat with its surroundings, so $ Q = 0 $ for the process. Likewise, the isolated surroundings cannot do work on the gas, nor does the gas do any work in expanding into a vacuum where $ p = 0 $, so $ W = 0 $ for the process as well. The first law for Joule expansion is then
	\begin{equation*}
		\Delta E = Q + W = 0 \implies \diff E = 0
	\end{equation*}
	
	\item Viewing internal energy as a function of temperature and volume $ E = E(T, V) $ gives a total differential
	\begin{equation*}
		\diff E = \pdveval{E}{T}{V}\diff T + \pdveval{E}{V}{T}\diff V
	\end{equation*}
	Since both $ \diff E = 0 $ (first law) and $ \diff T = 0 $ (experiment) for Joule expansion we have
	\begin{equation*}		
		\pdveval{E}{V}{T}\diff V = 0
	\end{equation*}
	and since $ \diff V \neq 0 $ (the gas expands!) we must have
	\begin{equation*}
		\pdveval{E}{V}{T} = 0
	\end{equation*}
	
	\item We can make a completely analogous argument in the interpretation $ E = E(T, p) $ with the total differential
	\begin{equation*}
		\diff E = \pdveval{E}{T}{p}\diff T + \pdveval{E}{p}{T}\diff p
	\end{equation*}	
	to conclude that $ \pdveval{E}{p}{T} = 0 $. These two results are important; I'll write them again:
	\begin{equation*}
		\pdveval{E}{V}{T} = 0 \eqtext{and} \pdveval{E}{p}{T} = 0
	\end{equation*}
	In other words, the internal energy of an ideal gas is independent of volume and pressure at constant temperature $ T $, which means the \textit{internal energy of an ideal gas is a function of temperature alone}. 
	
	The result $ E = E(T) $ allows us to define the internal energy of an ideal gas as
	\begin{equation*}
		E(T) = C_{V} T \eqtext{or} \diff E = C_{V} \diff T
	\end{equation*}
	In fact, we will derive this same result in the form $ E(T) = Nk_{B}T $ in the statistical physics section.

\end{itemize}


\subsubsection{Joule-Thomson-Kelvin Effect} \label{sss:jtk_experiment}
\textit{Explain the Joule-Kelvin (Joule-Thomson) effect and the associated experiment. Derive the Joule-Kelvin coefficient.}
\begin{itemize}
	\item The experiment involves two regions at different pressures in an isolated tube separated by a porous barrier. Gas slowly flows through the porous barrier from the high pressure chamber to the low pressure chamber. 
	
	\item Consider a fixed mass of gas that moving through the barrier. The gas starts in the high pressure region at pressure $ p_{h} $ and volume $ V_{h} $, transfers completely through the barrier, and finishes in the lower pressure region at pressure $ p_{l} $ and volume $ V_{l} $. Since the gas in the tube is isolated there is no heat transfer from the surroundings and $ Q = 0 $. The net work done on the gas in the process is $ W = W_{l} + W_{h} $ where 
	\begin{equation*}
		W_{h} = - p_{h}(0 - V_{h}) = p_{h}V_{h} \eqtext{and} W_{l} = - p_{l}(V_{l} - 0) = -p_{l}V_{l}
	\end{equation*}
	The first law then reads
	\begin{equation*}
		\Delta E = Q + W = p_{h}V_{h} - p_{l}V_{l}
	\end{equation*}
	If we write $ \Delta E = E_{l} - E_{h}  $ and rearrange we see
	\begin{equation*}
		E_{h} + p_{h}V_{h} =  E_{l} + p_{l}V_{l} \implies H_{h} = H_{l}
	\end{equation*}
	In other words, \textit{enthalpy $ H $ is conserved in the Joule-Thomson experiment}.
	
	\item The \textit{Joule-Kelvin coefficient} describes how much the gas's temperature changes as the gas moves across the pressure gradient. It is defined as
	\begin{equation*}
		\pdveval{T}{p}{H}
	\end{equation*}
	For an ideal gas the temperature does not change and the Joule-Kelvin coefficient is zero. We can show this with the ideal gas law, conservation of enthalpy in the Joule-Kelvin experiment, and the result $ E(T) = C_{V}T $. By enthalpy conservation
	\begin{equation*}
		H_{1} = H_{2} \implies E_{1} + p_{1}V_{1} = E_{2} + p_{2}V_{2}\\
	\end{equation*}
	The ideal gas law $ pV = Nk_{B}T $ and $ E(T) = C_{V}T $ gives
	\begin{align*}
		&C_{V}T_{1} + Nk_{B}T_{1} = C_{V}T_{2} + Nk_{B}T_{2}\\
		&T_{1}(C_{V} + Nk_{B}) = T_{2}(C_{V} + Nk_{B})\\
		&T_{1} = T_{2} \implies \Delta T = 0
	\end{align*} 
\end{itemize}


\subsection{Second Law of Thermodynamics}

\subsubsection{Common Questions}
\begin{enumerate}
    \item State and explain the second law of thermodynamics.
    
    \item Explain the Carnot cycle and how the cycle is reversible by construction.

    \item Explain how the Carnot cycle provides a canonical choice for a thermodynamic temperature scale.

    \item What is entropy and how is it derived? Discuss the relationship between entropy and the second law of thermodynamics.

    \item State some implications of the second law of thermodynamics (of the definition of entropy). Show that the second law requires heat to flow from hot regions to cold regions and not vice versa.

    \item Derive the dependence of the entropy of an ideal gas on temperature and pressure and the dependence on temperature and volume.

\end{enumerate}

\subsubsection{Terminology: Quasi-Static, Reversible and Irreversible Processes}
\textit{Note}: This subsubsection is not a question. I decided to include it to clear up some terminology that will be used when discussing the second law of thermodynamics in the following questions.

\vspace{2mm}

A \textit{thermodynamic process} is a way of getting a system between thermodynamic states. Here are some important classes of thermodynamic processes:
\begin{itemize}
	\item A \textit{quasi-static} process is a thermodynamic process in which energy transfers to or from a system slowly and gently enough that the system effectively remains in equilibrium and is well defined by its state variables throughout the entire process. 
	
	\item \textit{Reversible processes} are a special class of quasi-static process in which no energy is lost to friction. A process taking a system between two states $ A $ and $ B $ is reversible only if the process can be run forward from $ A $ to $ B $ and backward from $ B $ to $ A $ \textit{along the exact same intermediate states}. To maintain the quasi-static equilibrium condition, any heat transfer in a reversible process must occur at constant temperature, while any temperature change must occur without heat transfer.
	
	We usually model reversible processes as a series of many infinitesimal quasi-static steps, so the system remains in equilibrium throughout the process. 
	
	\item A \textit{irreversible process} is any process that cannot return from a final to an initial state \textit{along the exact same intermediate states}. This could be because of energy losses to friction or other dissipative forces, or because the process was violent and explosive, so the system did not remain in quasi-equilibrium throughout.
	
	Note that it is \textit{not} impossible to return the system to its initial state after a irreversible process. Far from it. We just can't return from the final to the initial state the exact same way (via the same intermediate states) that we got from the initial state to the final state.

	

\end{itemize}

\subsubsection{The Second Law of Thermodynamics}
\textit{State and explain the second law of thermodynamics.}

\smallskip
The second law of thermodynamics is usually given in two equivalent forms:
\begin{quote}
	\textbf{Kelvin formulation:} No process is possible whose \textit{only} effect is to extract heat from a heat reservoir and convert it entirely into work.
\end{quote}
\begin{quote}
	\textbf{Clausius formulation:} No process is possible whose \textit{only} effect is heat transfer from a colder to a hotter body. Equivalently, heat flows from hotter to colder bodies.
\end{quote}
These two statements of the second law allow the definition of the quantity and function of state called entropy. Some important implications of the second law are discussed in the following sections.

\smallskip 
If the Kelvin formulation were invalid the Clausius formulation would also be invalid. If Kelvin were invalid, we could perform a process who only result would be to transfer heat from a heat reservoir completely into work. We could then (e.g. with friction) completely transform this work back into heat and use the heat to arbitrarily increase the temperature of some given body. We could keep this up until the temperature of the body increased beyond the temperature of the heat reservoir. Once the $ T_{2} > T_{1} $ sole effect of the process is the transfer of heat from a colder body to a hotter body, in violation of Clausius's formulation. 

\subsubsection{The Carnot Cycle}
\textit{Explain the Carnot cycle and how the cycle is reversible by construction.}
\begin{itemize}
	\item The Carnot cycle is a theoretical series of four reversible process running in a continuous cycle. The system operates between two heat reservoirs: a hot reservoir with temperature $ T_{H} $ and a cold reservoir with temperature $ T_{C} $. The four steps are
	\begin{enumerate}
		\item Isothermal (constant temperature) expansion at the hot temperature $ T_{H} $. The gas pushes against the sides of its container and is allowed to slowly (quasi-statically) expand, performing usable work in the process. To keep the temperature constant on account of increasing volume, the system absorbs heat $ Q_{H} $ from its surroundings. This is the main work step!
		
		\item Adiabatic (without heat flow) expansion: the gas continues to expand and because the gas is adiabatically isolated (e.g. can't absorb heat) pressure and temperature decrease on account of increasing volume. Temperature decreases from $ T_{H} $ to $ T_{C} $. The gas performs work, but not as much as in step one.
		
		\item Isothermal contraction at the cold temperature $ T_{C} $. We do work on the system and the volume decreases. The system emits heat $ Q_{H} $ to its surroundings to account for decreasing volume at constant temperature.
		
		\item Adiabatic contraction from $ T_{C} $ to $ T_{H} $, bringing the system back to its original state. We continue to do work on the system, the volume decreases. To account for decreasing volume without heat flow, the system's temperature and pressure increase. We return to our original state and repeat the process.
	\end{enumerate}
	To be reversible, all heat transfer transfer must occur at constant temperature, while all temperature changes must occur in the absence of heat transfer. Because the Carnot engine exchanges heat with its surroundings in the isothermal steps and changes temperature in the adiabatic (absence of heat transfer) steps, the Carnot cycle is indeed reversible.

	\item At the end of the four steps, the system has returned to its original state, so $ \Delta E = 0 $ (because energy is a function of state). The net heat absorbed is $ Q_{H} - Q_{C} $ and the net work performed by the system is $ W $, and the first law tells us that
	\begin{equation*}
		Q_{H} - Q_{C} = W
	\end{equation*}
	where $ W $ is the work done \textit{by} the system.
	
	Naturally, we'd want to get as much work as possible with a minimum input of heat. With this in mind, we define the efficiency $ \eta $ of the engine as the ratio of the work done to the heat absorbed from the hot reservoir. Applying $ Q_{H} - Q_{C} = W $ we have
	\begin{equation*}
		\eta \equiv \frac{W}{Q_{H}} = \frac{Q_{H}-Q_{C}}{Q_{H}} = 1 - \frac{Q_{C}}{Q_{H}}
	\end{equation*}
	Conservation of energy and the first law imply a limit of $ \eta \leq 1$. $ \eta > 1 \implies W > Q_{H} $ would mean we get more energy out of the cycle from work than we inputted as heat. That's an obvious violation of conservation of energy! 
	
	Meanwhile, a cycle with $ \eta = 1 $ would have as its sole result the conversion of heat from a hot reservoir into work. This directly violates the Kelvin formulation of the second law. it follows that $ \eta < 1 $; we must content ourselves with depositing some heat $ Q_{C} $ back to the cold reservoir during each cycle. 
	 
	\item In fact, the Carnot cycle is the most efficient way to convert heat to work. This is formally stated in \textit{Carnot's theorem:}
	\begin{quote}
	 	Of all engines operating between two heat reservoirs, a reversible Carnot engine is the most efficient. More so, all reversible engines have the same efficiency $ \eta = \eta (T_{H}, T_{C}) $, which depends only on the temperatures of the reservoirs $ T_{H} $ and $ T_{C} $.
	\end{quote}
	Carnot's theorem can be proved with the Clausius formulation of the second law. I have a proof in the accompanying theory notes but left if out here, since it is not a part of the UL FMF curriculum.
\end{itemize}
	
	
\subsubsection{Carnot Cycle Temperature Scale}
\textit{Explain how the Carnot cycle provides a canonical choice for a thermodynamic temperature scale.}

\begin{itemize}
	\item Because the Carnot cycle depends only the reservoir temperatures $ T_{H} $ and $ T_{C} $ \textit{and not on the working substance}, we can use the Carnot cycle to define a universal temperature scale, which is, unlike the ideal gas scale,  independent of any specific material.
	
	\item The key is to consider two Carnot engines instead of just one. The first operates between two reservoirs with temperatures $ T_{1} > T_{2} $ and the second operates between reservoirs with temperatures $ T_{2} > T_{3} $. If the first engine extracts heat $ Q_{1} $ from the hot reservoir it must emit heat $ Q_{2} $. From the conservation of energy $ W = Q_{1} - Q_{2} $ and the Carnot definition of efficiency $ \eta = \frac{W}{Q_{1}} $ we have
	\begin{equation*}
		Q_{2} = Q_{1}(1 - \eta(T_{1}, T_{2}))
	\end{equation*}
	Note the efficiency $ \eta(T_{1}, T_{2}) $ is a function of only the reservoir temperatures $ T_{1} $ and $ T_{2} $ and nothing else.
	
	\item Analogously, the second engine operating between $ T_{2} $ and $ T_{3} $ absorbs the heat $ Q_{2} $ and emits the heat $ Q_{3} $; we have
	\begin{equation*}
		Q_{3} = Q_{2}(1 - \eta(T_{2}, T_{3})) = \left[Q_{1}(1 - \eta(T_{1}, T_{2}))\right](1 - \eta(T_{2}, T_{3}))
	\end{equation*}
	where we have plugged in $ Q_{2} = Q_{1}(1 - \eta(T_{1}, T_{2})) $ in the second equality.
	
	\item We can also consider both engines as a combined Carnot engine operating between $ T_{1} $ and $ T_{3} $. Such an engine has $ W = Q_{1} - Q_{3} $ and efficiency $ \eta(T_{1}, T_{3}) $ so that
	\begin{equation*}
		Q_{3} = Q_{1}(1 - \eta(T_{1}, T_{3}))
	\end{equation*}
	Equating the two expressions for $ Q_{3} $ and canceling $ Q_{1} $ gives
	\begin{equation*}
		1 - \eta(T_{1}, T_{3}) = (1 - \eta(T_{1}, T_{2}))(1 - \eta(T_{2}, T_{3}))
	\end{equation*}
	The key is recognizing that the left hand side is a function of only $ T_{1} $ and $ T_{3} $ (and not $ T_{2} $), so $ T_{2} $ must cancel in the right hand side. 
	
	\item For $ T_{2} $ to cancel from the right side, the first and second terms must be functions of the form 
	\begin{equation*}
		1 - \eta(T_{1}, T_{2}) = \frac{f(T_{2})}{f(T_{1})} \qquad \text{and} \qquad 1 - \eta(T_{2}, T_{3}) = \frac{f(T_{3})}{f(T_{2})} 
	\end{equation*}
	for some function $ f(T) $ so that their product $ \frac{f(T_{2})}{f(T_{1})}  \frac{f(T_{3})}{f(T_{2})} = \frac{f(T_{3})}{f(T_{1})}$ is not a function of $ T_{2} $, as desired. 
	
	\item The function $ f(T) $ is the temperature of the system. We could choose any monotonic function, but simplest and most natural choice is the identity function $ f(T) = T $. With the definition $ f(T) = T $ the efficiency of the Carnot cycle is 
	\begin{equation*}
		\eta = 1 - \frac{f(T_{C})}{f(T_{H})} \equiv 1 - \frac{T_{C}}{T_{H}} \implies  \frac{T_{C}}{T_{H}} = \frac{Q_{C}}{Q_{H}}
	\end{equation*}
	The scale $ \frac{T_{C}}{T_{H}} = \frac{Q_{C}}{Q_{H}} $ is sometimes called the \textit{thermodynamic temperature scale}. The Carnot temperature scale tells us that the ratio of any two temperatures equals the heats emitted and absorbed by a Carnot cycle operating between those temperatures. Since heat is a measurable quantity, this definition gives a way to measure temperature.
\end{itemize}

\subsubsection{Entropy}
\textit{What is entropy and how is it derived? Discuss the relationship between entropy and the second law of thermodynamics.}
\begin{itemize}
	\item First, a derivation of entropy. Applied to the Carnot cycle, the second law states that we cannot turn all the heat from the hot reservoir into work; we must emit some heat to the cold reservoir. From the Carnot cycle and thermodynamic temperature scale $ \frac{Q_{H}}{T_{H}} = \frac{Q_{C}}{T_{C}} $ we have
	\begin{equation*}
		\frac{Q_{H}}{T_{H}} - \frac{Q_{C}}{T_{C}} = 0
	\end{equation*}
	We now generalize our notation, so that heat $ Q $ is negative when the system releases heat and positive when the system absorbs heat, and we replace the indices $ H $ and $ C $ with $ 1 $ and $ 2 $. In this new convention we have $ Q_{2} = - Q_{C} $ (the system releases heat $ Q_{C} $) and the Carnot cycle is succinctly written
	\begin{equation*}
		\sum_{i=1}^{2} \frac{Q_{i}}{T_{i}} = 0
	\end{equation*}
	
	\item In fact, any reversible cycle, no matter the shape, can be equivalently constructed as sequence of (infinitesimally) small reversible isothermal and adiabatic segments (basically mini Carnot cycles), for which we have
	\begin{equation*}
		\sum_{i=1} \frac{Q_{i}}{T_{i}} = \frac{Q_{1}}{T_{1}} + \frac{Q_{2}}{T_{2}} + \cdots = 0
	\end{equation*}
	Summing all the infinitesimal contributions $\frac{Q_{i}}{T_{i}}$ along the path, we see the total heat absorbed in any reversible cyclic process must obey the relation
	\begin{equation*}
		\oint \frac{\dbar Q}{T} = 0
	\end{equation*}
	The result $ \oint \frac{\dbar Q}{T} = 0 $ means that if we reversibly change a system from state $ A $ to state $ B $, the quantity $ \int_{A}^{B} \frac{\dbar Q}{T} $ is independent of the path taken; it depends only on the initial and final state. In other words, the quantity $ \int \frac{\dbar Q}{T} $ is a new function of state. We call it entropy:
	\begin{equation*}
		S(A) = \int_{0}^{A} \frac{\dbar Q}{T} \eqtext{or} \diff S = \frac{\dbar Q}{T}
	\end{equation*}
	

\end{itemize}

\textbf{Entropy in Irreversible Processes}
\begin{itemize}
	\item In general, a heat engine operating between two temperatures $ T_{H} $ and $ T_{C} $ obeys
	\begin{equation*}
		\frac{Q_{H}}{T_{H}} - \frac{Q_{C}}{T_{C}} \leq 0
	\end{equation*}
	with an equality for reversible cycles and a strict inequality for irreversible cycles. This statement follows from Carnot's theorem and the second law; a proof is in the theory notes.
 
 	
	\item We can now breaking up the larger cycle into many infinitesimal reversible cycles, each contributing $ \frac{Q_{i}}{T_{i}} $. For the process we then have
	\begin{equation*}
		\sum_{i=1} \frac{Q_{i}}{T_{i}} = \frac{Q_{1}}{T_{1}} + \frac{Q_{2}}{T_{2}} + \cdots \leq 0
	\end{equation*}
	In the infinitesimal limit $ Q_{i} \to \dbar Q $, this generalizes to
	\begin{equation*}
		\oint \frac{\dbar Q}{T} \leq 0
	\end{equation*}
	This result holds for \textit{any} cyclic thermodynamic process, reversible or not, returning to its original state. We then generalize the definition entropy to 
	\begin{equation*}
		S(B) - S(A) \geq \int_{A}^{B} \frac{\dbar Q}{T} \eqtext{or} \diff S \geq \frac{\dbar Q}{T} 
	\end{equation*}
	which holds for all processes, reversible or not. 

\end{itemize}

\subsubsection{Implications of the Definition of Entropy}
\textit{State some implications of the second law of thermodynamics (of the definition of entropy). Show that the second law requires heat to flow from hot regions to cold regions and not vice versa.}

\smallskip
\textbf{Entropy of an Isolated System Increases}
\begin{itemize}
	\item Recall the definition of entropy
	\begin{equation*}
		\diff S \geq \frac{\dbar Q}{T} \eqtext{or} S(B) - S(A) \geq \int_{A}^{B} \frac{\dbar Q}{T} 
	\end{equation*}
	If the system is isolated from its environment then $ \dbar Q = 0 $. We then have
	\begin{equation*}
		S(B) - S(A) \geq 0  \implies S(B) \geq S(A)
	\end{equation*}
	In other words, the entropy of an \textit{isolated system} can never decrease in any thermodynamic process. More so, if the process is irreversible (and $ \geq $ is replaced with $ > $), entropy of an isolated system always increases.
\end{itemize}

\textbf{Heat Flows From High to Low Temperature}
\begin{itemize}
	\item Consider two bodies at temperatures $ T_{1} $ and $ T_{2} $ that are isolated from their surroundings. When the two bodies are placed in thermal contact, heat flows between the bodies. We denote the heat flowing into body 1 as $ Q_{1} $ and the heat flowing into body 2 as $ Q_{2} $ and choose a sign convention so that $ Q $ is positive if heat flows into a body and if $ Q $ is negative if heat flows out of a body. 
	
	\item The flow of heat from body 1 to body 2 and vice versa each have an associated change in entropy $ \Delta S_{1} = \frac{Q_{1}}{T_{1}} $ and $ \Delta S_{2} = \frac{Q_{2}}{T_{2}} $. Because entropy is additive, the total change in entropy is
	\begin{equation*}
		\Delta S = \Delta S_{1} + \Delta S_{2}
	\end{equation*}
	
	\item By the second law and definition of entropy, $ \Delta S > 0 $ for any reversible process, while the first law for an isolated system tells us that $ Q_{1} + Q_{2} = 0 $ or $ Q_{2} = - Q_{2} $. Putting these together gives
	\begin{equation*}
		\Delta S = \frac{Q_{1}}{T_{1}}  + \frac{Q_{2}}{T_{2}} = Q_{1} \left(\frac{1}{T_{1}} - \frac{1}{T_{2}}\right) > 0
	\end{equation*}
	There are two possibilities satisfying the inequality:
	\begin{itemize}
		\item $ T_{1} < T_{2} $ and $ Q_{1} > 0 $ (body 1 was at lower temperature and heat flowed into body 1)
		\item $ T_{2} < T_{1} $ and $ Q_{2} > 0 $ (body 2 was at lower temperature and heat flowed into body 2)
	\end{itemize}
	Evidently, heat flows from higher to lower temperature in each case. 
	
	\item Finally, for an analogous path to the same conclusion in the context of statistical mechanics, see Subsection \ref{sss:temperature_stat_phys}.
\end{itemize}


\subsubsection{Entropy of an Ideal Gas}
\textit{Derive the dependence of the entropy of an ideal gas on temperature and pressure and the dependence on temperature and volume.}
\begin{itemize}
	\item We start with the first law, writing $ \dbar Q = T \diff S $ and $ \dbar W = - p \diff V $ for an ideal gas
	\begin{equation*}
		\diff E = T \diff S - p \diff V \implies \diff S = \frac{\diff E}{T} + \frac{p\diff V}{T}
	\end{equation*}
	From Joule expansion (Subsection \ref{sss:joule_expansion}), we can write the internal energy of an ideal gas as $ E(T) = C_{V}T \implies \diff E = C_{V} \diff T $ while the ideal gas law gives $ \frac{p}{T} = Nk_{B} T $.
	
	Substituting these into the first law and rearranging gives
	\begin{equation*}
		\diff S = C_{V} \ln \frac{\diff T}{T} + Nk_{B}\frac{\diff V}{V}
	\end{equation*}
	Assuming $ C_{V} $ is constant, the expression can be integrated to give
	\begin{equation*}
		\Delta S =  C_{V}\frac{T_{2}}{T_{1}} + Nk_{B} \ln \frac{V_{2}}{V_{1}} 
	\end{equation*}
	which is the dependence of entropy on temperature and volume for an ideal gas.
	
	\item Alternatively, returning to $ \diff S = Nk_{B}\frac{\diff V}{V} + C_{V}\frac{\diff T}{T} $, and using the ideal gas law to express $ \frac{\diff V}{V} = \frac{\diff T}{T} - \frac{\diff p}{p} $ leads to
	\begin{equation*}
		\diff S = (Nk_{B} + C_{V})\frac{\diff T}{T} - Nk_{B}\frac{\diff p}{p}
	\end{equation*}
	which (assuming $ C_{V} $ is constant) can again be integrated to give 
	\begin{equation*}
		\Delta S = (Nk_{B} + C_{V}) \ln \frac{T_{2}}{T_{1}} - Nk_{B}\ln \frac{p_{2}}{p_{1}} = C_{p} \ln \frac{T_{2}}{T_{1}} - Nk_{B}\ln \frac{p_{2}}{p_{1}}
	\end{equation*}
	where $ C_{p} = Nk_{B} + C_{V} $ is heat capacity at constant pressure. This is the dependence of entropy on temperature and pressure for an ideal gas.
\end{itemize}


\subsection{Thermodynamic Potentials}

\subsubsection{Common Questions}
\begin{enumerate}
    \item How do we derive the expression for Helmholtz free energy? What is the physical meaning of Helmholtz free energy, and how does the free energy change as a system approaches thermodynamic equilibrium?

    \item What is Gibbs free energy? How do we derive the expression for Gibbs free energy? How does Gibbs free energy change as a system approaches equilibrium?

    \item State and derive Maxwell's relations for an ideal gas. How do the relations change for other thermodynamic systems?

    \item How do Maxwell's relations apply to a $ (p, V, T) $ system's coefficients of compressibility and thermal expansion? Derive the expression for the difference in heat capacities for an ideal gas using Maxwell's relations. Compare the order of magnitudes of the  difference of heat capacities for a gas and for a fluid.

\end{enumerate}


\subsubsection{Helmholtz Free Energy}
\textit{How do we derive the expression for Helmholtz free energy? What is the physical meaning of Helmholtz free energy, and how does the free energy change as a system approaches thermodynamic equilibrium?}

\begin{itemize}
	\item \textit{Helmholtz free energy} is defined as
	\begin{equation*}
		F = E - TS
	\end{equation*}
	Mathematically, $ F $ is a Legendre transform of internal energy $ E $ from the variable $ S $ to $ T $. This is best seen by comparing the total differentials
	\begin{equation*}
		\diff E = T \diff S - p \diff V \eqtext{and} \diff F = - S \diff T - p \diff V
	\end{equation*}
	and noting that $ \pdv{E}{S}S = TS $. From the total differential we see $ F = F(T, V) $ is naturally a function of temperature and volume, and is thus best suited to describing a system at constant temperature and volume. In this interpretation we can immediately deduce the relationships
	\begin{equation*}
		\pdveval{F}{S}{V} = - S \qquad \text{and} \qquad 	\pdveval{F}{S}{T} = - p
	\end{equation*}
	
	\item Physically, free energy is the amount of the system's energy available to perform work at the fixed temperature $ T $. To see this, consider taking a system through a reversible isothermal process from state $ A $ to state $ B $. For an isothermal process $ \diff T = 0 $, so $ \diff F = - p\diff V $ and the change in free energy during the process is
	\begin{equation*}
		F(B) - F(A) = \int_{A}^{B} \diff F = - \int_{A}^{B} p \diff V = W = -W_{sys}
	\end{equation*}
	where $ W_{sys} $ is the work done \textit{by} the system during the process. In the form $ F_{B} = F_{A} - W_{sys} $, we see a system's free energy decreases in any isothermal process in which the system performs work. This leads to the physical meaning of free energy: the amount of the system's energy available to perform work at the fixed temperature.
	

	
	\item A system at constant temperature and volume will always settle into the equilibrium state minimizing its free energy $ F $. A derivation follows below:
	
	Consider a primary system in contact with a large heat reservoir with fixed temperature and volume. The primary system has energy $ E $, the reservoir has energy $ E_{R} \gg E $ and the total energy $ E_{tot} = E + E_{R} $ of the combined system is fixed.  The entropy of the combined system is
	\begin{equation*}
		S_{tot}(E_{tot}) = S_{R}(E_{R}) + S(E) = S_{R}(E_{tot} - E) + S(E)
	\end{equation*}
	Because $ E_{tot} \gg E $, we can Taylor expand $ S_{R}(E_{tot} - E) $ about $ E_{tot} $ to get
	\begin{align*}
		S_{tot}(E_{tot}) &\approx S_{R}(E_{tot}) - E \pdv{S_{R}}{E_{tot}} + S(E) = S_{R}(E_{tot}) - \frac{E}{T} + S(E) \\
		&= S_{R}(E_{tot}) - \frac{F}{T}
	\end{align*}
	where $ \pdv{S_{R}}{E_{tot}} = \frac{1}{T} $ (known from statistical mechanics, or from the partial derivative of the first law $ \diff E = T \diff S - p \diff V $ with respect to $ E $, which gives $ 1 = T \pdv{S}{E} $).
	
	Next, examine the equality $ S_{tot}(E_{tot}) = S_{R}(E_{tot}) - \frac{F}{T} $. We know from the second law that system's total entropy $ S_{tot} $ can never decrease, which means the system's free energy $ F $ can never increase (this would cause $ S_{tot} $ to decrease). It follows that the system settles into an equilibrium position minimizing its free energy.
	
	In summary, an isolated system's total entropy can never decrease while its free energy can never increase. A system at constant temperature and volume will always settle into the equilibrium state minimizing its free energy and maximizing entropy.
	

\end{itemize}


\subsubsection{Gibbs Free Energy} \label{sss:gibbs_free_energy}
\textit{What is Gibbs free energy? How do we derive the expression for Gibbs free energy? How does Gibbs free energy change as a system approaches equilibrium?}

\begin{itemize}
	\item Gibbs free energy is defined as
	\begin{equation*}
		G = F + pV = E - TS + pV
	\end{equation*}
	Mathematically, $ G $ is a Legendre transform of $ F $ from the variable $ V $ to $ p $. Again, this is best seen by comparing the total differentials
	\begin{equation*}
		\diff F = - S \diff T - p \diff V \eqtext{and} \diff G = - S \diff T + V \diff p
	\end{equation*}
	and noting that $ \pdv{F}{V}V = - p V $. From the total differential we see $ G = G(T, p) $ is naturally a function of temperature ant pressure and is thus best suited to describing a system at constant temperature and pressure.
	
	\item A system at constant temperature and pressure will always settle into the equilibrium state minimizing its Gibbs free energy $ G $. A derivation follows below:
	
	Consider a primary system in contact with a large reservoir at temperature $ T $. The volume of each system can fluctuate but the total volume of the combined system and reservoir remains fixed. The system has energy $ E $ and volume  $ V $, the reservoir energy $ E_{R} \gg E $ and volume $ V_{R} \gg V $, and the total energy and volume is $ E_{R} = E_{tot} - E $ and $ V_{R} = V_{tot} - V $. The total entropy is
	\begin{equation*}
		S_{tot}(E_{tot}, V_{tot}) = S_{R}(E_{tot} - E, V_{tot} - V) + S(E, V)
	\end{equation*}
	Taylor expanding $ S_{R}(E_{tot}- E), V_{tot} - V $ about $ E_{tot} $ and $ V_{tot} $ gives
	\begin{equation*}
		S_{tot}(E_{tot}, V_{tot}) \approx S_{R}(E_{tot}, V_{tot}) - E\pdv{S_{R}}{E_{tot}} - V\pdv{S_{R}}{V_{tot}} + S(E, V)
	\end{equation*}
	Recognizing $ \pdv{S}{E} = \frac{1}{T}  $ and $ \pdv{S}{V} = \frac{p}{T} $ we get
	\begin{equation*}
		S_{tot}(E_{tot}, V_{tot}) = S_{R}(E_{tot}, V_{tot}) - \frac{E + pV - TS}{T} = S_{R}(E_{tot}, V_{tot}) - \frac{G}{T}
	\end{equation*}
	From the second law that system's total entropy $ S_{tot} $ can never decrease, which means the system's Gibbs free energy $ G $ can never increase (this would cause $ S_{tot} $ to decrease) so the system settles into an equilibrium position minimizing $ G $.
		
\end{itemize}

\subsubsection{Maxwell's Relations}
\textit{State and derive Maxwell's relations for an ideal gas. How do the relations change for other thermodynamic systems?}

\smallskip

Maxwell's relations are useful relationships between the thermodynamic potentials $ E, F, G $ and $ H $. They come from equating the potentials' mixed second partial derivatives.
\begin{itemize}
	\item We start with internal energy, whose total differential
	\begin{equation*}
		 \diff E = T \diff S - p \diff V 
	\end{equation*}
	suggests energy is most naturally viewed as a function of energy and volume. In the interpretation $ E = E(S, V) $, the partial derivatives are the familiar quantities
	\begin{equation*}
		\pdveval{E}{S}{V} = T \qquad \text{and} \qquad \pdveval{E}{V}{S} = - p
	\end{equation*}
	Next, we equate the mixed second partial derivatives to get
	\begin{equation*}
		\pdv{E}{S}{V} \equiv \pdv{E}{V}{S} \implies \pdveval{T}{V}{S} = - \pdveval{p}{S}{V}
	\end{equation*}
	The resulting equality 
	\begin{equation*}
		\pdveval{T}{V}{S} = - \pdveval{p}{S}{V}
	\end{equation*}
	is called \textit{Maxwell's first relation}. The remaining Maxwell's relations come from considering each of the remaining thermodynamic potentials in turn.
	
	\item For (Helmholtz) free energy $ F $, the total differential
	\begin{equation*}
		\diff F = - p \diff V - S \diff T
	\end{equation*}
	tells us free energy is a function of $ V $ and $ T $; the interpretation $ F = F(V, T) $ gives
	\begin{equation*}
		\pdveval{F}{V}{T} = - p \qquad \text{and} \qquad \pdveval{F}{T}{V} = - S
	\end{equation*}
	Equating the partial derivatives $ \pdv{F}{V}{T} \equiv \pdv{F}{T}{V} $ produces the \textit{second Maxwell relation}
	\begin{equation*}
		\pdveval{p}{T}{V} = \pdveval{S}{V}{T}
	\end{equation*}
	
	\item Next, Gibbs free energy: the total differential 
	\begin{equation*}
		\diff G = V\diff p - S \diff T
	\end{equation*}
	tells us $ G $ is a function of $ p $ and $ T $. The interpretation $ G = G(p, T) $ gives
	\begin{equation*}
		\pdveval{G}{T}{p} = - S \qquad \text{and} \qquad \pdveval{G}{p}{T} = V
	\end{equation*}
	Equating the partial derivatives $ \pdv{G}{T}{p} \equiv \pdv{G}{p}{T} $ produces the \textit{third Maxwell relation}
	\begin{equation*}
		\pdveval{S}{p}{T} = -\pdveval{V}{T}{p}
	\end{equation*}
	
	\item Finally, enthalpy: the total differential is
	\begin{equation*}
		\diff H = T \diff S + V \diff p 
	\end{equation*}
	so enthalpy is most naturally a function of $ S $ and $ p $; writing $ H = H(S, p) $ gives
	\begin{equation*}
		\pdveval{H}{S}{p} = T \qquad \text{and} \qquad \pdveval{H}{p}{S} = V
	\end{equation*}
	Equating the partial derivatives $ \pdv{H}{S}{p} \equiv \pdv{H}{p}{S} $ produces the \textit{fourth Maxwell relation}
	\begin{align*}
		\pdveval{T}{p}{S} = \pdveval{V}{S}{p} 
	\end{align*}
	
	\item Maxwell's relations are particularly useful because they equate entropy, which is difficult to measure experimentally, to the  state variables $ (p, V) $ and temperature $ T $, which are easy to measure or determine from the system's equation of state. Note that cross-multiplication of Maxwell's relations always yields $ pV = \pm TS $
	
	Analogous versions of Maxwell's relations hold for all thermodynamic systems; we just have to change the state variables and expression for work $ \dbar W $ accordingly. This leads to new expressions for the thermodynamic potentials $ \diff E, \diff F, \diff G $ and $ \diff H $. Once we know the thermodynamic potentials in terms of the new state variables, the procedure is analogous to that shown above for an ideal gas; we just equate mixed partial derivatives of each thermodynamic potential.
	
\end{itemize}


\subsubsection{Applications of Maxwell's Relations to $ (p, V, T) $ Systems} \label{sss:applications_max_rel}
\textit{How do Maxwell's relations apply to a $ (p, V, T) $ system's coefficients of compressibility and thermal expansion? Derive the expression for the difference in heat capacities for an ideal gas using Maxwell's relations. Compare the order of magnitudes of the  difference of heat capacities for a gas and for a fluid.}

\vspace{4mm}

\textbf{Compressibility and Thermal Expansion}
\begin{itemize}
	\item The general equation of state for a $ (p, V, T) $ system is  (see end of Subsection \ref{sss:td:eq_state})
	\begin{equation*}
		\frac{\diff V}{V} = \alpha \diff T - \chi_{T} \diff p
	\end{equation*}
	where $ \alpha $ is the coefficient of thermal expansion and $ \chi_{T} $ is the coefficient of isothermal compressibility. For situations with $ \diff V = 0 $, we can rearrange the equation and use Maxwell's second relation $ \pdveval{p}{T}{V} = \pdveval{S}{V}{T}$ to get
	\begin{equation*}
		\pdveval{p}{T}{V} = \frac{\alpha}{\chi_{T}} = \pdveval{S}{V}{T}
	\end{equation*}
	Meanwhile, for situations with $ \diff p = 0$, we can rearrange the equation and use Maxwell's third relation $  \pdveval{V}{T}{p} = -\pdveval{S}{p}{T} $ to get
	\begin{equation*}
		\pdveval{V}{T}{p} = \alpha V = - \pdveval{S}{p}{T}
	\end{equation*}
\end{itemize}

\vspace{-2mm}
\textbf{Adiabatic Changes}
\begin{itemize}
	\item Viewing $ S = S(T, V) $ and using $ C_{V} = \pdveval{E}{T}{V} = T\pdveval{S}{T}{V} $ and $ \pdveval{S}{V}{T} = \frac{\alpha}{\chi_{T}} $ gives
	\begin{equation*}
		\diff S = \pdveval{S}{T}{V}\diff T + \pdveval{S}{V}{T}\diff V = \frac{C_{V}}{T}\diff T + \frac{\alpha}{\chi_{T}}\diff V
	\end{equation*}
	Viewing $ S = S(T, p) $ and using $ C_{p} = \pdveval{H}{T}{p} = T\pdveval{S}{T}{p} $ and $ \pdveval{S}{p}{T} = - \alpha V $ gives
	\begin{equation*}
		\diff S = \pdveval{S}{T}{p}\diff T + \pdveval{S}{p}{T}\diff p = \frac{C_{p}}{T}\diff T - \alpha V\diff p
	\end{equation*}
	Finally, viewing $ S = S(p, V) $ gives the total differential
	\begin{equation*}
		\diff S = \pdveval{S}{p}{V} \diff p + \pdveval{S}{V}{p} \diff V
	\end{equation*}
	Multiplying the first equation (derived from $ S = S(T, V) $) by the ratio of heat capacities $ \kappa = \frac{C_{p}}{C_{V}} $ and subtracting the second equation (from $ S = S(T, p) $) gives
	\begin{equation*}
		(\kappa - 1) \diff S = \frac{\kappa \alpha}{\chi_{T}} \diff V + \alpha V \diff p \implies \diff S = \frac{\alpha}{\kappa - 1}\left(\frac{\kappa}{\chi_{T}}\diff V + V \diff p \right)
	\end{equation*}
	For adiabatic processes with $ \diff S = 0 $, we can rearrange the last result to get
	\begin{equation*}
		\left(\frac{\kappa}{\chi_{T}}\diff V - V \diff p\right) = 0 \implies - \frac{1}{V}\pdveval{V}{p}{S} = \frac{\chi_{T}}{\kappa}
	\end{equation*}
	The quantity on the left is called \textit{adiabatic compressibility}, defined as
	\begin{equation*}
		\chi_{S} = - \frac{1}{V}\pdveval{V}{p}{S}
	\end{equation*}
	and represents a system's change in volume in response to applied pressure for adiabatic processes. It is related to isothermal compressibility by $ \chi_{S} = \frac{\chi_{T}}{\kappa}$.
\end{itemize}	

\smallskip
\textbf{Difference in Heat Capacities}
\begin{itemize}	
	\item To find the difference in heat capacities for a $ (p, V, T) $ system, we write $ S = S(T, V(T, p)) $
	\begin{equation*}
		\pdveval{S}{T}{p} = \pdveval{S}{T}{V} + \pdveval{S}{V}{T} \pdveval{V}{T}{p}
	\end{equation*}
	Using $ C_{V} = T \pdveval{S}{T}{V} $ and $ C_{p} = T \pdveval{S}{T}{p} $ and the second relation $ \pdveval{p}{T}{V} = \pdveval{S}{V}{T}$ gives
	\begin{equation*}
		C_{p} - C_{V} = T \pdveval{S}{V}{T} \pdveval{V}{T}{p} = T \pdveval{p}{T}{V} \pdveval{V}{T}{p}
	\end{equation*}
	For a general $ (p, V, T) $ system, $ \pdveval{V}{T}{p} = \alpha V $ and $ \pdveval{p}{T}{V} = \frac{\alpha}{\chi_{T}} $ giving
	\begin{equation*}
		C_{p} - C_{V} = \frac{T\alpha^{2}V}{\chi_{T}} \eqtext{or} c_{p} - c_{V} = \frac{T\alpha^{2}}{\rho \chi_{T}}
	\end{equation*}
	where $ \rho = \frac{m}{V} $ is density. For an ideal gas, the result reduces to $ C_{p}- C_{V} = Nk_{B} $.
\end{itemize}

\smallskip
\textbf{Joule-Kelvin Coefficient}
\begin{itemize}
	\item The Joule-Kelvin experiment and coefficient are discussed in Subsection \ref{sss:jtk_experiment}, where the Joule-Kelvin coefficient was defined as $ \pdveval{T}{p}{H} $. We'll derive a new expression for the coefficient using Maxwell's relations, starting with 
	\begin{equation*}
		\diff H = T \diff S + V \diff p
	\end{equation*}
	writing $ \diff S = \frac{C_{p}}{T} \diff T - \alpha V \diff p  $ (see the adiabatic changes section just above) gives
	\begin{equation*}
		\diff H = T \left(\frac{C_{p}}{T} \diff T - \alpha V \diff p\right) + V \diff p = C_{p}\diff T - V(\alpha T - 1)\diff p
	\end{equation*} 
	For the Joule-Kelvin experiment where $ \diff H = 0 $ we get
	\begin{equation*}
		C_{p}\diff T - V(\alpha T - 1)\diff p = 0 \implies \pdveval{T}{p}{H} = \frac{V}{C_{p}}(\alpha T - 1) = \frac{\alpha T - 1}{\rho c_{p}}
	\end{equation*}
\end{itemize}


\subsection{Phase Changes}

\subsubsection{Common Questions}
\begin{enumerate}
    \item What is a phase change? Give examples of properties of matter that change during phase transitions and can distinguish different phases.

    \item Discuss the van der Waals equation as pertains to the liquid-gas phase change. Draw the corresponding isotherms and explain which region corresponds to a gas state and which to a liquid state. Explain which regions are metastable and which are non-physical.

    \item What are the conditions for phase equilibrium? Explain and derive the Maxwell construction for phase changes.

    \item Explain the difference between continuous and non-continuous phase changes. Discuss how enthalpy, Gibbs free enthalpy and specific heat at constant pressure change during both types of phase changes and explain the differences in behavior.

    \item Explain and derive the Clausius-Clapeyron equation. What is saturated vapor pressure, and what saturated vapor pressure does the equation predict for temperatures far below the critical temperature? 

\end{enumerate}


\subsubsection{Introduction to Phase Changes}
\textit{What is a phase change? Give examples of properties of matter that change during phase transitions and can distinguish different phases.}

\begin{itemize}
	\item A phase transition is an abrupt, discontinuous change in the properties of a system. Classic examples are liquid water freezing to ice or steam condensing to water.  
	
	Some examples of properties of matter that change during phase transitions are
	\begin{itemize}
		\item Symmetry of the molecular structure (e.g. when changing from a solid to liquid state)
		\item Density
		\item Magnetic or electric properties (e.g. transition from a paramagnetic to a ferromagnetic phase)
	\end{itemize}
	
	\item Molecular structure in water:  In the solid state the molecules are arranged in a crystal lattice and the positions of individual atoms are on average well-defined. In the liquid state the molecular structure breaks up while the density remains roughly the same as in the solid phase. Meanwhile, in water's liquid-gas transition, molecular symmetry remains roughly the same in both states, while density changes considerably.
	
	\item Ferroelectric substance: In the low-temperature state, we imagine the crystal lattice as a  square cell with negatively charged ions at each corner and a positively charged ion slightly displaced from the center. The positive charge cancels the negative charges and the cell is electrically neutral. But because the positive charge is slightly displaced from the center the substance has a net electric dipole moment, so the substance is electrically polarized even in the absence of an external electric field.
	
	As temperature increases, the positive charge falls into the center of the cell and the electric dipole moment and polarization vanish. In this case the change in electric properties are just a consequence of a change in the crystal structure.
	

	
\end{itemize}

\subsubsection{Liquid-Gas Phase Change, Revisited} \label{sss:liq_gas_phase_change}
\textit{Discuss the van der Waals equation as pertains to the liquid-gas phase change. Draw the corresponding isotherms and explain which region corresponds to a gas state and which to a liquid state. Explain which regions are metastable and which are non-physical.}

\iffalse
\begin{itemize}

	\item The van der Waals isotherms fall into three regimes classified by the value of temperature $ T $. These are
	\begin{enumerate}
		\item For very high temperatures the isotherms are monotonically decreasing functions scaling approximately as $ p \sim \frac{1}{v} $, just like the isotherms of an ideal gas. This is expected---most interacting gases begin to behave like an ideal gas in the high temperature regime.
				
		\item At very low temperatures (when both terms in the van der Waals equation contribute comparably, roughly for temperatures where $ k_{B}T \sim \frac{a}{v} $), the $ (v, p) $ curve has a wiggle. You basically need an actual graph in front of you to visualize this. The wiggle represents an highly unstable, non-physical state; we will discuss it more detail shortly.
				
		\item At an intermediate \textit{critical temperature}  $ T = T_{c} $, the maximum and minimum of the wiggle meet, and the wiggle flattens out to form an inflection point. This occurs where $ \dv{p}{v} = \dv[2]{p}{v} = 0 $ and is satisfied only for $ T = T_{c} $ given by
		\begin{equation*}
			k_{B}T_{c} = \frac{8a}{27b}
		\end{equation*}
		Along the way, we also get the critical volume $ v_{c} $ and pressure $ p_{c} $; these are
		\begin{equation*}
			v_{c} = 3b \qquad \text{and} \qquad p_{c} = \frac{a}{27b^{2}}
		\end{equation*}
	\end{enumerate}
	
	\item The non-physical and metastable states occur in the low temperature $ T < T_{c} $ isotherm. 	
	
	The region with $ \pdv{p}{v} > 0 $ is non-physical. Positive slope $ \pdv{p}{v} $ means that if we increase the gas's volume, the pressure increases. Alternatively, if we decrease the gas's volume (e.g. by compression) the pressure decreases! Such a system has negative compressibility, which is non-physical. 
	
	I'll discuss meta-stable states more in the Subsection \ref{sss:liq_gas_phase_change}, where I think the context is better. But to give a quick answer, meta-stable state are the regions of the van der Waals isotherm between the equilibrium pressure at which the gaseous and liquid phases coexist and the non-physical region with $ \pdv{p}{v} > 0 $. The equilibrium pressure comes from the Maxwell construction of the liquid-gas phase transition, discussed in the section on phase transitions.

\end{itemize}
\fi


\begin{itemize}
	\item For the van der Waals equation, \textit{isotherms} are $ p $ vs $ v $ curves ($ p $ on the ordinate and $ v $ on the abscissa) evaluated at fixed values of temperature $ T $; think of temperature parametrizing the $ (v, p) $ curves. (In general, isotherms are curves of a system's state variables at constant temperature $ T $; for the van der Waals equation the state variables are $ p $ and $ v $).
	
	\item 	\textit{Note: this discussion is much easier to visualize if you have a graph of a few van der Waals isotherms in front of you.}
	
	The van der Waals isotherms are best analyzed with the equation in the form
	\begin{equation*}
		p = \frac{k_{B}T}{v-b} - \frac{a}{v^{2}}
	\end{equation*}
	First, note that $ v $ is always larger than the parameter $ b = \frac{2\pi r_{0}^{3}}{3} $ and pressure approaches infinity as $ v $ approaches $ b $ from the left. This makes sense; $ b $ represents the volume of a particle, and the volume per particle $ v $ cannot be smaller than the volume of each particle $ b $.
				
	At large $ v $ (and thus low density $ \rho = \frac{1}{v} $) and high $ T $, the $ p $ versus $ v $ isotherms gently decrease monotonically as $ p \sim \frac{1}{v} $, just like the isotherms of an ideal gas. The gentle slope $ \abs{\dv{p}{v}} $ means only a small amount of pressure is needed for an appreciable change in volume: the substance is highly compressible, like a gas. \textit{The region of small slope $ \abs{\dv{p}{v}} $ at high $ v $ and high $ T $ represents a gaseous state.}
			
	For small $ v $ as $ v $ approaches $ b $ from the right, pressure steeply increases as $ p(v) \sim \frac{1}{v - b} $. The steep slope $ \abs{\dv{p}{v}} $ means a great deal of pressure is required for only a small change in volume: the material is nearly incompressible, like a liquid. \textit{The region of large slope $ \abs{\dv{p}{v}} $ at low $ v $ corresponds to a liquid state.}
		
	\item At intermediate $ v $ and temperatures below a critical temperature $ T = T_{c} $, the isotherm has a wiggle, which corresponds to the region of phase transition. The part of the wiggle region with $ \pdv{p}{v} > 0 $ is non-physical. Positive slope $ \pdv{p}{v} $ means that if we increase the gas's volume, the pressure increases. Alternatively, if we decrease the gas's volume (e.g. by compression) the pressure decreases! Such a system has negative compressibility, which is non-physical. 
	
	\item The parts of the isotherm between the the equilibrium pressure at which the gaseous and liquid phases coexist and the non-physical region with $ \pdv{p}{v} > 0 $ correspond to meta-stable states. The equilibrium pressure comes from the Maxwell construction of the liquid-gas phase transition, discussed shortly. Meta-stable states are physically possible, but are unstable.
	
	Matter in the gaseous phase can be compressed very gently beyond the coexistence curve into a meta-stable state of high density called a super-cooled vapor, but any small disturbance will cause some amount of the gas to condense into the liquid phase. 
	
	Similarly, matter in the liquid phase can be expanded very gently beyond the coexistence curve into a meta-stable state of low density called a super-heated liquid, and any small disturbance will cause some amount of the liquid to evaporate into the gas phase. 

\end{itemize}

\iffalse %TODO clean up maybe
\subsubsection{Phase Diagrams for Fluids}
\textit{Discuss the a typical phase diagram of a fluid. What is the triple point and what is the critical point? How is the phase diagram for water unique for fluids?}

\begin{itemize}
	\item Here's a typical fluid phase diagram. At low pressure and high temperature the substance is in a gaseous phase. At low temperatures and high pressures the substance is in a solid phase. At intermediate pressures and temperatures, the substance is in a liquid phase.
	
	Phase diagrams have lines separating regions of different phases. The phase changes upon crossing these lines. 
	
	\item A critical point is a point beyond which we can no longer distinguish between a liquid and gaseous state. In this region, density changes continuously with temperature and pressure. The existence of the critical point is possible because the fluid and liquid state are roughly equally symmetric.
	
	At a triple point is a point on the phase diagram where all three phases coexist in equilibrium. 
	
\end{itemize}
\fi

\subsubsection{Phase Equilibrium and the Maxwell Construction} \label{sss:liquid_gas_phase_equil}
\textit{What are the conditions for phase equilibrium? Explain and derive the Maxwell construction for phase changes.}

\begin{itemize}
	
	\item Recall the wiggle region of the van der Waals isotherm for $ T < T_{c} $ corresponds to phase transition. We start with a conceptual step and assume that in the transition region: part of the system could be in the liquid phase and part of the system could be in the gas phase. How to confirm the stable coexistence of two phases in the same system is allowed? 
	
	\item For a system to be stable, it must be in \textit{thermal} and \textit{mechanical} equilibrium. This occurs in the liquid-gas phase transition if the gaseous and liquid phases have the same temperatures and pressures. The temperature condition is satisfied by default: the two solutions lie on the same isotherm. For the pressure condition we require 
	\begin{equation*}
		p_{g} = p_{l}
	\end{equation*}
	
	\item A constant pressure, constant temperature situation is best suited to Gibbs free energy $ G $. If we allow for fluctuations of particle number $ N $, we have $ G = G(p, T, N) $, and the total differential is
	\begin{equation*}
		\diff G = - S \diff T + V \diff p + \mu_{g}\diff N_{g} + \mu_{l} \diff N_{l}
	\end{equation*}
	where the chemical potential $ \mu = \pdveval{G}{N}{T,p}$ represents the amount of energy needed to add more particles to the system (we'll discuss it much more in statistical mechanics). 
	
	Because we require $ \diff T = \diff p = 0 $ for thermal and mechanical equilibrium we get
	\begin{equation*}
		\diff G = \mu_{g}\diff N_{g} + \mu_{l} \diff N_{l}
	\end{equation*}
	Next, because the total number of particles $ N = N_{g} + N_{l} $ in the system is constant we have $ \diff N = 0$
	\begin{equation*}
		0 = \diff N = \diff N_{g} + \diff N_{l} \implies \diff N_{g} = -\diff N_{l}
	\end{equation*}
	Substituting $ \diff N_{g} = -\diff N_{l} $ into the Gibbs free energy differential gives
	\begin{equation*}
		\diff G = (\mu_{l} - \mu_{g})\diff N_{l}
	\end{equation*}
	Finally, because equilibrium for a constant temperature and constant pressure system occurs in a state of minimum $ G $  (see Subsection \ref{sss:gibbs_free_energy}), we require Gibbs free energy cannot increase i.e. $ \diff G \leq 0 $. Putting this together gives
	\begin{equation*}
		\diff G = (\mu_{l} - \mu_{g})\diff N_{l} \leq 0
	\end{equation*}
	The inequality is satisfied by two cases:
	\begin{itemize}
		\item $ \mu_{l} > \mu_{g} $ and $ \diff N_{l} < 0$ the chemical potential of the liquid is larger than for the gas, and the number of liquid particles decreases
		\item $ \mu_{l} < \mu_{g} $ and $ \diff N_{l} > 0$ the chemical potential of the liquid is less than for the gas, and the number of liquid particles increases
	\end{itemize}
	The number of particles in the liquid and gas phases balance out until an equilibrium state with $ \mu_{g} = \mu_{l} $ is reached. The condition
	\begin{equation*}
		\mu_{g} = \mu_{l}
	\end{equation*}
	is called \textit{chemical equilibrium}. From the convenient relationship between chemical potential and Gibbs free energy $ G = \mu N $, the chemical equilibrium condition is often written
	\begin{equation*}
		\frac{G_{g}}{N_{g}} = \frac{G_{l}}{N_{l}} \implies g_{g} = g_{l}
	\end{equation*}
	where $ g = \frac{G}{N} $ is the Gibbs free energy per particle.
		
	Finally, note the equilibrium conditions involve only intensive quantities: $ T $, $ p $, and $ \mu $, which essentially means that our equilibrium arguments apply to any size system.
		
	\item The chemical equilibrium condition $ \mu_{g} = \mu_{l} $ leads to the Maxwell construction for chemical equilibrium, which replaces the phase transition wiggle region of the the van der Waals isotherm with a straight line of constant pressure. 
	
	
	From $ G(p, T, N) = \mu(p, T)N $ we get the expression $ \pdveval{G}{p}{T, N} = \pdveval{\mu}{p}{T} N $ while from $ \diff G = -S \diff T + V \diff p + \mu \diff N $ we have 	$ \pdveval{G}{p}{T, N} = V(p) $. Equating the expressions for $ \pdveval{G}{p}{T, N} $ gives an expression for chemical potential
	\begin{equation*}
		\pdveval{\mu}{p}{T} = \frac{V(p)}{N} \implies \mu = \int \frac{V(p)}{N} \diff p
	\end{equation*}
	Starting in the liquid phase at $ \mu_{l} $ and integrating along the isotherm to $ \mu_{g} $ gives 
	\begin{equation*}
		\mu_{g} = \mu_{l} + \int_{p_{l}}^{p_{g}} \frac{V(p)}{N}\diff p 
	\end{equation*}
	Meanwhile, the chemical equilibrium condition $ \mu_{g} = \mu_{l}$ requires $ \mu_{g} = \mu_{l} $ or
	\begin{equation*}
		\mu_{g} = \mu_{l} + \int_{p_{l}}^{p_{g}} \frac{V(p)}{N}\diff p  \equiv \mu_{l} \implies  \int_{p_{l}}^{p_{g}} \frac{V(p)}{N}\diff p = 0
	\end{equation*}
	In other words, the liquid-gas phase transition must occur at the the unique pressure $ p^{*} $ satisfying  $ p^{*} = p_{g} = p_{l} $.  Geometrically, the equilibrium pressure $ p^{*} $ is such that the areas above and below the wiggle in the isotherm have equal areas. This equal area condition is called the \textit{Maxwell construction} for chemical equilibrium and implicitly defines the pressure at which the gaseous and liquid phases can coexist. We then replace the wiggle region of the van der Waals isotherm with a line of constant pressure $  p^{*} = p_{g} = p_{l} $.
	 
\end{itemize}

\subsubsection{Continuous and Non-Continuous Phase Changes}
\textit{Explain the difference between continuous and non-continuous phase changes. Discuss how enthalpy, Gibbs free enthalpy and specific heat at constant pressure change during both types of phase changes and explain the differences in behavior.}

\begin{itemize}
	\item The \textit{order} of a phase transition is classified by the continuity of the relevant thermodynamic potentials involved in the phase change. In an $ n $-th order phase transition, the $ n $th derivative of a relevant thermodynamic potential is discontinuous over the transition. For example, in a first-order phase transition the first derivative of a relevant thermodynamic potential is discontinuous; in a second-order transition the second derivative is discontinuous, etc..
	
	\item Second-order phase transitions, which have smooth thermodynamic potentials, are sometimes called \textit{continuous} phase transitions, while first-order phase transitions, in which the thermodynamic potential's derivative has a discontinuity, are called \textit{discontinuous} phase transitions.
	
	\item A quick aside: \textit{specific latent heat} $ L $, defined as
	\begin{equation*}
		L = T(s_{g} - s_{l})
	\end{equation*}
	is a convenient quantity measuring the energy released per particle when passing through a phase transition. Latent heat is commonly used when describing phase transitions.
	
	\item Because second-order phase transitions do not release latent heat, there is not change in entropy in the transition and $ \Delta S = 0 $. Meanwhile, first-order transitions with latent heat have a corresponding change in entropy, and the entropy of the higher-temperature phase is always larger.
	
	The liquid-gas transition releases latent heat which means the entropy $ S = -\pdveval{G}{T}{p} $ is discontinuous. Alternatively, we can say the system's volume $ V = \pdveval{G}{p}{T} $ is discontinuous in the transition from liquid to gas. In either case, the first derivative of the thermodynamic potential $ G $ is discontinuous, so the liquid-gas transition is a first-order transition. 
	
	From the relationships $ \pdveval{H}{S}{p} = T $ and $ S = - \pdveval{G}{T}{p} $, the latent heat $ N L $ in a first-order liquid-gas transition at temperature $ T^{*} $ is
	\begin{equation*}
		N L = H_{g} - H_{l} = T^{*}(S_{g} - S_{l}) = T^{*} \left[\pdv{G_{l}}{T} - \pdv{G_{g}}{T} \right]_{T=T^{*}}
	\end{equation*}
	
\end{itemize}


\subsubsection{The Clausius-Clapeyron Equation}
\textit{Explain and derive the Clausius-Clapeyron equation. What is saturated vapor pressure, and what saturated vapor pressure does the equation predict for temperatures far below the critical temperature? }

\begin{itemize}

	\item Saturated vapor pressure $ p_{s}(T) $ is the pressure at which the liquid and gas phases are in equilibrium, corresponding to the straight line of constant pressure in the Maxwell construction. The saturated vapor pressure for a given transition is a function of temperature only. The relationship between a transition's saturated vapor pressure and temperature is given by the Clausius-Clapeyron equation.
	
	\item The line line plotting equilibrium pressure versus temperature in the $ (T, p) $ plane separating the liquid and gas phases marks the presence of a phase transition. On either side of the line, all particles are either all in the liquid or all in the gas phase. Recall the chemical equilibrium condition $ \mu_{g} = \mu_{l} $ means the Gibbs free energies per particle of both states are equal, i.e.
	\begin{equation*}
		 g_{g} = g_{l} 
	\end{equation*}
	This means $ G $ is continuous as we cross the line of phase transitions. 
	
	\item Next, instead of crossing the phase transition line, suppose we move along the line itself. How does $ g $ change? From the total differential $ \diff G = -S \diff T + V\diff p  $ we have
	\begin{equation*}
		\diff g = - s \diff T + v \diff p
	\end{equation*}
	where $ s = \frac{S}{N}$ and $ v = \frac{V}{N} $ are the entropy and volume per particle, respectively. Applying the chemical equilibrium condition $ g_{g} = g_{l}  $ we have
	\begin{equation*}
		g_{g} = g_{l} \implies - s_{l} \diff T + v_{l} \diff p = - s_{g} \diff T + v_{g} \diff p
	\end{equation*}
	Rearranging and solving for $ \dv{p}{T} $ gives the slope of the line of phase transitions in the $ (T, p) $ plane:
	\begin{equation*}
		\dv{p}{T} = \frac{s_{g} - s_{l}}{v_{g} - v_{l}}
	\end{equation*}
	
	\item In terms of specific latent heat $ L = T(s_{g} - s_{l}) $, the expression for the slope of the phase transition line reads
	\begin{equation*}
		\dv{p}{T} = \frac{L}{T(v_{g} - v_{l})}
	\end{equation*}
	This result is the \textit{Clausius-Clapeyron equation}. It tells us that the slope of the phase transition line in the $ (T, p) $ plane is determined by the ratio of specific latent heat released in the transition to the change in volume. Alternatively, the equation predicts the saturated vapor pressure for a transition at a temperature $ T $.
	
	\item We can easily find an approximate solution to the Clausius-Clapeyron equation with a few simplifications. We assume:
	\begin{itemize}
		\item The latent heat $ L $ is constant throughout the transition
		\item $ v_{g} \gg v_{l} $ (which corresponds to $ T \ll T_{c} $) so $ v_{g} - v_{l} \approx v_{g}$ (a safe approximation for many substances; e.g. for water the error is $ \sim 0.1 \% $)
		\item We assume the gas obeys the ideal gas equation $ pv = k_{B}T $
	\end{itemize}
	Using the ideal gas law to solve for $ v_{g} $, the equation reduces to
	\begin{equation*}
		\dv{p}{T} = \frac{L}{Tv_{g}} = \frac{Lp}{k_{B}T^{2}} \implies p(T) = p_{0} \exp(-\frac{L}{k_{B}}\left(\frac{1}{T}-\frac{1}{T_{0}}\right))
	\end{equation*}
\end{itemize}



\subsection{Transport Phenomena}

\subsubsection{Common Questions}
\begin{enumerate}
    \item What is diffusion? How do we analyze diffusion in a binary mixture? Derive the continuity and diffusion equation for diffusion of matter.

    \item Derive the continuity and diffusion equations for heat.

    \item What are cross transport phenomena? Explain the thermoelectric effect and the Peltier effect.

\end{enumerate}


\subsubsection{Diffusion of Matter}
\textit{What is diffusion? How do we analyze diffusion in a binary mixture? Derive the continuity and diffusion equation for diffusion of matter.} 

\begin{itemize}
	\item \textit{Transport phenomena} are a name for processes involving the exchange of matter, energy, momentum, and other quantities between systems. \textit{Diffusion} is the net movement of a quantity (e.g. mass, energy, etc...) from a region of high concentration to a region of low concentration. 
	
	\item Consider a container with two separated substances of equal density---such a system is called a binary mixture. Because the two substances have equal densities the system's center of mass stays constant during diffusion, which simplifies the analysis.
	
	\item Consider a flow of mass and hypothetical cubic region of space inside the mass flow with volume $ V $ and cross-sectional area $ A $. Suppose the mass flow is described by a current density $ j $ describing amount of mass per unit time per cross-sectional area. By components:
	\begin{equation*}
		\dv{m}{t} = A(j_{in} - j_{out}) \eqtext{or} A(j_{out} - j_{in}) = - \dv{m}{t}
	\end{equation*}
	Since $ V $ is constant, $ \diff m = V \diff \rho $ and
	\begin{equation*}
		 A(j_{out} - j_{in}) = - \dv{m}{t} = - V\dv{\rho}{t} = - (A \Delta x) \pdv{\rho}{t}
	\end{equation*}
	where we allow for $ \rho = \rho (x, t) $ to change with both time and position along the $ x $ axis inside the region of space. Dividing by $ \Delta x $ and taking the limit $ \Delta x \to 0 $ gives
	\begin{equation*}
		\frac{\Delta j}{\Delta x} = \pdv{\rho}{t} \implies \pdv{j}{x} = \pdv{\rho}{t}
	\end{equation*}
	We can make an analogous argument for the components $ y $ and $ z $. The result is
	\begin{equation*}
		\pdv{\rho}{t} = - \left(\pdv{j}{x} + \pdv{j}{y} + \pdv{j}{z}\right) = - \nabla \cdot \bm{j}
	\end{equation*}
	The result $ \pdv{\rho}{t} = - \nabla \cdot \bm{j} $ is called the \textit{continuity equation}.
	
	\item From experiment, we know mass always flows from regions of high density to regions of low density, and the mass flow increases as the density gradient increases (i.e. the bigger the difference in densities). This relationship is given by
	\begin{equation*}
		\bm{j} = - D \nabla \rho(\bm{r}, t)
	\end{equation*}
	where $ D $ is the diffusion constant with SI units $ \si{\meter^{2} \cdot s^{-1}} $ and $ \rho = \rho(\bm{r}, t) $ is the mass density.
	
	\item If we substitute $ \bm{j} = - D \nabla \rho(\bm{r}) $ into the continuity equation and assume  the diffusion constant $ D $ is constant throughout space, we get the \textit{diffusion equation}
	\begin{equation*}
		\pdv{\rho}{t} = - \nabla \cdot \left[- D \nabla \rho(\bm{r}, t)\right] = D \left[\nabla \cdot \nabla \rho(\bm{r}, t)\right] = D \Delta \rho(\bm{r}, t)
	\end{equation*}
	where $ \Delta = \pdv[2]{}{x} + \pdv[2]{}{y} + \pdv[2]{}{z}$ is the Laplacian operator.
	
\end{itemize}



\subsubsection{Heat Transfer}
\textit{Derive the continuity and diffusion equations for heat.}
\begin{itemize}
	\item The derivation is similar to diffusion of mass in the presence of a density gradient. This time we'll consider diffusion of energy (as heat flow) in the presence of a temperature gradient.
	
	
	Consider a flow of energy and hypothetical cubic region of space inside the energy flow with volume $ V $ and cross-sectional area $ A $. Suppose the energy flow is described by a current density $ j $ describing amount of energy per unit time per cross-sectional area. If we work at constant pressure, we use enthalpy $ H $ and heat capacity $ C_{p} $. By components
	\begin{equation*}
		\dv{H}{t} = A(j_{in} - j_{out}) \eqtext{or} A(j_{out} - j_{in}) = - \dv{H}{t}
	\end{equation*}
	We substitute in $ \diff H = C_{p} \diff T = m c_{p} \diff T$  to get
	\begin{equation*}
		m c_{p}\dv{T}{t}= - A(j_{out} - j_{in})
	\end{equation*}
	Dividing through by $ m c_{p} $ and writing $ m = \rho A \Delta x $ gives
	\begin{equation*}
		\pdv{T}{t} = - \frac{A}{mc_{p}}(j_{out} - j_{in}) = -\frac{1}{\rho c_{p}} \frac{\Delta j}{\Delta x}
	\end{equation*}
	Taking the limit $ \Delta x \to 0 $ gives
	\begin{equation*}
		\pdv{T}{t} = -\frac{1}{\rho c_{p}} \pdv{j}{x}
	\end{equation*}
	We can make an analogous argument for the components $ y $ and $ z $. The result is
	\begin{equation*}
		\pdv{T}{t} = - \left(\pdv{j}{x} + \pdv{j}{y} + \pdv{j}{z}\right) = - \frac{1}{\rho c_{p}} \nabla \cdot \bm{j}
	\end{equation*}
	The result $ \pdv{T}{t} = - \nabla \cdot \bm{j} $ is the continuity equation for heat.
	
	\item From experiment and the second law of thermodynamics, we know heat always flows from regions of high temperature to regions of low temperature. Likewise, the heat flow increases as the temperature gradient increases. This relationship is modeled by
	\begin{equation*}
		\bm{j} = - \lambda \nabla T(\bm{r}, t)
	\end{equation*}
	where $ - \lambda $ is the coefficient of thermal conductivity with SI units $ \si{\watt \cdot \meter \cdot \kelvin} $ and $ T = T(\bm{r}, t) $ is the temperature.
	
	\item If we substitute $ \bm{j} = - \lambda \nabla T(\bm{r}, t) $ into the continuity equation for heat and assume $ \lambda $ is constant throughout space, we get the \textit{heat equation}
	\begin{equation*}
		\pdv{T}{t} = \frac{\lambda}{\rho c_{p}} \nabla \cdot \nabla T(\bm{r}, t) = \frac{\lambda}{\rho c_{p}} \Delta T(\bm{r}, t)
	\end{equation*}
	where $ \Delta = \pdv[2]{}{x} + \pdv[2]{}{y} + \pdv[2]{}{z}$ is the Laplacian operator. As before, we can define a diffusion coefficient $ D = \frac{\lambda}{\rho c_{p}} $ for heat. Like for mass, it has SI units $ \si{\meter^{2} \cdot \second^{-1}} $.
	
\end{itemize}


\subsubsection{Cross Transport Phenomena}
\textit{What are cross transport phenomena? Explain the thermoelectric effect and the Peltier effect.}

\begin{itemize}
	\item Typically, a density gradient causes mass flow and a temperature gradient causes heat (energy) flow. \textit{Cross transport phenomena} are processes in which a density gradient causes heat flow, a temperature gradient causes mass flow, etc...
	
	\item Consider a circuit made of two different wires, and assume the contact points between the wires are at two different temperatures $ T_{C} $ and $ T_{H} $ (cold and hot). Such a configuration is called a \textit{thermocouple}. The temperature difference between the contacts in a thermocouple causes a potential difference between the contacts, which leads a net flow of charge carriers around the circuit. This effect is called the \textit{thermoelectric effect} and is an example of a cross transport phenomenon: a temperature difference leads to a flow of mass (electrons) and thus also a flow of charge.
	
	\item The voltage $ U $ between the contacts is approximately linearly dependent on the temperature difference; the relationship is given by
	\begin{equation*}
		U = \alpha \Delta T
	\end{equation*}
	where $ \alpha $ is the thermoelectric coefficient. $ \alpha $ depends on the materials in the circuit and typically falls in the $ \si{\micro \volt \cdot \kelvin^{-1}} $ to the $ \si{\milli \volt \cdot \kelvin^{-1}} $ range. 
	
	\item The thermoelectric effect occurs because of thermodiffusion: the flow of mass in response to a temperature gradient. A wire whose ends are at different temperature has a higher chemical potential at the higher temperature end, so electrons accumulate at the lower temperature, lower chemical potential end. This leads to the potential difference observed in the thermoelectric effect.
	
	Because different metals have different dependence of chemical potential on temperature, the chemical potentials at the contact points of a thermocouple are different, causing electrons to cross to the region of lower chemical potential. Upon crossing, the thermoelectric voltage sweeps across to the second contact point, and the cycle repeats, leading to the electric current seen in the thermoelectric effect.
	
\end{itemize}

\textbf{Peltier Effect}
\begin{itemize}
	\item Consider a similar thermocouple setup as before, but this time we apply an external voltage to the circuit, causing an electric current $ I $ and subsequent heat flow $ P_{Q} = \pdv{Q}{t} $. The flow of heat is linearly proportional to the electric current:
	\begin{equation*}
		P_{Q} = \Pi I
	\end{equation*}
	where the constant of proportionality $ \Pi $ is called the \textit{Peltier coefficient}. 
	
	\item Since the Peltier effect is closely related to the thermoelectric effect, we expect a relationship between the Peltier and thermoelectric coefficients $ \Pi $ and $ \alpha $.
	
	We can determine the relationship from the efficiency of a Carnot engine, which is
	\begin{equation*}
		\eta = 1 - \frac{T_{C}}{T_{H}} = 1 - \frac{T_{C}}{T_{C} + \Delta T} = \frac{\Delta T}{T_{C} + \Delta T} 
	\end{equation*}
	If the temperature difference between the ends is small we can write, $ T = T_{C} \approx T_{H} \gg \Delta T $ and we can approximate
	\begin{equation*}
		\eta \approx \frac{\Delta T}{T}
	\end{equation*}
	
	\item Meanwhile, the efficiency must be equal to the ratio of electric power dissipated on the thermocouple to the inputted power $ P_{Q} $
	\begin{equation*}
		\eta = \frac{P_{\text{electric}}}{P_{Q}} = \frac{U I}{\Pi I} = \frac{\alpha \Delta T}{\Pi}
	\end{equation*}
	Equating the expressions for efficiency $ \eta $ gives the relationship between $ \Pi $ and $ \alpha $:
	\begin{equation*}
		\frac{\Delta T}{T} = \frac{\alpha \Delta T}{\Pi} \implies \Pi = \alpha T
	\end{equation*}
	
	
\end{itemize}

\newpage

\section{Statistical Physics}

\subsection{Introduction to Statistical Physics}
\subsubsection{Common Questions}
\begin{enumerate}
    \item What is a phase space? Discuss the phase space of a system of free point particles. How is the time evolution of a system described in terms of the phase space?

    \item What are microstates? What are probability density and equilibrium in statistical physics? What conditions must hold for a given probability density to be stationary?

    \item How is entropy defined in statistical physics? What is the Boltzmann entropy formula?

    \item What is the microcanonical ensemble? What kind of system does it describe and what is the associated probability distribution?

    \item Derive the second law of thermodynamics using a statistical approach.

    \item How is temperature defined in statistical physics? Discuss the relationship between temperature and the zeroth law.

\end{enumerate}


\subsubsection{Phase Space}
\textit{What is a phase space? Discuss the phase space of a system of free point particles. How is the time evolution of a system described in terms of the phase space?}
 \begin{itemize}
	 \item The system's \textit{phase space} is a $ 3N $ dimensional space consisting of the system's possible states. A single point in phase space contains all the information needed to uniquely specify the state of the system; each state variable needed to uniquely specify the system's state is assigned a single coordinate. Because most macroscopic systems are incredibly complex, we need many variables to uniquely specify the systems state, and the dimensionality of the phase space is absurdly large. 
	 
	 A point in phase space specifies a system's state, while a curve in phase space corresponds to the system's time evolution.
	 
	 \item For a system of $ N $ point particles the state of the system is defined by specifying the position $ \bm{q}_{i} $ and momentum $ \bm{p}_{i} $ of each of the $ i = 1, \ldots, N $ particles. In three dimensions, we need 3 position and 3 momentum coordinates per particle, and a total of $ 6N $ coordinates for the entire system. 
	 
	 For macroscopic systems $ N $ is typically of the order of Avogadro's number $ N_{A} = \SI{6.02e23}{\mole^{-1}} $, and phase spaces are very large.
	 
	 \item A differential of phase space, which we'll denote by $ \diff \Gamma $, is the product of the differentials of all of the coordinates needed to specify the system's state. For a three-dimensional system of $ N $ particles, the differential of phase space is
	 \begin{equation*}
		 \diff \Gamma = \diff \bm{r}_{1} \diff \bm{p}_{1}, \ldots, \diff \bm{r}_{N} \diff \bm{p}_{N}
	 \end{equation*}
 
 \end{itemize}



\subsubsection{Microstates, Probability Density, Equilibrium}
\textit{What are microstates? What are probability density and equilibrium in statistical physics? What conditions must hold for a given probability density to be stationary?}
 
\begin{itemize}
	
	\item Formally, the \textit{energy microstates} of an isolated system of $ N $ particles at energy $ E $ are the energy eigenstates of the system's Schr\"{o}dinger equation 
	\begin{equation*}
		\hat{H}\ket{\psi} = E\ket{\psi}
	\end{equation*}
	However, for the enormous values of $ N $ and degrees of freedom typical in statistical physics, working directly with energy eigenstates is complicated and not useful in practice. 
	
	Instead of working in terms of individual quantum microstates, statistical mechanics describes systems in terms of a \textit{statistical ensemble}, which is model for a thermodynamic system consisting large number of virtual copies considered all at once, each representing a possible state that the real system could be in. Essentially, a statistical ensemble is a probability distribution of the system's microstates.
			
	So we describe a system with a \textit{probability distribution} over all possible quantum microstates.  We denote the basis of the allowed states by $ \ket{n} $ and the probability that the system is in a given state as $ p(n) $.
	
	\item A system in  macroscopically steady state of energy and momentum, it is said to be in \textit{equilibrium}. In equilibrium the probability distribution, and thus expectation values of operators, are time-independent. Systems in which the probability distribution depends only on the system's energy are stationary (i.e. at rest).
	
	\item Finally, here is the fundamental assumption of statistical mechanics:
	\begin{quote}
		For an isolated system in equilibrium, all accessible microstates are equally likely.
	\end{quote}
	The word accessible is used intentionally (instead of e.g. allowed) to imply a bit of flexibility. Accessible microstates can include states that could be reached with small perturbations of the system. For now, for an isolated system of fixed energy in equilibrium, accessible means all states with the fixed energy $ E $.
	
	\item Notation: The number of states in the system with energy between $ E $ and $ E + \delta E $ is denoted by $ \Omega(E) $ and is usually at least of the order $ N \sim 10^{23} $.
	\begin{equation*}
		\Omega(E) = \text{number of microstates with energy $ E $}
	\end{equation*}
	for a quantum system with two spin degrees of freedom, the total number of microstates is of the order $ \sim 2^{10^{23}} $. This is a very large number! Because energy levels with spacing of the order $ \sim 2^{-10^{23}} $ are effectively continuous, $ \Omega(E) $ is essentially the number of microstates at energy $ E $.
	

\end{itemize}

\subsubsection{Entropy at Fixed Energy} \label{sss:entropy_fixed_E}
\textit{How is entropy defined in statistical physics? What is the Boltzmann entropy formula?}

\vspace{2mm}
\textit{Note}: Entropy is introduced much later in the UL FMF curriculum, but I have put the basic Boltzmann definition of entropy right at the beginning. Introducing entropy earlier makes the derivation of the canonical ensemble, in my opinion, more elegant and intuitive. It also allows us to derive the second law from statistical considerations.
\begin{itemize}
	\item The \textit{entropy} of a system at energy $ E $ is defined as
	\begin{equation*}
		S(E) = k_{B} \ln \Omega(E)
	\end{equation*}
	where $ \Omega(E) $ is the number of states at energy $ E $ and $ k_{B} = \SI{1.381e-23}{\joule \, \kelvin^{-1}} $. This formula is called the \textit{Boltzmann entropy formula} and defines entropy in terms of the total number of accessible microstates.
	
	\item Entropy is additive across non-interacting systems. For two non-interacting systems with energies $ E_{1} $ and $ E_{2} $ the total number of states for both system considered as a whole is 
	\begin{equation*}
		\Omega(E_{1}, E_{2}) \propto \Omega_{1}(E_{1})\Omega_{2}(E_{2})
	\end{equation*}
	while, because of the logarithm $ S \sim \ln \Omega $, the total entropy is
	\begin{equation*}
		S(E_{1}, E_{2}) \propto S_{1}(E_{2}) + S_{2}(E_{2})
	\end{equation*}
\end{itemize}

\subsubsection{Microcanonical Ensemble}
\textit{What is the microcanonical ensemble? What kind of system does it describe and what is the associated probability distribution?}

\begin{itemize}

	\item The microcanonical ensemble is used to described systems at a fixed energy. In the microcanonical ensemble for a system of fixed energy $ E $, there is a non-zero probability only for states with energy $ E $. 
	
	\item Under the assumption that all available states are equally likely, the probability in the microcanonical ensemble that an isolated system of fixed energy $ E $ occurs in a the microstate $ \ket{n} $ is 
	\begin{equation*}
		p(n) = \frac{1}{\Omega(E)}
	\end{equation*}
	while all states with energy than $ E $ have probability equal to zero.	For systems at fixed energy the expectation value of an operator $ \hat{O} $ is 
	\begin{equation*}
		\langle \hat{\mathcal{O}} \rangle = \sum_{n}p(n)\bra{n}\hat{\mathcal{O}}\ket{n}
	\end{equation*}
\end{itemize}


\subsubsection{Entropy and the Second Law}
\textit{Derive the second law of thermodynamics using a statistical approach.}
\begin{itemize}
	\item Consider taking two systems with energies $ E_{1} $ and $ E_{2} $ and placing them together so that they can exchange energy. Assume the systems exchange energy in a way that the energy levels of each system shift negligibly; the only relevant interaction is transfer of energy between the systems. The energy of the combined system is $ E_{tot} = E_{1} + E_{2} $.
	
	After energy transfer, the first system can have energies $ E \leq E_{tot} $, which means the energy of the second system is $ E_{tot} - E $. If we are formal, the first system can only take on discrete energies $ E_{i} $ that are energy eigenvalues of its Hamiltonian. In this case, the first system can have energies $ E_{i} \leq E_{tot} $, so the second system can have energies $ E_{tot} - E_{i} $. \footnote{\textit{Technicality:} As quantum systems, both system 1 and 2 have discrete energy levels. This is why we had to restrict the energies system 1 could take on to the discrete energies $ E_{i} $. We said system 1 could take on the energies $ E_{i} $ and then assume the energies of system 2 were the discrete energies $ E_{tot} - E_{i} $. However, there is no reason to know a priori that the discrete energies $ E_{tot} - E_{i} $ are allowed energy eigenvalues of the second system! In practice, the resolution of this dilemma is to ignore it. Because the energy levels are so closely spaced, they are effectively continuous, and the quantum considerations of discreteness are not important.}
	
	
	\item From Subsection \ref{sss:entropy_fixed_E}  the number of microstates states for two \textit{non-interacting} systems considered as a whole is
	\begin{equation*}
		\Omega(E_{1}, E_{2}) \propto \Omega_{1}(E_{1})\Omega_{2}(E_{2})
	\end{equation*}
	In our case, the systems can exchange energies and $ E_{1} $ can take on energies $ E_{i} $ in the range $ [0, E_{tot}] $, while $ E_{2} $ can take on energies $ E_{tot} - E_{i} $. To cover all possible energy levels, we must sum $ \Omega(E_{tot}) $ over all possible energies $ E_{i} $. The result is 
	\begin{equation*}
		\Omega(E_{tot}) = \sum_{\{E_{i}\}}\Omega_{1}(E_{i})\Omega_{2}(E_{tot} - E_{i})
	\end{equation*}
	where the sum runs over all the allowed energy eigenvalues $ E_{i} $ of the first system, which fall in the range $ [0, E_{tot}] $.
	
	\item Inverting the definition of entropy $ S(E) = k_{B}\ln \Omega(E) $, produces $ \Omega(E) = \exp \frac{S(E)}{k_{B}} $, which leads to 
	\begin{align*}
		\Omega(E_{tot}) &= \sum_{\{E_{i}\}} \exp(\frac{S_{1}(E_{i})}{k_{B}}) \exp(\frac{S_{1}(E_{tot} - E_{i})}{k_{B}}) \\
		& = \sum_{\{E_{i}\}} \exp(\frac{S_{1}(E_{i})}{k_{B}} + \frac{S_{1}(E_{tot} - E_{i})}{k_{B}})
	\end{align*}
	

	
	\item After interaction, the combined system has the fixed energy $ E_{tot} $, so it rests in a microcanonical ensemble with distribution $ p(n) = \frac{1}{\Omega(E_{tot})}$. Because the microstates of the original systems are a subset of the total number of possible states of the combined system, the entropy $ S(E_{tot}) $ of the combined system is greater than or equal to the entropy $ S_{1}(E_{1}) + S_{2}(E_{2}) $ of the original system in which the two systems $ 1 $ and $ 2 $ were separate and non-interacting.

	Or, more simply, because there are at least as many possible states in the combined system as in the separated isolated systems, the entropy of the combined systems is at least as large as the entropy of the isolated systems separately. In any case,
	\begin{equation*}
		S(E_{tot}) \geq S_{1}(E_{1}) + S_{2}(E_{2})
	\end{equation*}
	
	\item \textit{Important approximation:} Consider $ S(E_{tot}) \equiv k_{B}\ln \Omega(E_{tot}) \geq S_{1}(E_{1}) + S_{2}(E_{2}) $ when the number of particles $ N $ is very large and recall the expression for the total number of allowed microstates in the combined system is 
	\begin{equation*}
		\Omega(E_{tot}) = \sum_{\{E_{i}\}} \exp(\frac{S_{1}(E_{i})}{k_{B}} + \frac{S_{2}(E_{tot} - E_{i})}{k_{B}})
	\end{equation*}	
	Entropy scales as $ S \sim N $, so $ \Omega $ scales as a sum of exponential of $ N $. But $ N \sim 10^{23} $ is itself an exponential, so $ \Omega $ scales as a sum of exponentials of exponentials! Because of how rapidly exponentials grow with increasing arguments, a sum of exponentials with exponentially increasing arguments is completely dominated by the maximum term. In our particular case, the sum defining the system's total number of allowed microstates is very well approximated\footnote{ As a example of why the approximation is valid, make the reasonable assumption that some large $ E_{max} $ is slightly larger than all other energies, making the exponent $ e^{E_{max}} $ e.g. twice as large as for any other $ E $. In this case, the term in the sum for $ \Omega(E_{tot}) $ corresponding to $ E_{max} $ will be larger by all other terms by the absurdly large factor $ e^{N} \sim e^{10^{23}} $. In other words, the other terms are completely negligible.} by the largest term
	\begin{equation*}
		\Omega(E_{tot}) \approx \exp(\frac{S_{1}(E_{max})}{k_{B}} + \frac{S_{2}(E_{tot} - E_{max})}{k_{B}})
	\end{equation*}	
	where the maximum value $ E_{max} $ occurs at the point satisfying the condition 
	\begin{equation*}
		\pdv{S_{1}}{E} (E_{max}) - \pdv{S_{2}}{E} (E_{tot} - E_{max}) = 0
	\end{equation*}
		
	\item After energy transfer, the system is \textit{overwhelmingly likely} to be found in a state with energy $ E_{max} $ because the $ E_{max} $ term in the sum for $ \Omega(E_{tot}) $ utterly dominates the other terms. \textit{Because there are so many more states available at energy $ E_{max} $ than at any other energy (by a factor $ \sim e^{10^{23}} $), it makes sense the first system would occupy a state with energy $ E_{max} $.} More so, once the first system reaches energy $ E_{max} $, it is likewise overwhelmingly unlikely to ever again be found in a state with energy other than $ E_{max} $. Because the system settles into the state corresponding to the largest number of microstates, it necessarily settles into the state maximizing its entropy. 
	
	\item It is the \textit{overwhelming probability} of the first system having energy $ E_{max} $ that is responsible for the second law of thermodynamics and the principle that entropy increases in all physical processes. When two systems are brought together, energy is statistically all but certain to be transferred in such a way that the total number of available for the combined systems is vastly enlarged compared to when the constituent systems where separate, maximizing the system's entropy.

\end{itemize}	

\subsubsection{Temperature and the Zeroth Law} \label{sss:temperature_stat_phys}
\textit{How is temperature defined in statistical physics? Discuss the relationship between temperature and the zeroth law.}
\begin{itemize}
	\item In statistical mechanics the temperature $ T $ of a system is defined as
	\begin{equation*}
		\frac{1}{T} = \pdv{S}{E}
	\end{equation*}
	A fundamental property in statistical mechanics is: \textit{If two separate systems in equilibrium, both at the same temperature $ T $, are placed in contact, nothing happens}! The systems stay the same and no energy is transferred between them.
	
	\item How to explain this? We showed earlier by statistical arguments how two such systems placed in contact will (are overwhelmingly likely to) maximize their entropy (i.e. maximize the number of available microstates $ \Omega $), which occurs when the first system takes on the energy $ E_{max} $ and the second system takes on the energy $ E_{tot} - E_{max} $, where $ E_{max} $ is defined by
	\begin{equation*}
		\pdv{S_{1}}{E} (E_{max}) - \pdv{S_{2}}{E} (E_{tot} - E_{max}) = 0
	\end{equation*}
	If nothing happens when the systems are placed in contact, the first system must already have been at energy $ E_{max} $. This means that before being placed in contact, $ E_{1} \equiv E_{max} $ and $ E_{2} \equiv E_{tot} - E_{max} $. In this case, the equation defining $ E_{max} $ reads
	\begin{equation*}
		\pdv{S_{1}}{E} (E_{1}) = \pdv{S_{2}}{E} (E_{2})
	\end{equation*}
	which, from the definition $ \frac{1}{T} = \pdv{S}{E} $, occurs precisely when the system's temperatures $ T_{1} $ and $ T_{2} $ were already equal! The condition that no energy is transferred between the systems leads directly to $ T_{1} = T_{2} $.
	
	\item What happens when two systems at slightly different temperatures are placed in contact? The systems will exchange energy, and conservation of energy ensures the changes obey $ \delta E_{1} = -\delta E_{2} $. 
	
	Because entropy is additive, the entropy of the combined system is $ S(E_{tot}) = S_{1}(E_{1}) + S_{2}(E_{2}) $. For small changes in entropy $ \delta S $, a differential approximation and the identity $ \delta E_{1} = -\delta E_{2}  $ (conservation of total energy) leads to
	\begin{align*}
		\delta S &= \pdv{S_{1}(E_{1})}{E}\delta E_{1} + \pdv{S_{2}(E_{2})}{E}\delta E_{2} = \left( \pdv{S_{1}(E_{1})}{E} - \pdv{S_{2}(E_{2})}{E} \right)\delta E_{1}\\
		&=\left(\frac{1}{T_{1}} - \frac{1}{T_{2}}\right) \delta E_{1}
	\end{align*}
	This is an important result. The second law states that $ \delta S $ is always positive. This implies that energy always flows from the system with higher temperature to the system with lower temperature. For example, if $ \delta E_{1} $ is positive, meaning energy flowed into system 1 $ T_{2} > T_{2} $ to satisfy $ \delta S > 0 $.
	
	Mathematically, the energy flows from higher to lower temperature because $ \pdv{S}{E} $ is a monotonically decreasing function of temperature.
\end{itemize}

\subsubsection{Example: The Two-State System in the Microcanonical Ensemble} \label{sss:two_state_mce}
This is subsubsection is not a question in itself, but the two-state system is referenced a few other times in other questions, so I have included a discussion here.
\begin{itemize}
	\item Our so-called two state system is a system of $ N $ non-interacting particles, each fixed in position so that the system's volume is constant. Each individual particle can occupy one of two possible states, e.g. spin up $ \ket{\uparrow} $ or spin down $ \ket{\downarrow} $. 
	
	\item The states have energy $ E_{\downarrow} = 0$ and $ E_{\uparrow} = \epsilon $, so it is energetically favorable for the particles to be in the spin down position, i.e. the position with lower energy. 
	
	Suppose there are $ N_{\uparrow} $ particles with spin up and $ N_{\downarrow} = N - N_{\uparrow}$ particles with spin down, so the energy of the system is 
	\begin{equation*}
		E = N_{\uparrow} \epsilon
	\end{equation*}
	
	\item The number of states $ \Omega(E) $ of the total system at the energy $ E $ is the number of ways to choose $ N_{\uparrow} $ particles from a total of $ N $, which from basic combinatorics is 
	\begin{equation*}
		\Omega(E) = \frac{N!}{N_{\uparrow}!(N - N_{\uparrow}!)}
	\end{equation*}
	The two-state system's entropy is thus
	\begin{equation*}
		S(E) = k_{B}\ln \Omega(E) = k_{B} \ln(\frac{N!}{N_{\uparrow}!(N - N_{\uparrow}!)})
	\end{equation*}
	
	\item Using Stirling's approximation $ \ln N! \approx N\ln N - N $ and canceling common terms, the system's entropy is very nearly
	\begin{equation*}
		S(E) = -k_{B}\big[ (N-N_{\uparrow})\ln(N-N_{\uparrow}) + N_{\uparrow}\ln N_{\uparrow} - N \ln N \big]
	\end{equation*}
	Adding and subtracting $ N_{\uparrow}\ln N $ from the right side of the equation, applying logarithm properties and substituting in the energy expression $ E = N_{\uparrow} \epsilon $ gives
	\begin{align*}
		S(E) &= -k_{B} \left[(N - N_{\uparrow})\ln(\frac{N - N_{\uparrow}}{N}) + N_{\uparrow} \ln(\frac{N_{\uparrow}}{N}) \right]\\
		&= -k_{B}N \left[\left (1 - \frac{E}{N\epsilon}\right )\ln(1 - \frac{E}{N\epsilon}) + \frac{E}{N\epsilon} \ln(\frac{E}{N\epsilon}) \right]
	\end{align*}
	Note that the entropy vanishes for $ E = 0 $ (all spins down) and $ E = N\epsilon $ (all spins up), because there is only one microstate for these energies. The microstates and entropy are maximized for $ E = \frac{N\epsilon}{2} $ (half spin up and half spin down) when $ S = N k_{B} \ln 2 $.
	
	\item If the system has energy $ E $, its temperature is
	\begin{equation*}
		\frac{1}{T} = \pdv{S}{E} = \frac{k_{B}}{\epsilon}\ln(\frac{N\epsilon}{E} - 1)
	\end{equation*}
	From this expression, the fraction of particles with spin up at temperature $ T $ is
	\begin{equation*}
		\frac{N_{\uparrow}}{N} = \frac{E}{N\epsilon} =  \frac{1}{e^{\epsilon/k_{B}T} + 1} 
	\end{equation*}
	As $ T \to \infty $, the fraction approaches $ \frac{N_{\uparrow}}{N} = \frac{1}{2} $, the configuration of maximum entropy.
	
	\item For energies $ E > \frac{N\epsilon}{2} $ where $ \frac{N_{\uparrow}}{N} > \frac{1}{2}$, the temperature as defined by $ \frac{1}{T} = \pdv{S}{E} $ is negative, which can be interpreted as hotter than infinity. In systems with negative temperature, the entropy/number of microstates decreases with increasing energy. Such systems can be momentarily achieved in laboratory conditions by instantaneously inverting all particle spins.
	
	\item From the relationship $ E = \frac{N\epsilon}{e^{\epsilon/k_{B}T} + 1} $, the system's heat capacity $ C = \dv{E}{T} $ is 
	\begin{equation*}
		C = \frac{N \epsilon^{2}}{k_{B}T^{2}} \frac{e^{\epsilon/k_{B}T}}{(e^{\epsilon/k_{B}T} + 1)^{2}} 
	\end{equation*}
	As expected, $ C \sim N $. Note that $ C $ increases with temperature, reaches a maximum near the characteristic temperature $ T \sim \frac{\epsilon}{k_{B}} $, then decreases. As $ T \to 0 $, $ C $ approaches zero exponentially as $ C \sim e^{-1/T} $.
\end{itemize}


\subsection{Canonical Ensemble}
\subsubsection{Common Questions}
\begin{enumerate}
    \item What is the canonical ensemble and what kind of system does it describe?  Derive the associated probability distribution for the canonical ensemble.

    \item What is a partition function for the canonical ensemble? How do we define the average of a quantity in statistical physics? Derive the expression for average energy in the canonical ensemble.

    \item Derive the expression for energy fluctuations in the canonical ensemble. Show that the ratio of energy fluctuation to the system's average energy is negligible in a macroscopic system.

    \item State and derive Gibbs formula for entropy and discuss its scope.

    \item Discuss the relationship between the Gibbs and Boltzmann entropy formulas for the microcanonical ensemble. Derive an expression for entropy in terms of the canonical partition function.

    \item How is Helmholtz free energy defined in statistical physics? What is the expression for free energy in the canonical ensemble?

    \item Discuss and derive the equipartition principle. How is the equipartition principle used to predict the specific heat of monatomic and diatomic gases?

\end{enumerate}


\subsubsection{Canonical Ensemble}
\textit{What is the canonical ensemble and what kind of system does it describe?  Derive the associated probability distribution for the canonical ensemble.}
\begin{itemize}
	\item The microcanonical ensemble describes systems with fixed energy $ E $, from which we can deduce the system's equilibrium temperature $ T $. It is often more convenient (and physically appropriate) to describe a system at fixed temperature $ T $, since, in practice, a system's energy constantly fluctuates as it interacts with its environment. The \textit{canonical ensemble} describes a system at fixed temperature $ T $, from which it is possible to deduce and average (equilibrium) energy.
	
	\item To model the canonical ensemble, consider the system of interest in contact with a second system called a \textit{heat reservoir} at an equilibrium temperature $ T $. The reservoir has very large energy compared to the system of interest, so any flow of energy from the system to the reservoir has no appreciable effect on the reservoir's energy or temperature. 
	
	We label the system's states as $ \ket{n} $ and assign each of the primary system's states the energy $ E_{n} $. The number of microstates of the combined system and reservoir is given by the sum over all of the primary system's states $ \ket{n} $ \footnote{Note that the sum is over all of $ S $'s states and \textit{not} all of $ S $'s energy levels $ \{E_{i}\} $. If we summed over energy levels we would have to include a degeneracy factor $ \Omega_{S}(E_{n}) $ since each energy $ E_{n} $ occurs $ \Omega_{S}(E_{N}) $ times because of the degeneracy of states with energy $ E_{n} $.}
	\begin{equation*}
		\Omega(E_{tot}) = \sum_{n}\Omega_{R}(E_{tot} - E_{n}) = \sum_{n} \exp(\frac{S_{R}(E_{tot}- E_{n})}{k_{B}})
	\end{equation*}
	Because of the reservoir's large energy, $ E_{n} \ll E_{tot}$. A first-order Taylor approximation of $ S_{R} $ for small deviations $ E_{n} $ from $ E_{tot} $ is
	\begin{equation*}
		S_{R}(E_{tot} - E_{n}) \approx S_{R}(E_{tot}) - E_{n}\pdv{S_{R}}{E_{tot}}
	\end{equation*}
	and the number of microstates becomes
	\begin{equation*}
		\Omega(E_{tot}) \approx \sum_{n} \exp(\frac{S_{R}(E_{tot}) }{k_{B}} - \frac{E_{n}}{k_{B}}\pdv{S_{R}}{E_{tot}}) = \exp(\frac{S_{R}(E_{tot})}{k_{B}}) \sum_{n} \exp(- \frac{E_{n}}{k_{B}}\pdv{S_{R}}{E_{tot}})
	\end{equation*}
	By introducing temperature $ \frac{1}{T} = \pdv{S_{R}}{E_{tot}} $, the expression becomes
	\begin{equation*}
		\Omega(E_{tot}) = e^{S_{R}(E_{tot})/k_{B}} \sum_{n} e^{-E_{n}/k_{B}T}
	\end{equation*}
		
	\item By the fundamental principle of statistical mechanics, all of the total system's energy microstates states are equally likely. The number $ \Omega_{n} $ of \textit{single particle} states for which the system occurs in a given microstate $ \ket{n} $ is
	\begin{equation*}
		\Omega_{n}(E_{tot}) = e^{S_{R}/k_{B}} e^{-E_{n}/k_{B}T}
	\end{equation*}
	The probability the system occurs in the state $ \ket{n} $ is the ratio of this value to the total number of states
	\begin{equation*}
		p(n) = \frac{\Omega_{n}}{\Omega} = \frac{e^{S_{R}/k_{B}} e^{-E_{n}/k_{B}T}}{e^{S_{R}(E_{tot})/k_{B}} \sum_{m} e^{-E_{m}/k_{B}T}} = \frac{ e^{-E_{n}/k_{B}T}}{\sum_{m} e^{-E_{m}/k_{B}T}}
	\end{equation*}
	This distribution is called the \textit{Boltzmann distribution} or the \textit{canonical ensemble}. Because of the exponential proportionality $ p(n) \propto  e^{-E_{n}/k_{B}T} $, states with $ E_{n} \gg k_{B}T $ are highly unlikely to be occupied, while states with $ E_{n} \leq k_{B}T $ have a good chance of being occupied. As $ T \to 0 $, $ p(n) \to 0 $ for $ E_{n} \neq 0 $ and the system is forced into its ground state with energy $ E_{0} = 0$.
	
	\item Because the terms $ \frac{1}{k_{B}T} $ will come up all the time, we denote it by
	\begin{equation*}
		\beta \equiv \frac{1}{k_{B}T}
	\end{equation*}
	The term is sometimes called the \textit{thermodynamic beta}. In terms of $ \beta $, the canonical ensemble is
	\begin{equation*}
		p(n) =  \frac{ e^{-\beta E_{n}}}{\sum_{m} e^{-\beta E_{m}}}
	\end{equation*}
	
	
\end{itemize}


\subsubsection{Canonical Partition Function}
\textit{What is a partition function for the canonical ensemble? How do we define the average of a quantity in statistical physics? Derive the expression for average energy in the canonical ensemble.}

\begin{itemize}
	\item Recall the probability distribution for canonical ensemble is
	\begin{equation*}
		p(n) =  \frac{ e^{-\beta E_{n}}}{\sum_{m} e^{-\beta E_{m}}}
	\end{equation*}
	The normalization factor in the denominator of the Boltzmann probability distribution is called the \textit{partition function} and is denoted by
	\begin{equation*}
		Z = \sum_{n} e^{-\beta E_{n}}
	\end{equation*}
	The partition function counts the total number of microstates $ \Omega $ in a system and has important applications throughout statistical mechanics. 
	
	\item In terms of $ Z $, the probability to find a canonical system in the state $ \ket{n} $ is
	\begin{equation*}
		p(n) = \frac{e^{-\beta E_{n}}}{Z}
	\end{equation*}
	while the average value of an operator quantity $ \hat{\mathcal{O}} $ in the canonical ensemble is
	\begin{equation*}
		\expval{\hat{\mathcal{O}}} = \sum_{n}p(n) \mathcal{O}_{n} = \sum_{n} \frac{ \mathcal{O}_{n}e^{-\beta E_{n}}}{Z}
	\end{equation*}
	where $ \mathcal{O}_{n} $ are the eigenvalues of $  \hat{\mathcal{O}} $ in the state $ \ket{n} $.
	
	\item The average energy $ \expval{E} $ of a system in the canonical ensemble is
	\begin{equation*}
		\expval{E} = \sum_{n}p(n)E_{n} = \sum_{n} \frac{E_{n}e^{-\beta E_{n}}}{Z}
	\end{equation*}
	
	The average energy can be neatly written in terms of the partition function as
	\begin{equation*}
		\expval{E} = - \pdv{}{\beta} \ln Z
	\end{equation*}
	We verify this with a direct computation of the partial derivative
	\begin{align*}
		- \pdv{}{\beta} \ln Z &= - \pdv{}{\beta} \ln(\sum_{n}e^{-\beta E_{n}}) = - \frac{\sum_{n}-E_{n}e^{-\beta E_{n}}}{\sum_{n}e^{-\beta E_{n}}} = + \frac{\sum_{n}E_{n}e^{-\beta E_{n}}}{Z} \\
		&= \sum_{n}p(n)E_{n} = \expval{E} 
	\end{align*}
	\vspace{-5mm}

	\item The partition function is multiplicative for independent systems that don't interact with each other, e.g. $ Z_{tot} = Z_{1}Z_{2} $. To prove this, consider two systems with energies $ E^{(1)} $ and $ E^{(2)} $ and total energy $ E_{tot} =  E^{(1)} + E^{(2)} $. The total partition function is 
	\begin{align*}
		Z_{tot} &= \sum_{j} e^{-\beta E^{(tot)}_{j}} = \sum_{n, m}e^{-\beta (E^{(1)}_{n} + E^{(2)}_{m} )} = \sum_{n, m}e^{-\beta E^{(1)}_{n}}e^{-\beta E^{(2)}_{m}}\\
		&=\sum_{n}e^{-\beta E^{(1)}_{n}} \sum_{m}e^{-\beta E^{(2)}_{m}} = Z_{1}Z_{2}
	\end{align*}
\end{itemize}

\subsubsection{Energy Fluctuations in The Canonical Ensemble}
\textit{Derive the expression for energy fluctuations in the canonical ensemble. Show that the ratio of energy fluctuation to the system's average energy is negligible in a macroscopic system.}
\begin{itemize}
	
	\item The spread $ \Delta E^{2} $ of energy about the mean value $ \expval{E} $, which represents energy fluctuations in the canonical ensemble, is the energy variance
	\begin{equation*}
		\Delta E^{2} = \expval{\left(E - \expval{E}\right)^{2}} = \langle E^{2}\rangle - \expval{E}^{2}
	\end{equation*}
	The energy variance can be neatly expressed both in terms of the average energy and partition function as
	\begin{equation*}
		\Delta E^{2} = -\pdv{\expval{E}}{\beta} = \pdv[2]{}{\beta}\ln Z
	\end{equation*}
	The second identity can be verified directly with a straightforward, but rather tedious, application of the quotient rule:
	\begin{align*}
		\Delta E^{2} &= \pdv{}{\beta}\left[\pdv{}{\beta} \ln Z\right] = \pdv{}{\beta}\left[- \frac{\sum_{n}-E_{n}e^{-\beta E_{n}}}{\sum_{n}e^{\beta E_{n}}}\right]\\
		&= \frac{\sum_{n}E_{n}^{2}e^{-\beta E_{n}}\sum_{n}e^{-\beta E_{n}} - \left(\sum_{n}E_{n}e^{-\beta E_{n}}\right)^{2}}{\left(\sum_{n}e^{-\beta E_{n}}\right)^{2}}\\
		&=\frac{\sum_{n}E_{n}^{2}e^{-\beta E_{n}}}{Z} - \left(\frac{\sum_{n}E_{n}e^{-\beta E_{n}}}{Z}\right)^{2} =  \expval{E^{2}} - \expval{E}^{2}
	\end{align*}
	where the last line follows from the identity $  \langle \mathcal{O}\rangle = \sum_{n}p(n)\mathcal{O}_{n} $ with $ \mathcal{O} = E^{2} $ and $ E $.
	
	\item The definition of capacity $ C_{V} $ in the canonical ensemble is
	\begin{equation*}
		C_{V} = \pdv{\expval{E}}{T}\bigg |_{V}
	\end{equation*} 
	The energy variance $ \Delta E^{2} $ can be expressed in terms of heat capacity $ C_{V} $ as
	\begin{equation*}
		\Delta E^{2} = k_{B}T^{2} C_{V}
	\end{equation*}
	This expression comes from the identity $ \Delta E^{2} = -\pdv{}{\beta} \expval{E} $ and changing from differentiation with respect to $ \beta $ to differentiation with respect to $ T $, i.e. $ \pdv{}{\beta} = - k_{B}T^{2} \pdv{}{T} $.
	\begin{equation*}
		\Delta E^{2} = -\pdv{}{\beta} \expval{E} = + k_{B}T^{2} \pdv{\expval{E} }{T} = k_{B}T^{2}C_{V}
	\end{equation*}
	
	\item Because $ C_{V} \sim N $, the equation $ \Delta E^{2} = k_{B}T^{2} C_{V} $ shows that $ \Delta E^{2} \sim N $, implying $ \Delta E \sim \sqrt{N} $. Because the system's energy typically grows as $ E \sim N $, the \textit{relative size} of the fluctuations scales as
	\begin{equation*}
		\frac{\Delta E}{E} \sim \frac{\sqrt{N}}{N} = \frac{1}{\sqrt{N}}
	\end{equation*}
	In the limit of many particles $ N \to \infty $, the size of the fluctuations relative to the system's total energy is negligible, i.e. $ \frac{\Delta E}{E} \to 0 $. The limit $ N \to \infty $ is called the \textit{thermodynamic limit}.
	
	\item In the thermodynamic limit, as $ \frac{\Delta E}{E} \to 0 $ we have $ E \to \expval{E} $, and the system's energy becomes arbitrarily close to the mean value $ \expval{E} $. In other words, in the thermodynamic limit of large $ N $, the canonical ensemble behaves as a microcanonical ensemble with fixed energy $ \expval{E} $.
	
	Because in practice $ N $ is at least $ \sim 10^{23} \approx \infty$, we can consider most systems well within the thermodynamic limit. For this reason, we can safely approximate a canonical ensemble's energy $ E $ with the average energy $ \expval{E} $.

\end{itemize}

\subsubsection{Gibbs Entropy Formula}
\textit{State and derive Gibbs formula for entropy and discuss its scope.}

\begin{itemize}
	\item Recall that entropy was defined in the microcanonical ensemble in terms of the number of states with the fixed energy $ E $. For the canonical ensemble this won't work, because by construction the canonical ensemble is probability distribution over states with different energies. The Gibbs entropy formula will give a more powerful definition of entropy that also applies to systems with variable energies.
	
	\item We consider a primary system and heat reservoir together and use a trick. Instead of one copy of the primary system, assume we have a large number $ W $ of identical copies, with each system occurring in a state $ \ket{n} $. If $ W $ is very large, the number of primary systems in a state $ \ket{n} $ is $ W p(n) $.
	
	To determine the entropy in the canonical ensemble, we treat the collection of $ W $ primary systems as occurring in a microcanonical ensemble to which we can apply the familiar definition $ S = k_{B} \ln \Omega$. 
	
	\item From combinatorics, number of ways of putting $ W p(n) $ systems into the state $ \ket{n} $ for each state $ \ket{n} $ is
	\begin{equation*}
		\Omega = \frac{W!}{\prod_{n}(p(n)W)!}
	\end{equation*}
	so the entropy for the $ W $ primary systems together is
	\begin{align*}
		S_{W} &= k_{B} \ln \Omega = k_{B}\ln(\frac{W!}{\prod_{n}(p(n)W)!})=k_{B}\ln W! - k_{B}\ln  \big[\textstyle \prod_{n}(p(n)W)! \big]\\
		&=k_{B}\ln W! - k_{B} \sum_{n} \ln\big[(p(n)W)!\big]
	\end{align*}
	For large $ W $, we can use Stirling's formula $ \ln N! \approx N \ln N - N $ to get
	\begin{equation*}
		S_{W} = - k_{B} W \sum_{n}p(n)\ln p(n)
	\end{equation*}
	
	\item \textit{Derivation (feel free to skip):} I've divided by $ k_{B} $ to minimize clutter
	\begin{itemize}
		\item Apply Stirling's formula to $ \frac{S_{W}}{k_{B}} = \ln W! - \sum_{n} \ln\big[(p(n)W)!\big] $ to get
		\begin{equation*}
			\frac{S_{W}}{k_{B}} = W \ln W - W - \sum_{n}\left[p(n)W \ln (p(n)W) -p(n)W \right]
		\end{equation*}
	
		\item The key is simplifying the sum, which we'll call $ \mathcal{S} =  \sum_{n}[\ldots]$. First, factor into
		\begin{equation*}
			\mathcal{S} \equiv \sum_{n}\left[p(n)W \ln (p(n)W) -p(n)W \right] = \sum_{n} p(n)W \ln (p(n)W) - \sum_{n} p(n)W 
		\end{equation*}
		
		\item Factor out the constant term $ W $ and apply $ \sum_{n}p(n) = 1 $ to get
		\begin{equation*}
			\mathcal{S} = W \sum_{n} p(n) \ln (p(n)W) - W \sum_{n} p(n) =W \sum_{n} p(n) \ln (p(n)W) - W 
		\end{equation*}
		
		\item Split up the remaining sum using $ \ln (p(n) W) = \ln p(n) + \ln W $ to get
		\begin{align*}
			\mathcal{S} &= W \sum_{n} p(n)(\ln p(n) + \ln W) - W \\
			&= W \sum_{n} p(n) \ln p(n) + W \sum_{n} p(n)\ln W - W\\
			&= W \sum_{n} p(n) \ln p(n) + W \ln W \sum_{n} p(n) - W 
		\end{align*}
		
		\item Finally, apply the normalization condition $ \sum_{n}p(n) = 1 $ to get
		\begin{equation*}
			\mathcal{S} = W \sum_{n} p(n) \ln p(n) + W\ln W - W 
		\end{equation*}	
		
		\item The hard work is done. Returning to $ \frac{S_{W}}{k_{B}} $, we have
		\begin{align*}
			\frac{S_{W}}{k_{B}} &= W \ln W - W - \mathcal{S} \\
			&= W \ln W - W - \left(W \sum_{n} p(n) \ln p(n) + W\ln W - W \right) \\
			&= - W \sum_{n} p(n) \ln p(n)
		\end{align*}
		from which follows $ S_{W} = - k_{B} W \sum_{n}p(n)\ln p(n) $
	\end{itemize}

	\item The expression $ S_{W} = - k_{B} W \sum_{n}p(n)\ln p(n) $ is the entropy for all $ W $ copies of the primary system. Because entropy additive, we get the entropy $ S $ for a single system by dividing by $ W $, giving
	\begin{equation*}
		S = \frac{S_{W}}{W} = - k_{B} \sum_{n}p(n)\ln p(n) 
	\end{equation*}
	This powerful formula is called the \textit{Gibbs entropy formula} and applies to any system in which the energy states are distributed as $ p(n) $.
\end{itemize}


\subsubsection{Gibbs Formula in the Microcanonical and Canonical Ensembles}
\textit{Discuss the relationship between the Gibbs and Boltzmann entropy formulas for the microcanonical ensemble. Derive an expression for entropy in terms of the canonical partition function.}
\begin{itemize}
	\item For the microcanonical distribution $ p(n) = \frac{1}{\Omega(E)} $, the Gibbs formula recovers the formula $ S(E) = k_{B} \ln \Omega (E) $. Plugging in $ p(n) = \frac{1}{\Omega(E)}$ gives
	\begin{equation*}
		S = - k_{B} \sum_{n}p(n)\ln p(n) = - k_{B} \sum_{n} \frac{1}{\Omega(E)} \ln \frac{1}{\Omega(E)}
	\end{equation*}
	Factor out the constant term $ \ln \frac{1}{\Omega(E)} $ and apply $ \sum_{n} \frac{1}{\Omega(E)} = \sum_{n}p(n) = 1 $ to get
	\begin{equation*}
		S = - k_{B} \ln \frac{1}{\Omega(E)} \sum_{n} \frac{1}{\Omega(E)} = -k_{B} \ln \frac{1}{\Omega(E)}
	\end{equation*}
	Apply the logarithm identity $ p \ln x = \ln x^{p} $ with $ p = -1 $ to get
	\begin{equation*}
		S = -k_{B} \ln \frac{1}{\Omega(E)} = - k_{B} \ln \left(\Omega(E)\right)^{-1} = + k_{B} \ln \Omega (E)
	\end{equation*}
\end{itemize}

\textbf{Gibbs Formula and the Canonical Ensemble}
\begin{itemize}
	\item Returning to the canonical ensemble with $ p(n) = \frac{e^{-\beta E_{n}}}{Z} $, the entropy is
	\begin{equation*}
		S = \frac{\expval{E}}{T} + k_{B} \ln Z
	\end{equation*}
	\textit{Derivation:} Apply basic logarithm properties and $ \sum_{n} \frac{e^{-\beta E_{n}}}{Z} \equiv \sum_{n} p(n) = 1 $ to get
	\begin{align*}
		S &= - \frac{k_{B}}{Z} \sum_{n}e^{-\beta E_{n}}\ln \frac{e^{-\beta E_{n}}}{Z}\\
		&= - \frac{k_{B}}{Z} \sum_{n}e^{-\beta E_{n}}  \left (\ln e^{-\beta E_{n}} -\ln Z\right )\\
		&= \frac{k_{B}}{Z} \sum_{n} \beta E_{n} e^{-\beta E_{n}} +  k_{B}\ln Z  \sum_{n}  \frac{e^{-\beta E_{n}}}{Z}\\
		&=k_{B}\beta \sum_{n} \frac{E_{n}e^{-\beta E_{n}}}{Z} + k_{B}\ln Z =\frac{\expval{E}}{T} + k_{B} \ln Z
	\end{align*}
	From a reverse-engineered product rule on $ S = \frac{k_{B}\beta}{Z}\sum_{n} E_{n}e^{-\beta E_{n}} + k_{B}\ln Z  $, it follows that $ S $ can be neatly expressed in terms of the partition function $ Z $ as
	\begin{equation*}
		S = k_{B} \pdv{}{T}[T \ln Z]
	\end{equation*}
	Note that a canonical system's entropy is determined entirely by its temperature $ T $.
	
\end{itemize}

\subsubsection{Helmholtz Free Energy}
\textit{How is Helmholtz free energy defined in statistical physics? What is the expression for free energy in the canonical ensemble?}
\begin{itemize}
	\item Macroscopically, we define (Helmholtz) free energy $ F $ as
	\begin{equation*}
		F = \expval{E} - TS
	\end{equation*}
	In statistical mechanics, free energy captures the competition between minimizing energy and maximizing entropy in a system at fixed temperature. In Newtonian mechanics systems tend to minimize their total energy. However, in statistical mechanics with its large number of particles, minimizing energy and maximizing energy play a sort of balancing act. Take the two-state system as an example. In practice, such a system does not reside in the lowest possible energy state with all spins down (even though this would minimize the system's energy) because such a state minimizes entropy. Instead, the system is much more likely found in a state half spins up and half spins down, which maximizes entropy at the expense of higher energy.
	
	\item As in classical thermodynamics, the total differential $ \diff F $ of free energy is
	\begin{equation*}
		\diff F = \diff \expval{E} - T \diff S - S \diff T
	\end{equation*}
	by substituting in the first law of thermodynamics $ \diff \expval{E} = T\diff S - p \diff V $
	\begin{equation*}
		\diff F = - S \diff T - p \diff V
	\end{equation*}
	with the differentials $ \diff T $ and $ \diff V $ portray $ F $ as a function of temperature and volume $ F = F(T, V) $. From this expression, entropy and pressure are 
	\begin{equation*}
		S = -\pdv{F}{T}\bigg |_{V} \qquad \text{and} \qquad p = - \pdv{F}{V}\bigg |_{T}
	\end{equation*}
	
	\item Free energy is related to the canonical partition function $ Z $ by
	\begin{equation*}
		F = - k_{B} T \ln Z
	\end{equation*}
	The expression follows directly from the partition function definitions of $ \expval{E} $ and $ S $:
	\begin{align*}
		F &= \expval{E} - TS = \left(- \pdv{}{\beta} \ln Z\right) - T \left( k_{B} \pdv{}{T}[T \ln Z]\right)\\
		&= + k_{B}T^{2}\pdv{}{T} \ln Z - k_{B}T^{2} \pdv{}{T} \ln Z - k_{B}T \ln Z\\
		&=- k_{B}T \ln Z
	\end{align*}
	
\end{itemize}

\subsubsection{Equipartition Principle}
\textit{Discuss and derive the equipartition principle. How is the equipartition principle used to predict the specific heat of monatomic and diatomic gases?}

\begin{itemize}

	\item The canonical partition function for an ideal gas is (see Subsection \ref{sss:ideal_gas_part_func}) 
	\begin{equation*}
		Z(N, V, T) = \frac{V^{N}}{N!} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2}
	\end{equation*}
	
	\item We calculate the average energy of an ideal gas with the familiar equation
	\begin{equation*}
		\expval{E} = - \pdv{}{\beta} \ln Z = - \pdv{}{\beta}  \ln\left [\frac{V^{N}}{N!} \left(\frac{m}{2\pi \hbar^{2} \beta}\right)^{3N/2}\right ] = \frac{3N}{2\beta} = \frac{3}{2}Nk_{B}T
	\end{equation*}
	Note that each particle contributes an energy $ E_{1} = \frac{3}{2}k_{B}T $ to the average energy.
	
	\item The is an important, more general relationship called \textit{equipartition of energy} behind the formula $ E_{1} = \frac{3}{2}k_{B}T $ when we generalize our analysis to $ D $ dimensions. Because $ Z $ is multiplicative, the partition function generalizes to
	\begin{equation*}
		Z_{D} = \frac{V^{N}}{N!} \left(\frac{m}{2\pi \hbar^{2} \beta}\right)^{DN/2}
	\end{equation*}
	The corresponding average energy and energy per particle are
	\begin{equation*}
		E = - \pdv{}{\beta} \ln Z_{D} = \frac{D}{2}Nk_{B}T \qquad \text{and} \qquad E_{1} = \frac{D}{2}k_{B}T
	\end{equation*}
	The lesson is that in $ D $ dimensions, each particle contributes energy $ E_{1} = \frac{D}{2}k_{B}T $ to the average energy.
	
	\item More generally, for $ D $ degrees of freedom, each particle in a non-interacting classical systems contributes energy $ \frac{D}{2}k_{B}T $ to the average energy. Equipartition of energy states:
	\begin{quote}
		The average energy of each degree of freedom in a non-interacting classical system at temperature $ T $ is $ \expval{E} = \frac{1}{2} k_{B}T $.
	\end{quote}
	It follows that the average energy of a system of $ N $ particles in three dimensions is 
	\begin{equation*}
		E = 3N \cdot \frac{1}{2} k_{B}T = \frac{3}{2}Nk_{B}T
	\end{equation*}
	as predicted above for a three-dimensional monatomic ideal gas.
	
	\item Using the equipartition principle, the heat capacity of a monatomic ideal gas in three dimensions is simply
	\begin{equation*}
		C_{V} = \pdv{E}{T}\bigg|_{V} = \frac{3}{2}N k_{B}
	\end{equation*}
	and each particle contributes $ \frac{3}{2} k_{B} $ to the system's total heat capacity.
	
	\item Diatomic gases (discussed in detail in Subsection \ref{sss:diatomic_C_V}) are a bit more complicated because they have more degrees of freedom. These are
	The diatomic particles degrees of freedom are
	\begin{itemize}
		\item Three translational degrees of freedom
		\item Two rotational degrees of freedom about the two axes perpendicular to the axis of symmetry
		\item One vibrational degree of freedom corresponding to oscillation along the axis of symmetry.
	\end{itemize}
	For a total of 6 degrees of equipartition of energy would then predict 
	\begin{equation*}
		E = \frac{6}{2}Nk_{B}T = 3Nk_{B}T \eqtext{and} C_{V} = \pdv{E}{T}\bigg|_{V} = 3 N k_{B}
	\end{equation*}
	As we'll see in Subsection \ref{sss:diatomic_C_V}, the correct result is $ C_{V} = \frac{7}{2}Nk_{B}T$. The discrepancy is because our version of the equipartition principle applies only to \textit{non-interacting} degrees of freedom, but vibrational degree of freedom has associated potential energy. 
\end{itemize}




\subsection{Equations of State and the Virial Expansion}

\subsubsection{Common Questions}
\begin{enumerate}
    \item Derive the ideal gas partition function.

    \item How do we calculate the pressure of a gas with statistical physics? Derive the ideal gas law from the canonical ideal gas partition function.

    \item What is the virial equation of state and what is the second virial coefficient? What are some common models for inter-particle potentials? How is the second virial coefficient related to the potential energy interaction between neighboring particles in a canonical ensemble?

    \item Derive the second virial coefficient for a system of spherical particles described by a hard-core potential. Show how the expression can be used to derive the van der Waals equation of state.

\end{enumerate}


\subsubsection{Ideal Gas Partition Function} \label{sss:ideal_gas_part_func}
\textit{Derive the ideal gas partition function.}
\begin{itemize}
	\item The standard model for a so-called \textit{ideal gas} is a system of $ N $ non-interacting particles contained in a volume $ V $. Non-interacting means there is no potential between the particles and any collisions are elastic. 
	
	If the particles have no internal structure and thus no rotational or vibrational degrees of freedom, the gas is called an \textit{monatomic ideal gas}. The Hamiltonian for a monatomic ideal gas particle is simply the kinetic energy
	\begin{equation*}
		H = \frac{\bm{p}^{2}}{2m}
	\end{equation*}
	
	\item The partition function for a monatomic ideal gas particle is
	\begin{equation*}
		Z_{1}(V, T) = \frac{1}{h^{3}} \int e^{-\beta H} \diff \Gamma = \frac{1}{h^{3}} \int \exp(\frac{- \beta }{2m}\bm{p}^{2}) \diff^{3}\bm{r}\diff^{3}\bm{p}
	\end{equation*}
	The integral over position is simply the volume of the box: $ \int \diff^{3}\bm{r} = V $. The integral over momentum factors by components into integrals over $ p_{x}, p_{y} $ and $ p_{z} $ of the form
	\begin{equation*}
		\int_{-\infty}^{\infty} \exp(\frac{- \beta }{2m}p_{x,y,z}^{2})\diff p_{x,y,z}
	\end{equation*}
	This is a standard Gaussian-type integral with the general solution
	\begin{equation*}
		\int_{-\infty}^{\infty} e^{-ax^{2}} \diff x = \sqrt{\frac{\pi}{a}}
	\end{equation*}
	Putting the position and momentum pieces together, the single-particle partition function is
	\begin{equation*}
		Z_{1} = V \frac{1}{(2\pi \hbar)^{3}} \left(\frac{2\pi m}{\beta}\right)^{3/2} = V \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3/2}
	\end{equation*}
	
	\item $ Z_{1} $ is partition function for a single particle. The partition function is multiplicative for non-interacting systems, so the natural candidate for the $ N $-particle partition function is $ Z_{1}^{N} $. It turns out the correct expression is
	\begin{equation*}
		Z(N, V, T) = \frac{V^{N}}{N!} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2}
	\end{equation*}
	The normalization $ N! $ occurs because the gas particles are indistinguishable and arises from a concept called Gibbs' paradox.
	

	\item The combination of factors in the parentheses occurs often enough to warrant a special name and symbol, the \textit{thermal de Broglie wavelength} $ \lambda $
	\begin{equation*}
		\lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}}
	\end{equation*}
	In terms of $ \lambda $, the partition function for a monatomic ideal gas particle is $ Z_{1} = \frac{V}{\lambda^{3}} $. $ \lambda $ has dimensions of length (as expected for a wavelength) so $ Z_{1} $ is dimensionless, as it should be. $ \lambda $ can be thought of as the average de Broglie wavelength of a particle at temperature $ T $, while the partition function counts how many of these thermal wavelengths can fit in the volume $ V $.

	
	
\end{itemize}


\subsubsection{Ideal Gas Equation of State} \label{sss:ideal_gas_eq_state}
\textit{How do we calculate the pressure of a gas with statistical physics? Derive the ideal gas law from the canonical ideal gas partition function.}
\begin{itemize}
	
	\item To derive pressure, we start with free energy $ F = - k_{B}T \ln Z $ and evaluate $ p = - \pdv{F}{V} $. The initially intimidating simplifies to
	\begin{align*}
		p &= -\pdv{F}{V} = \pdv{}{V}\left( k_{B}T \ln\left [\frac{V^{N}}{N!} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2}\right ]\right)  \\
		&= k_{B} T\pdv{}{V} N \ln V = \frac{k_{B}T N}{V}
	\end{align*}
	This equation is the familiar ideal gas law $ pV = N k_{B}T $, which you may also have seen in the form $ pV = nRT $ where $ R $ is the ideal gas constant and $ n $ is the number of moles of particles in the system. Equations linking a system's pressure, volume, temperature and certain other quantities are called \textit{equations of state}. 
	
	\item The derivation rests on the knowledge that pressure is $ p = - \pdv{F}{V} $. There are few ways to get to this. Probably the simplest is to rearrange the total differential of free energy 
	\begin{equation*}
		\diff F = - S \diff T - p \diff V \implies p = -\pdveval{F}{V}{T}
	\end{equation*}
\end{itemize}



\subsubsection{Virial Equation of State}
\textit{What is the virial equation of state and what is the second virial coefficient? What are some common models for inter-particle potentials? How is the second virial coefficient related to the potential energy interaction between neighboring particles in a canonical ensemble?}

\smallskip
\textbf{Virial Expansion}
\begin{itemize}
	\item We can consider the ideal gas law as the limit of no interactions between gas particles. An ideal gas is a good approximation when density $ \frac{N}{V} $ is small, which suggests the ideal gas law is a low-density limit of a more general equation of state. This is the \textit{virial expansion}, the most general equation of state for a monatomic gas:
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} + B_{2}(T) \frac{N^{2}}{V^{2}} + B_{3}(T) \frac{N^{3}}{V^{3}} + \cdots
	\end{equation*}
	The temperature-dependent coefficients $ B_{j}(T) $ are call the \textit{virial coefficients}.
	
	\item The goal is to calculate the virial coefficients from a inter-particle potential $ U(r) $. Any inter-particle potential must have two key features: an attractive component scaling as $ \sim \frac{1}{r^{6}} $ and a rapidly rising, short-range repulsive component.
	
	The attractive $ \frac{1}{r^{6}} $ interaction arises from fluctuating electric dipoles whose interactions scale as $ \sim \frac{p_{1}p_{2}}{r^{3}} $. Although neutral particles don't have permanent dipoles, they can acquire a temporary dipole from quantum fluctuations. 
	
	The rapidly rising term arises from the Pauli exclusion principle, which prevents two particles from occupying the same space. 
	
	\item A conventional choice for the potential between monatomic gas particles is the \textit{Lennard-Jones potential}
	\begin{equation*}
		U(r) \sim \left(\frac{r_{o}}{r}\right)^{12} - \left(\frac{r_{o}}{r}\right)^{6}
	\end{equation*}
	A simpler (and also common) choice is the \textit{hard core repulsion}
	\begin{equation*}
		U(r) = 
		\begin{cases}
			\infty & r < r_{0}\\
			-U_{0}\left(\frac{r_{o}}{r}\right)^{6} & r \geq r_{0}
		\end{cases}
	\end{equation*}
\end{itemize}

\textbf{Deriving the Partition Function}
\begin{itemize}
	\item The Hamiltonian for an interacting monatomic gas of $ N $ particles with potential $ U(r) $ is
	\begin{equation*}
		H = \sum_{i=1}^{N} \frac{p_{i}^{2}}{2m} + \sum_{i>j}U(r_{ij})
	\end{equation*}
	where $ r_{ij} = \abs{\bm{r}_{i} - \bm{r}_{j}}$ denotes the separation between gas particles. The restriction $ i > j $ in the sum over $ U(r_{ij}) $ ensures we counts each pair of particles exactly once.
	
	\item The classical partition function for $ N $ particles is then
	\begin{equation*}
		Z(N, V, T) = \frac{1}{N!}\frac{1}{(2\pi \hbar)^{3N}} \int \prod_{i}^{N}e^{-\beta H} \diff^{3} p_{i} \diff^{3} r_{i} 
	\end{equation*}
	By separating the two terms in $ H $ and using the identity $ e^{a + b} = e^{a}e^{b} $, the integral can be factored into a momentum and position term
	\begin{equation*}
		\textstyle{Z = \frac{1}{N!}\frac{1}{(2\pi \hbar)^{3N}} \Big[\int \prod_{i}^{N} \exp \Big(-\beta \sum_{j} \frac{p_{j}^{2}}{2m}\Big) \diff^{3} p_{i}\Big] \Big[\int \prod_{i}^{N}\exp\Big(-\beta \sum_{j < k}U(r_{jk})\Big) \diff^{3} r_{i} \Big]}
	\end{equation*}
	The momentum term together with the normalization factor $ \frac{1}{(2\pi \hbar)^{3N}} $ is just like the momentum term in the partition function for an ideal gas and evaluates to $ \frac{1}{\lambda^{3N}} $, where $ \lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}} $ is the thermal de Broglie wavelength. The partition function simplifies slightly to
	\begin{equation*}
		Z = \frac{1}{N!}\frac{1}{\lambda^{3N}} \int \prod_{i}^{N}\exp \bigg (-\beta \sum_{j < k}U(r_{jk})\bigg ) \diff^{3} r_{i} 
	\end{equation*}
	
	\item There is no easy way to evaluate the position integral, since the potential term with $ U(r_{jk}) $ does not factor in an obvious way. At this point, we use approximation techniques to evaluate the integral.
	
	Instead of working directly with the sum over $ U(r_{ij}) $, we will use a quantity called the \textit{Mayer f-function}
	\begin{equation*}
		f(r) = e^{-\beta U(r)} -1
	\end{equation*}
	To construct a suitable expansion of the partition function in terms of $ f $, we define
	\begin{equation*}
		f_{ij} = f(r_{ij})
	\end{equation*}
	We then factor $ Z $'s exponential term using $ e^{xy} = e^{x}e^{y} \implies \exp \sum_{i} x_{i} = \prod_{i}e^{x_{i}} $ and write the partition function in terms of $ f $ as
	\begin{align*}
		Z(N, V, T) &= \frac{1}{N! \lambda^{3N}} \int \prod_{i}^{N} \prod_{j > k} e^{-\beta U(r_{jk})} \diff^{3} r_{i}   = \frac{1}{N! \lambda^{3N}} \int \prod_{i}^{N} \prod_{j > k}(1 + f_{jk})\diff^{3}r_{i}\\
		&=\frac{1}{N! \lambda^{3N}} \int \prod_{i}^{N}\left(1 + \sum_{j>k}f_{jk} + \sum_{j > k, l>m}f_{jk}f_{lm} + \cdots\right)\diff^{3}r_{i}
	\end{align*}
	The first term $  \int \prod_{i}^{N}\diff^{3}r_{i} $ contributes a volume factor to each integral for a total of $ V^{N} $. The second term involving $ \sum_{j>k} f_{jk}$ contributes terms of the form
	\begin{equation*}
		\int \prod_{i}^{N} \diff^{3}r_{i} f_{jk} = V^{N-2}\int \diff^{3}r_{j} \diff^{3}r_{k}f(r_{jk}) = V^{N-1}\int f(r) \diff^{3}r 
	\end{equation*}
	In the last equality, we have changed from integration over $ \bm{r}_{j} $ and $ \bm{r}_{k} $ to integration over the center of mass $ \bm{R} = \tfrac{1}{2}(\bm{r}_{j} + \bm{r}_{k}) $ and separation $ \bm{r} = \bm{r}_{j} - \bm{r}_{k} $. 
	
	We sum $ \sum_{j>k} f_{jk} $ such terms---one for each pair of particles. Formally there are $ \frac{1}{2}N(N-1) $ pairs, but for $ N \sim 10^{23} $ we can safely replace $ \frac{1}{2}N(N-1) $ with $ \frac{1}{2}N^{2} $. We then have $ \frac{N^{2}}{2} $ terms of the form $  V^{N-1}\int f(r) \diff^{3}r  $.
	
	\item If we expand the partition function in terms of the Mayer function $ f $ and ignore terms quadratic in $ f $, we can put the pieces together to get
	\begin{align*}
		Z(N, V, T) & =\frac{1}{N! \lambda^{3N}} \int \prod_{i}^{N}\left(1 + \sum_{j>k}f_{jk} + \cdots\right)\diff^{3}r_{i}\\
		&=\frac{1}{N! \lambda^{3N}} \left[V^{N} + \frac{N^{2}}{2}\left( V^{N-1}\int f(r) \diff^{3}r  \right) \right]
	\end{align*}
	Factoring out $ V^{N} $ and recognizing $ \frac{V^{N}}{N! \lambda^{3N}} = Z_{ideal}  $, we finally get
	\begin{equation*}
		Z(N, V, T) = \frac{V^{N}}{N! \lambda^{3N}} \left(1 + \frac{N^{2}}{2V} \int \diff^{3}r f(r) \right) = Z_{ideal} \left(1 + \frac{N^{2}}{V} B_{2}(T) \right) 
	\end{equation*}
\end{itemize}
	
\textbf{Recovering the Equation of State}
\begin{itemize}
	\item To show we are correct in our assignment of the second virial coefficient, we have to recover the virial equation of state. We do this with
	\begin{equation*}
		\frac{p}{k_{B}T} = \pdv{}{V}\ln Z = \pdv{}{V} \left[\ln Z_{ideal} + \ln(1 + \frac{N^{2}}{V} B_{2}(T)) \right]
	\end{equation*}
	In Subsection \ref{sss:ideal_gas_eq_state} we implicitly derived\footnote{Using definition of pressure $ p = -\pdv{F}{V} $ and the identity $ -\pdv{}{V} F_{ideal} = \frac{Nk_{B}T}{V} $ } the identity $  \pdv{}{V}\ln Z_{ideal} = \frac{N}{V} $, giving
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} +  \pdv{}{V}\ln(1 + \frac{N^{2}}{V} B_{2}(T)) 
	\end{equation*}
	To evaluate the second term, we have to make an approximation. Assuming fairly that $ \frac{N^{2}}{V} B_{2}(T) \ll 1 $, we can use $ \ln (1 + x) \approx x $ to get
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} +  \pdv{}{V}\frac{N^{2}}{V} B_{2}(T) = \frac{N}{V} - \frac{N^{2}}{V^{2}} B_{2}(T)
	\end{equation*}

	\item We see that up to second order in density $ \frac{N}{V} $ our result 
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} - \frac{N^{2}}{V^{2}} B_{2}(T)
	\end{equation*}
	almost matches the form of the virial expansion. If we are picky, we were off by a minus sign, and the correct definition is
	\begin{equation*}
		B_{2}(T) = - \frac{1}{2} \int f(r)\diff^{3} r = - 2\pi \int r^{2} \left (e^{-\beta U(r)} - 1\right ) \diff r
	\end{equation*}

	
	
\end{itemize}

\subsubsection{van Der Waals Equation and Second Virial Coefficient} \label{sss:vdW_derivation}
\textit{Derive the second virial coefficient for a system of spherical particles described by a hard-core potential. Show how the expression can be used to derive the van der Waals equation of state.}

\begin{itemize}
	\item We start with our result
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} - \frac{N^{2}}{2V^{2}} \int  f(r) \diff^{3}r
	\end{equation*}
	To interpret this further, we must evaluate $ \int  f(r) \diff^{3}r $ using our chosen potential.
	
	\item We consider an interaction in which the potential is repulsive at short distances and attractive at long distances. We will use the hard core repulsion
	\begin{equation*}
		U(r) = 
		\begin{cases}
			\infty & r < r_{0}\\
			-U_{0}\left(\frac{r_{o}}{r}\right)^{6} & r \geq r_{0}
		\end{cases}
	\end{equation*}
	The integral of the Mayer function $ f = e^{-\beta U(r)} - 1$ then reads
	\begin{align*}
		\int f(r)\diff^{3}r &= \int_{0}^{r_{0}} \left(e^{-\beta \cdot \infty} - 1\right)\diff^{3}r + \int_{r_{0}}^{\infty} \left(e^{\beta U_{0}\left(\frac{r_{0}}{r}\right)^{6}} - 1\right)\diff^{3}r\\
		&=\int_{0}^{r_{0}}(-1)\diff^{3}r + \int_{r_{0}}^{\infty} \left(e^{\beta U_{0}\left(\frac{r_{0}}{r}\right)^{6}} - 1\right)\diff^{3}r
	\end{align*}
	We will approximate the second integral with a first order Taylor expansion in the high-temperature limit where $ \beta U_{0} \ll 1$ and make use of spherical symmetry by integrating over solid angle in both integrals.
	\begin{align*}
		\int f(r)\diff^{3}r &= - 4\pi \int_{0}^{r_{0}}r^{2}\diff r + 4\pi \int_{r_{0}}^{\infty} \beta U_{0}\left(\frac{r_{0}}{r}\right)^{6} r^{2}\diff r\\
		&=\frac{4\pi r_{0}^{3}}{3}\left(\frac{U_{0}}{k_{B}T}-1\right)
	\end{align*}
	\vspace{-5mm}

	\item Inserting the expression for $ \int f(r)\diff^{3}r $ into our two-term virial expansion
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} - \frac{N^{2}}{2V^{2}} \int \diff^{3}r f(r),
	\end{equation*}
	Defining $ a = \frac{2\pi r_{0}^{3}U_{0}}{3} $ and $ b = \frac{2\pi r_{0}^{3}}{3} $ leads to the equation of state
	\begin{equation*}
		\frac{pV}{Nk_{B}T} = 1 -\frac{N}{V}\left(\frac{a}{k_{B}T} - b\right)
	\end{equation*}
	
	\item To get to the van der Waals equation, we write our equation of state in the form 
	\begin{equation*}
		k_{B}T = \frac{V}{N}\left(p + \frac{N^{2}}{V^{2}}a\right)\left(1 + \frac{N}{V}b\right)^{-1}
	\end{equation*}
	\textit{Derivation:} Multiplying the equation by $ k_{B}T $ and isolate $ k_{B}T $
	\begin{equation*}
		\frac{pV}{N} = k_{B}T - \frac{N}{V}a + \frac{Nk_{B}T}{V}b \implies \frac{pV}{N} + \frac{N}{V}a = k_{B}T\left(1 + \frac{N}{V}b\right)
	\end{equation*}
	Solve for $ k_{B}T $, then factor out $ \frac{V}{N} $ from the first term to get
	\begin{equation*}
		k_{B}T = \left(\frac{pV}{N} + \frac{N}{V}a\right)\left(1 + \frac{N}{V}b\right)^{-1} = \frac{V}{N}\left(p + \frac{N^{2}}{V^{2}}a\right)\left(1 + \frac{N}{V}b\right)^{-1}
	\end{equation*}
	
	
	
	\item Because we are working in an expansion of small density, we can expand the term $ \left(1 + \frac{N}{V}b\right)^{-1} $ using $ \frac{1}{1+x} \approx 1 - x $ which leads to
	\begin{equation*}
		k_{B}T = \left(p + \frac{N^{2}}{V^{2}}a\right)\left(\frac{V}{N} - b\right)
	\end{equation*}
	This is the famous van der Waals equation. It models monatomic gases with inter-particle interaction. Notice that in the limit $ a, b \to 0 $ (representing an absence of interaction) the van der Waals equation recovers the familiar ideal gas equation.
	
	\iffalse
	\item The van der Waals equation is valid only at low densities (because we used an expansion in small $ \frac{N}{V} $) and at high temperatures (because we made the approximation $ \beta U_{0} \ll 1 $) when evaluating the integral of the Mayer $ f $ function.
	
	Although our derivation used a hard core potential where the attractive component falls as $ \sim \frac{1}{r^{6}} $, the van der Waals equation successfully models other inter-atomic potentials that are repulsive at short range and attractive at long range, as long as the attractive potential falls as $ \frac{1}{r^{n}} $ where $ n \geq 4 $. Why $ n \geq 4 $? Evaluating the integral of the Meyer $ f $ function leads to an integral of the form $ \int \frac{r_{0}^{n}}{r^{n-2}} $, which converges for $ n \geq 4 $.
	
	In particular, the $ n \geq 4 $ restriction means the van der Waals equation does not apply to Coulomb interactions, which decrease as $ \frac{1}{r} $.
	
	\item A note on the parameters: $ a $ (which contains the factor $ U_{0} $ representing the depth of the potential well) models the attractive interaction between particles that takes effect at large distances. $ a $ reduces the pressure of the interacting gas compared to an ideal gas.
	
	The parameter $ b $ contains only $ r_{0} $ (the minimum distance two particles can approach) and models hard-core repulsion at short distances. $ b $ reduces the effective volume of the interacting gas compared to an ideal gas; more volume is taken up by the finite-size particles, so the gas's effective volume (the ``empty'' space) is reduces. 
	\fi

\end{itemize}



\subsection{Quantum Statistical Physics}

\subsubsection{Common Questions}
\begin{enumerate}
    \item Discuss symmetry and anti-symmetry of multi-particle equations of state under particle exchange. Which symmetry applies to bosons and which to fermions? How do we construct a properly behaving multi-particle equation of state from single particle equations of state?

    \item How do we normalize probability density in quantum statistical physics?

    \item What is density of states and how is it used in quantum statistical physics?

    \item Derive the expression for the occupation number of a quantum harmonic oscillator as a function of temperature.

    \item Discuss the analysis of heat capacity of a diatomic ideal gas with quantum statistical physics.

    \item What is the Einstein model of solids? Discuss the model's predictions for heat capacity.

    \item What is the Debye model? How are the lattice oscillations of a solid described in terms of the Debye model? What are the Debye frequency and temperature? Discuss the low-temperature behavior of a solid's heat capacity as predicted by the Debye model and the law of Dulong and Petit.

\end{enumerate}


\subsubsection{Multi-Particle Wave Functions}
\textit{Discuss symmetry and anti-symmetry of multi-particle equations of state under particle exchange. Which symmetry applies to bosons and which to fermions? How do we construct a properly behaving multi-particle equation of state from single particle equations of state?}

\begin{itemize}

	\iffalse
	\item Consider a system of $ N $ independent quantum particles with wave functions $ \psi_{1}(\bm{q}_{1}), \ldots,  \psi_{N}(\bm{q}_{N}) $. Because the particles are independent, the wave function for the entire system can be written as the product
	\begin{equation*}
		\psi(\bm{q}_{1}, \ldots, \bm{q}_{N}) = \psi_{1}(\bm{q}_{1}) \cdot \psi_{2}(\bm{q}_{2}) \cdot \cdots  \cdot \psi_{N}(\bm{q}_{N})
	\end{equation*}
	\fi
	\item There are two classes of fundamental particles: fermions, which have half-integer spin, and bosons, which have integer spin. Fermions obey the Pauli exclusion principle, which states that two particles cannot at the same time coexist in a state with all equal quantum numbers. Bosons do not obey the exclusion principle and tend to aggregate in states with the same quantum numbers.
	
	\item Quantum particles are indistinguishable, which has important consequences for quantum statistical mechanics. Consider two distinguishable particles $ A $ and $ B $, each of which can occupy either state $ \ket{0} $ or $ \ket{1} $. Both states have equal energies and are thus equally likely.
	
	There are four possible combined states: $ \ket{0} \ket{0} $, $ \ket{1}, \ket{1} $ and the mixed states $ \ket{0}\ket{1} $ and $ \ket{1}\ket{0} $.  
	
	But when both particles are indistinguishable, we can only identity three possible combined states. If the particles are bosons, these are $  \ket{00} $, $ \ket{11} $ and the mixed state $ \frac{1}{\sqrt{2}}(\ket{01} + \ket{10}) $. Boson wavefunctions are always symmetric under particle exchange.
	
	If the particles are fermions they cannot coexist in the same state, the only possible combined state
	is the asymmetric mixed state $ \frac{1}{\sqrt{2}}(\ket{01} - \ket{10}) $ where the minus ensures that the wave function vanishes if the two particles are put in the same state. Fermion wavefunctions are always antisymmetric under particle exchange.
	
	For three particles, we have
	For example, for three fermions $ 1, 2, 3 $ with states $ \alpha, \beta, \gamma $ we have
	\begin{align*}
		\psi_{b}(\bm{r}_1, \bm{r}_2, \bm{r}_3) \propto & + \phi_{\alpha}(\bm{r}_1)\phi_{\beta}(\bm{r}_2)\phi_{\gamma}(\bm{r}_3) & \psi_{f}(\bm{r}_1, \bm{r}_2, \bm{r}_3) \propto & + \phi_{\alpha}(\bm{r}_1)\phi_{\beta}(\bm{r}_2)\phi_{\gamma}(\bm{r}_3)\\ 
		&+\phi_{\alpha}(\bm{r}_3)\phi_{\beta}(\bm{r}_1)\phi_{\gamma}(\bm{r}_2) & &+\phi_{\alpha}(\bm{r}_3)\phi_{\beta}(\bm{r}_1)\phi_{\gamma}(\bm{r}_2)\\ 
		&+\phi_{\alpha}(\bm{r}_2)\phi_{\beta}(\bm{r}_3)\phi_{\gamma}(\bm{r}_1) & &+\phi_{\alpha}(\bm{r}_2)\phi_{\beta}(\bm{r}_3)\phi_{\gamma}(\bm{r}_1)\\ 
		&+\phi_{\alpha}(\bm{r}_3)\phi_{\beta}(\bm{r}_2)\phi_{\gamma}(\bm{r}_1) & &-\phi_{\alpha}(\bm{r}_3)\phi_{\beta}(\bm{r}_2)\phi_{\gamma}(\bm{r}_1)\\ 
		&+\phi_{\alpha}(\bm{r}_1)\phi_{\beta}(\bm{r}_3)\phi_{\gamma}(\bm{r}_2) & &-\phi_{\alpha}(\bm{r}_1)\phi_{\beta}(\bm{r}_3)\phi_{\gamma}(\bm{r}_2)\\ 
		&+\phi_{\alpha}(\bm{r}_2)\phi_{\beta}(\bm{r}_1)\phi_{\gamma}(\bm{r}_3) & &-\phi_{\alpha}(\bm{r}_2)\phi_{\beta}(\bm{r}_1)\phi_{\gamma}(\bm{r}_3)\\ 
	\end{align*}
	For fermions, all odd permutations of particles give a negative sign and all even permutations give a positive sign (to satisfy fermion antisymmetry under particle exchange) while for bosons all products have a positive sign (to satisfy boson symmetry under particle exchange).
	
	In general there are $ N! $ possible product states for $ N $ particles, e.g. $ 3! = 6 $ for $ 3 $ particles as above.
	
\end{itemize}

\subsubsection{Bohr-Sommerfeld Rule}
\textit{How do we normalize probability density in quantum statistical physics?}
\begin{itemize}

	\item The Bohr-Sommerfeld quantization rule, also called the quantum of action, states that the line integral over a closed trajectory of any particle in a bound state is
	\begin{equation*}
		\oint \bm{p} \cdot \diff \bm{q} = n h
	\end{equation*}
	where $ h $ is Planck's constant and $ n \in \mathbb{N} $ is an index ranging over the natural numbers. In other words, the integral $ \oint \bm{p} \cdot \diff \bm{q} $ can only take on integer multiples of Planck's constant, meaning a quantum particle's phase space is quantized and only certain quantum states are allowed.
	
	\item In terms of the volume of the phase space $ \Delta \Gamma $, the Bohr-Sommerfeld rule gives an expression for the total number of the system's states:
	\begin{equation*}
		\Omega = \frac{\Delta \Gamma}{N! h^{3N}}(2j+1)^{N}
	\end{equation*}
	The $ N! $ accounts for the possibility of only odd or even permutations of particle numbers in the construction of the total wavefunction, while the term $ (2j+1)^{N} $ accounts for distinguishing particles by their spin $ j $, which allows for more possible total states. 
	
	\item For a normalized the probability density the number of possible states must equal $ C \Delta \Gamma $, so the normalization factor in quantum statistical mechanics is
	\begin{equation*}
		C = \frac{(2j+1)^{N}}{N!h^{3N}}
	\end{equation*}

\end{itemize}


\subsubsection{Density of States}
\textit{What is density of states and how is it used in quantum statistical physics?}
%\textit{Discuss energy dependence of the probability density of a stationary state in quantum statistical physics. How do we calculate the expected value of an operator in quantum statistical physics?}

\begin{itemize}
	\item Partition functions involve sums over energy eigenstates, but sums are often hard to evaluate. In quantum statistical physics, we often replace the sum with an integral over a quantity called the density of states. 
	
	As an example, consider an ideal gas contained in a cubical box with side lengths $ L $ and volume $ V = L^{3}$. We assume there are no interactions between particles, so the energy eigenstates are plane waves of the form
	\begin{equation*}
		\psi = \frac{1}{\sqrt{V}}e^{i\bm{k}\cdot \bm{r}}
	\end{equation*}
	We impose periodic boundary conditions, so the wave vector $ \bm{k} = (k_{1}, k_{2}, k_{3}) $  is quantized as
	\begin{equation*}
		k_{i} = \frac{2\pi n_{i}}{L} \qquad n_{i} \in \mathbb{Z}
	\end{equation*}
	Using the notation $ k = \abs{\bm{k}} $, the energy of each (free) particle is
	\begin{equation*}
		E_{\bm{n}} = \frac{\hbar^{2}k^{2}}{2m} = \frac{4\pi^{2}\hbar^{2}}{2mL^{2}}n^{2}
	\end{equation*}
	where $ n^{2} = (n_{1}^{2} + n_{2}^{2} + n_{3}^{2}) $.
	
	\item The quantum mechanical partition function for a single gas particle is the sum over all energy eigenstates
	\begin{equation*}
		Z_{1} = \sum_{\bm{n}}e^{-\beta E_{\bm{n}}}
	\end{equation*}
	But how do we evaluate the sum? In quantum statistical physics, we usually approximate sums with an integral, justified by the close (i.e. essentially continuous) spacing of the energy levels. For a more rigorous justification, note that up to a factor $ \pi $, the exponents in the sum are of the form 
	\begin{equation*}
		\beta E_{\bm{n}} \sim \frac{\lambda^{2}n^{2}}{L^{2}}
	\end{equation*}
	where $ \lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}} $ is the particles' thermal de Broglie wavelength. For a macroscopic size box, $ \lambda \lll L $. This means there are a great many states with $ E_{\bm{n}} \leq k_{B}T $ (for which $ e^{-\beta E_{n}} $ is non-negligible) which contribute appreciable to the sum, so we can safely approximate the sum with an integral. 
	
	\item We write change from summation to integration using the identity $ \diff n = \frac{L}{2\pi} \diff k $ and integrating $ k $ over solid angle.
	\begin{equation*}
		\sum_{\bm{n}} \approx \int \diff^{3}n = \frac{L^{3}}{(2\pi)^{3}}\int \diff^{3} k = \frac{4\pi V}{(2\pi)^{3}}\int_{0}^{\infty}k^{2} \diff k
	\end{equation*}
	It is conventional to change to integration over $ E $, which turns out to be more useful for future applications. If we assume a quadratic dispersion relation we have
	\begin{equation*}
		E = \frac{\hbar^{2}k^{2}}{2m} \implies \diff E = \frac{\hbar^{2}k}{m}\diff k \qquad \text{and} \qquad k^{2} = \frac{2mE}{\hbar^{2}}
	\end{equation*}
	
	\item Using the dispersion relation, we can write the integral over $ k $ as
	\begin{equation*}
		\frac{4\pi V}{(2\pi)^{3}}\int_{0}^{\infty}k^{2} \diff k = \frac{V}{2\pi^{2}} \int \sqrt{\frac{2mE}{\hbar^{2}}} \frac{m}{\hbar^{2}} \diff E \equiv \int g(E) \diff E
	\end{equation*}
	where we have defined the so-called \textit{density of states}
	\begin{equation*}
		g(E) \equiv \frac{V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \sqrt{E}
	\end{equation*}
	The quantity $ g(E)\diff E $ is dimensionless and counts the number of states with energy $ E $ between $ E $ and $ E + \diff E $ (note that $ g(E) $, without the $ \diff E $, has units $ \si{\joule^{-1}} $). 	
	
	Note that we haven't actually performed an integral over $ \int g(E) \diff E $. Instead, the density of states is used as a measure which we can integrate over any function $ f(E) $, in which case we would be interested in the quantity $ \int f(E) g(E) \diff E $. 
	
	\item The density of states is introduced to replace a summation of any quantity over the eigenstates $ \sum_{\bm{n}} $ with an integration of the same quantity over the density of states. 
	\begin{equation*}
		\sum_{n} \to \int g(E) \diff E 
	\end{equation*}
	Viewed pragmatically, the density of states is a way to avoid evaluating the difficult summation over eigenstates; the integral is usually easier to evaluate. In practice, for some quantity $ f(E) $, we would write
	\begin{equation*}
		\sum_{n} f(E) \approx \int f(E) g(E) \diff E 
	\end{equation*}

	\item The density of states depends on the system's dispersion relation. For relativistic systems, the dispersion relation (using the relativistic equation for energy) is
	\begin{equation*}
		E = \sqrt{\hbar^{2}k^{2}c^{2} + m^{2}c^{4}}
	\end{equation*}
	An analogous (if slightly cumbersome) derivation to the one above produces the density of states
	\begin{equation*}
		g(E) = \frac{VE}{2\pi^{2}\hbar^{3}c^{3}} \sqrt{E^{2} - m^{2}c^{4}}
	\end{equation*}
	For massless particles, the density of states is
	\begin{equation*}
		g(E) = \frac{VE^{2}}{2\pi^{2}\hbar^{3}c^{3}}
	\end{equation*}
	Notice that for relativistic particles $ g(E) \sim E^{2} $, quite different from the classical behavior $ g(E) \sim E^{1/2} $/
\end{itemize}

\subsubsection{Quantum Harmonic Oscillator} \label{sss:quantum_harm_osc}
\textit{Derive the expression for the occupation number of a quantum harmonic oscillator as a function of temperature.}
	
\begin{itemize}
	\item The quantum harmonic oscillator has eigenstates with energy
	\begin{equation*}
		E_n = \hbar \omega \left (n + \frac{1}{2}\right )
	\end{equation*}
	where $ \omega  $ is the angular frequency of oscillation, and the partition function is
	\begin{equation*}
		Z = \sum_{n = 0}^{\infty} \exp(-\beta E_n) = \sum_{n=0}^{\infty} \exp(-\beta n \hbar \omega - \frac{\beta}{2}\hbar \omega) = e^{-\beta \hbar \omega/2} \sum_{n=0}^{\infty}e^{-\beta n \hbar \omega}
	\end{equation*}
	The sum is evaluated with as geometric series of the form $ \frac{1}{1-x} = \sum x^{n} $
	\begin{equation*}
		Z = \frac{ e^{-\beta \hbar \omega/2}}{1 - e^{-\beta \hbar \omega}}
	\end{equation*}

	\item Using the partition function, we can calculate the average energy:
	\begin{align*}
		\expval{E} &= - \pdv{}{\beta} \ln Z = \frac{ \hbar \omega}{2} + \frac{\hbar \omega e^{-\beta \hbar \omega}}{1- e^{-\beta \hbar \omega}} =   \hbar \omega\left(\frac{1}{2} + \frac{e^{-\beta \hbar \omega}}{1- e^{-\beta \hbar \omega}} \right)\\
		&=\hbar \omega \left( \frac{1}{2} + \frac{1}{e^{\beta \hbar \omega } - 1}\right)
	\end{align*}
	Comparing this expression to the energy eigenvalues $ E_n = \hbar \omega \left(\frac{1}{2} + n \right)  $ we can deduce that the average occupied state is
	\begin{equation*}
		\expval{n} = \frac{1}{e^{\beta \hbar \omega } - 1}
	\end{equation*}

\end{itemize}

\subsubsection{Heat Capacity of a Diatomic Ideal Gas} \label{sss:diatomic_C_V}
\textit{Discuss the analysis of heat capacity of a diatomic ideal gas with quantum statistical physics.}

\smallskip
\textbf{Degrees of Freedom}\\
In a simple model of a diatomic gas, consisting of two point masses attached by a spring, we must consider translational, rotational and vibrational degrees of freedom. The diatomic particles' degrees of freedom are
\begin{itemize}
	\item Three translational degrees of freedom
	\item Two (not three) rotational degrees of freedom about the two axis perpendicular to the axis of symmetry (rotation about the axis of symmetry has negligible moment of inertia and is ignored).
	\item One vibrational degree of freedom corresponding to oscillation along the axis of symmetry.
\end{itemize}
\textit{Important:} In our simple diatomic model, we consider the rotational and vibrational degrees of freedom to be independent, so we can factor the partition function of a single molecule into the independent contributions from each degree of freedom
\begin{equation*}
	Z_{trans} = Z_{trans}Z_{rot}Z_{vib}
\end{equation*}

\textbf{Translation}\\
The translation degree of freedom is the same as for a monatomic ideal gas, which we know to be
\begin{equation*}
	Z_{1} = V \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3/2}
\end{equation*}
where $ m $ is the mass of the gas molecule

\smallskip
\textbf{Rotation, Classical Approach}
\begin{itemize}
	\item The plan is to derive the rotational Hamiltonian $ H_{rot} $ from the Lagrangian $ L_{rot} $ using Hamiltonian mechanics, and use the Hamiltonian to calculate the partition function by integrating over $ e^{-\beta H_{rot}} $. 
	
	\item From classical mechanics, the Lagrangian for two rotational degrees of freedom with equal moments of inertia $ I $ is
	\begin{equation*}
		L_{rot} = \frac{1}{2} I\left(\dot{\theta}^{2} + \sin^{2}\theta \dot{\phi}^{2}\right)
	\end{equation*}
	The conjugate momenta (don't worry if this is unfamiliar; it comes from Hamiltonian dynamics but is largely irrelevant to the understanding of diatomic gases) are
	\begin{equation*}
		p_{\theta}  = \pdv{L_{rot}}{\dot{\theta}} = I \dot{\theta} \qquad \text{and} \qquad p_{\phi}  = \pdv{L_{rot}}{\dot{\phi}} = I \sin^{2}\theta \dot{\phi}
	\end{equation*}
	From the conjugate momenta, we get the Hamiltonian $ H_{rot} $ form
	\begin{equation*}
		H_{rot} = \dot{\theta}p_{\theta} + \dot{\phi}p_{\phi} - L_{rot} = \frac{p_{\theta}^{2}}{2I} + \frac{p_{\phi}^{2}}{2I\sin^{2}\theta}
	\end{equation*}
	
	\item The rotational contribution the partition function is thus
	\begin{align*}
		Z_{rot} &= \frac{1}{(2\pi \hbar)^{2}} \int e^{-\beta H_{rot}} \diff \theta \diff \phi \diff p_{\theta} \diff p_{\phi}\\
		& = \frac{1}{(2\pi \hbar)^{2}} \int \exp(-  \frac{\beta p_{\theta}^{2}}{2I} -  \frac{\beta p_{\phi}^{2}}{2I\sin^{2}\theta}) \diff \theta \diff \phi \diff p_{\theta} \diff p_{\phi}\\
		&= \frac{1}{(2\pi \hbar)^{2}}  \int_{0}^{\pi} \diff \theta  \int_{0}^{2\pi} \diff \phi \int_{-\infty}^{\infty} \exp(-  \frac{\beta p_{\theta}^{2}}{2I}) \diff p_{\theta} \int_{-\infty}^{\infty} \exp(-  \frac{\beta p_{\phi}^{2}}{2I\sin^{2}\theta}) \diff p_{\phi}\\
		&=\frac{1}{(2\pi \hbar)^{2}} \sqrt{\frac{2\pi I}{\beta}}  \int_{0}^{2\pi} \diff \phi  \int_{0}^{\pi} \sqrt{\frac{2\pi I \sin^{2}\theta}{\beta}} \diff \theta = \frac{2Ik_{B}T}{\hbar^{2}}
	\end{align*}
	Note the normalization factor is $\frac{1}{h^{2}}$ (and not $ \frac{1}{h^{3}} $ as in $ Z_{trans} $) because there are only two rotational degrees of freedom.
	
	\item The rotational contribution to average energy is, using the familiar $ E = -\pdv{}{\beta} \ln Z $
	\begin{equation*}
		E_{rot} = - \pdv{}{\beta} \ln(\frac{2I}{\hbar^{2} \beta}) = -\pdv{}{\beta} \ln \frac{1}{\beta} = \frac{1}{\beta} = k_{B}T
	\end{equation*}
	Similarly rotation contribution to heat capacity is simply
	\begin{equation*}
		C_{V_{rot}} = \pdv{E_{rot}}{T} \bigg |_{V} = k_{B}
	\end{equation*}
	Both results could be predicted from equipartition of energy; for 2 degrees of freedom, the average energy per particle is $ \frac{2}{2}k_{B}T = k_{B}T $.
	
\end{itemize}

\textbf{Rotation, Quantum Approach}
\begin{itemize}
	\item The quantum Hamiltonian for rotation has energy eigenvalues
	\begin{equation*}
		E_{j} = \frac{\hbar^{2}}{2I}j(j+1)
	\end{equation*}
	where $ j $ corresponds to total angular momentum and the degeneracy of each energy level is $ 2j + 1 $. Making sure to include the degeneracy factor, the quantum rotational partition function for a single molecule is
	\begin{equation*}
		Z_{rot} = \sum_{j} (2j + 1)e^{-\beta E_{j}} =  \sum_{j} (2j + 1) \exp(\frac{-\beta\hbar^{2}}{2I}j(j+1))
	\end{equation*}
	
	\item In the high temperature limit $ T \gg \frac{\hbar^{2}}{2Ik_{B}} $ the energy levels are densely populated and we can safely approximate the sum with an integral, making the substitution $  u = \frac{\beta \hbar^{2}}{2I}j(j+1) $ to get
	\begin{equation*}
		Z_{rot} \approx \int_{0}^{\infty}(2j + 1) \exp(\frac{-\beta\hbar^{2}}{2I}j(j+1)) \diff j = - \frac{2I}{\beta\hbar^{2}} \int_{0}^{\infty} e^{u}\diff u=  \frac{2I}{\beta \hbar^{2}}
	\end{equation*}
	This agrees with the result $ Z_{rot} = \frac{2I}{\beta \hbar^{2}} $ in the classical analysis.
	
	\item In the low temperature limit $ T \ll \frac{\hbar^{2}}{2Ik_{B}} $ the exponent $ e^{-\beta E_{j}} $ is very small for $ j \neq 0 $. Only the term with $ j = 0 $ contributes appreciably to the sum and we have $ Z_{rot} \approx 1 $.
	
	It follows that $ E_{rot} = -\pdv{}{\beta}\ln Z_{rot} \approx 0 $ at low temperature, so $ C_{V_{rot}} = \pdv{E}{T} \approx 0 $. In other words, the rotational degrees of freedom are frozen out at low temperature. This occurs because the thermal energy $ k_{B}T $ is not large enough to excite the rotational modes from the ground state. 
	
	Freezing out of the rotational modes occurs near the characteristic rotational temperature
	\begin{equation*}
		 T_{rot} = \frac{\hbar^2}{2Ik_B}
	\end{equation*}
	$ T_{rot} $ is of the order $ \SI{80}{K} $ for helium and $ \SI{10}{K} $ for other diatomic gases.

\end{itemize}

\smallskip
\textbf{Vibration, Classical Approach}
\begin{itemize}
	\item The Hamiltonian for a single vibrational degree of freedom is the familiar harmonic oscillator. If $ \xi $ is the displacement from the equilibrium and $ \omega $ is the vibration frequency, the Hamiltonian is		
	\begin{equation*}
		H_{vib} = \frac{p_{\xi}^{2}}{2m} + \frac{1}{2} m \omega^{2}\xi^{2}
	\end{equation*}
	By the way, the presence of a potential energy term $ \frac{1}{2} m \omega^{2}\xi^{2} $ means that our formulation of equipartition of energy does not apply to vibration. 
	
	\item From the Hamiltonian, we can compute the vibrational contribution to the partition function
	\begin{align*}
		Z_{vib} &= \frac{1}{2\pi \hbar} \int e^{-\beta H_{vib}} \diff \xi \diff p_{\xi} = \frac{1}{2\pi \hbar} \int \exp(-\beta \frac{p_{\xi}^{2}}{2m} -\beta  \frac{1}{2} m \omega^{2}\xi^{2}) \diff \xi \diff p_{\xi}\\
		&=\frac{1}{2\pi \hbar} \int_{-\infty}^{\infty} \exp(-\beta \frac{p_{\xi}^{2}}{2m})\diff p_{\xi} \int_{-\infty}^{\infty}\exp(-\beta  \frac{1}{2} m \omega^{2}\xi^{2}) \diff \xi \\
		&=\frac{1}{2\pi \hbar} \sqrt{\frac{2\pi m}{\beta}} \sqrt{\frac{2\pi}{\beta m \omega^{2}}} = \frac{k_{B}T}{\hbar \omega}
	\end{align*}
	
	\item The average vibrational energy of a gas molecule is $ E_{vib} = -\pdv{}{\beta} \ln Z  $
	\begin{equation*}
		E_{vib} = -\pdv{}{\beta} \ln(\frac{1}{\hbar \omega \beta}) = -\pdv{}{\beta} \ln(\frac{1}{\beta}) = \frac{1}{\beta} = k_{B}T
	\end{equation*}
	As for rotation, the contribution to heat capacity is
	\begin{equation*}
		C_{V_{vib}} = \pdv{E_{vib}}{T} \bigg |_{V} = k_{B}
	\end{equation*}
\end{itemize}

\textbf{Vibration, Quantum Approach}
\begin{itemize}
	\item We use the familiar harmonic oscillator with energy eigenvalues
	\begin{equation*}
		E_{n} = \hbar \omega \left(n + \frac{1}{2}\right)
	\end{equation*}
	The corresponding partition function is
	\begin{equation*}
		Z_{vib} = \sum_{n} e^{-\beta E_{n}} = e^{-\beta \hbar \omega /2 } \sum_{n} e^{-\beta \hbar \omega n }
	\end{equation*}
	Recognizing the familiar sum of the geometric series $ \sum_{n}e^{-xn} = \frac{1}{1-e^{x}}$ we write
	\begin{equation*}
		Z_{vib} = \frac{e^{-\beta \hbar \omega /2 }}{1 - e^{-\beta \hbar \omega }} = \frac{1}{2 \sinh(\beta \hbar \omega / 2)}
	\end{equation*}
	
	\item In the high-temperature limit $ k_{B} T \gg \hbar \omega \implies \beta \hbar \omega \ll 1$  we can approximate the hyperbolic sine with a Taylor series $ \sinh x \approx x + \cdots $ to get
	\begin{equation*}
		Z_{vib} \approx \frac{1}{2} \frac{2}{\beta \hbar \omega} = \frac{k_{B}T}{\hbar \omega} \qquad (\beta \hbar \omega \ll 1)
	\end{equation*}
	in agreement with the classical result.
	
	\item At low temperatures, $ \beta \hbar \omega \gg 1 $ and $ e^{-\beta \hbar \omega n} \approx 0$ for $ n > 0 $, so only the term $ n = 0 $ contributes appreciably to the partition function. The result is
	\begin{equation*}
		Z_{vib} = e^{-\beta \hbar \omega /2 } \sum_{n} e^{-\beta \hbar \omega n } \approx  e^{-\beta \hbar \omega /2 }  \qquad (\beta \hbar \omega \gg 1)
	\end{equation*}
	In this case we have 
	\begin{equation*}
		E_{vib} = - \pdv{}{\beta} \ln Z_{vib}\approx - \pdv{}{\beta}e^{-\beta \hbar \omega /2 } = \frac{\hbar \omega}{2}
	\end{equation*}
	which is the familiar zero-point energy of the quantum harmonic oscillator, while the heat capacity is
	\begin{equation*}
		C_{V_{vib}} = \pdv{E}{T} \approx \pdv{}{T} \frac{\hbar \omega}{2} = 0 \qquad (\beta \hbar \omega \gg 1)
	\end{equation*}
	As for rotation, the vibrational modes are frozen out at low temperature, in agreement with experiment. In fact, it turns out this temperature is not that low at all. Vibrataional modes begin to freeze out near the characteristic temperature
	\begin{equation*}
		T_{vib} = \frac{\hbar \omega}{k_B} \quad \implies \quad \frac{T_{vib}}{T} = \beta \hbar \omega
	\end{equation*}
	$ T_{vib} $ is of the order $ \SI{1000}{K} $ for typical diatomic molecules, well above room temperature!
\end{itemize}


\smallskip
\textbf{Summary and Note on Quantum Effects}
\begin{itemize}
	\item The classical predictions
	\begin{align*}
		&E = E_{trans} + E_{rot} + E_{vib} = \frac{7}{2}k_{B}T\\
		&C_{V} = C_{V_{trans}} +	C_{V_{rot}} + C_{V_{vib}} = \frac{7}{2}k_{B}
	\end{align*}
	only agrees with experiment in the high temperature limit. As the temperature is lowered, experiment suggests the vibrational and then the rotational modes become frozen out and do not contribute to heat capacity.
	
	\item We can theoretically predict this behavior with a quantum approach, in which we expect degrees of freedom to be frozen out when the discrete minimum amount of energy needed to excite the degree of freedom exceeds the thermal energy $ k_{B}T $. For example, the freezing out of high-frequency photons in the Planck distribution in the case of blackbody radiation resolved the classical ultraviolet catastrophe and led to a reduced temperature heat capacity for phonons.
	
	\item Finally, a more thorough analysis of the low-temperature limits would show that the rotational and vibrational heat capacities approach $ T = 0 $ as
	\begin{equation*}
		C_{V_{rot}} \sim \frac{T_{rot}^{2}}{T^{2}} \exp(-\frac{T_{rot}}{T}) \eqtext{and} C_{V_{vib}} \sim \frac{T_{vib}^{2}}{T^{2}} \exp(-\frac{T_{vib}}{T})
	\end{equation*}
	Note that both expressions still predict $ C_{V} \to 0 $ as $ T \to 0 $.
\end{itemize}





\subsubsection{Einstein Model} \label{sss:einstein_model}
\textit{What is the Einstein model of solids? Discuss the model's predictions for heat capacity.}

\begin{itemize}
	\item The Einstein model models a solid as a lattice of uncoupled harmonic oscillators (think of molecules in a crystal lattice connected by ideal springs). From Subsection \ref{sss:quantum_harm_osc}, the average energy harmonic oscillator is
	\begin{equation*}
		\expval{E} = \hbar \omega \left(\frac{1}{2} + \frac{1}{e^{\beta \hbar \omega}-1}\right)
	\end{equation*}
	where $ \omega $ is the frequency of oscillation. The associated heat capacity is
	\begin{equation*}
		C = \pdv{\expval{E}}{T} =  -\frac{1}{k_{B}T^{2}} \pdv{\expval{E}}{\beta} = \frac{(\hbar \omega)^{2}}{k_{B}T^{2}}\frac{e^{\beta \hbar \omega}}{(e^{\beta \hbar \omega}-1)^{2}}
	\end{equation*}
	
	\item In the low-temperature limit $ k_{B}T \ll \hbar \omega \implies \beta \hbar \omega \gg 1 $, the denominator is $ e^{\beta \hbar \omega}-1 \approx e^{\beta \hbar \omega} $ and the Einstein model predicts a heat capacity
	\begin{equation*}
		C \approx \frac{(\hbar \omega)^{2}}{k_{B}T^{2}}e^{-\beta \hbar \omega}
	\end{equation*}
	In other words, the Einstein model predicts a low-temperature heat capacity scaling as $ C \sim e^{-1/T}$. This does not agree with experiment---the observed behavior in solids is $ C \sim T^{3} $, which is predicted by the more refined Debye model, discussed below.
	
\end{itemize}

\subsubsection{Debye Model}
\textit{What is the Debye model? How are the lattice oscillations of a solid described in terms of the Debye model? What are the Debye frequency and temperature? Discuss the low-temperature behavior of a solid's heat capacity as predicted by the Debye model and the law of Dulong and Petit.}

\smallskip
\textbf{The Debye Model}
\begin{itemize}
	\item The Debye model describes lattice oscillations in a solid as sound waves of discrete quantum packets called \textit{phonons} (just like electromagnetic waves are modeled by photons). Taking the photon-phonon analogy further, we take the energy of a phonon to be of the form $ E = \hbar \omega $. Assuming the dispersion relation $ \omega = k c_{s} $ the energy of a phonon is
	\begin{equation*}
		E = \hbar \omega = \hbar k c_{s}
	\end{equation*}
	where $ c_{s} $ is the speed of sound.
	
	\item Phonons have three polarization states (two transverse polarizations for the two axes transverse to the direction of propagation and one longitudinal polarization). The phonon density of states is
	\begin{equation*}
		g(E)\diff E = \frac{3VE^{2}}{2\pi^{2}\hbar^{3}c^{3}} \diff E
	\end{equation*}
	which is the general density of states for massless relativistic particles, multiplied by a factor of three to account for the three polarization states.
	
	In terms of frequency, using $ E = \hbar \omega $ and $ \diff E = \hbar \diff \omega $, the number of energy states available to a phonon with frequency between $ \omega  $ and $ \omega + \diff \omega $ is 
	\begin{equation*}
		g(\omega)\diff \omega = \frac{3V}{2\pi^{2}c_{s}^{3}} \omega^{2} \diff \omega
	\end{equation*}
	
	\item Important concept: phonons in solids are bounded by a maximum possible frequency (or a minimum possible wavelength). A minimum wavelength should make intuitive sense. Sound can only propagate as vibrations through a material medium, and phonons with wavelength smaller than a solid's inter-molecular spacing will encounter empty space in the inter-molecular regions. Such phonons have no material medium for sound propagation, so phonons are limited to wavelengths comparable to or larger than the inter-atomic spacing. 
	
	The inter-atomic spacing is proportional to the $ \left(\frac{V}{N}\right)^{1/3} $, so we expect
	\begin{equation*}
		\lambda_{min} \sim \left(\frac{V}{N}\right)^{1/3} \eqtext{or} \omega_{max} \equiv \omega_{D} \sim \left(\frac{V}{N}\right)^{1/3} c_{s}
	\end{equation*}
	The maximum frequency is called the \textit{Debye frequency} in honor of Debye and carries his initial. The Debye frequency is found by equating the number of single phonon states $ \int g(\omega)\diff \omega $ to the number of degrees of freedom in a solid. The number of single phonon states (bounded by the maximum frequency $ \omega_{D} $) is
	\begin{equation*}
		\int_{0}^{\omega_{D}} g(\omega)\diff \omega  = \int_{0}^{\omega_{D}}\frac{3V}{2\pi^{2}c_{s}^{3}} \omega^{2} \diff \omega = \frac{V\omega_{D}^{3}}{2\pi^{2}c_{s}^{3}}
	\end{equation*}
	The number of degrees of freedom in a crystal lattice of $ N $ atoms is $ 3N $ since each atom can move in three spatial dimensions. Equating the two gives
	\begin{equation*}
		\frac{V\omega_{D}^{3}}{2\pi^{2}c_{s}^{3}} = 3N \implies \omega_{D} = \left(\frac{6\pi^{2}N}{V}\right)^{\frac{1}{3}}c_{s}
	\end{equation*}
	Note the proportionality $ \omega_{D} \sim \left(\frac{V}{N}\right)^{1/3} c_{s} $, as expected. 
	
	\item The Debye frequency gives rise to an associated energy scale and temperature. Following the pattern $ E = \hbar \omega $, the Debye energy is simply $ \hbar \omega_{D} $. The Debye temperature comes from equating the thermal energy $ k_{B}T $ to the Debye energy. The result is
	\begin{equation*}
		T_{D} = \frac{\hbar \omega_{D}}{k_{B}}
	\end{equation*}
	The Debye temperature is the temperature at which the highest frequency phonon (i.e. phonon with frequency $ \omega_{D} $) becomes thermally excited. $ T_{D} $ is near room temperature for most solids. Toward the extreme ends of the spectrum are lead with $ T_{D} \approx \SI{100}{\kelvin} $ and diamond with $ T_{D} \approx \SI{2000}{\kelvin} $. 
	
\end{itemize}
\textbf{Debye Model Prediction for Heat Capcity}
\begin{itemize}
	\item First, we construct the phonon partition function. First, we find the single-phonon partition function $ Z_{\omega} $ at fixed frequency, then integrate over all frequencies to get the complete partition function. 
	
	Phonon number is not conserved, so we must count over all possible phonon numbers when constructing $ Z_{\omega} $. Using the geometric series $ \sum x^{n} = \frac{1}{1 - x}$ to evaluate the sum, the result is
	\begin{equation*}
		Z_{\omega} = \sum_{N=1}^{\infty}e^{-N \beta\hbar \omega} = 1 + e^{-\beta \hbar \omega} + \cdots = \frac{1}{1 - e^{-\beta \hbar \omega}}
	\end{equation*}
	
	\item Next, we need to integrate over all possible frequencies. We will take advantage of the fact that independent partition functions are multiplicative, which means that the logarithms of the partition functions add. In terms of $ Z_{\omega} $, the number of phonon states at fixed frequency $ \omega $, the complete partition function for a phonon gas is
	\begin{equation*}
		\ln Z = \int_{0}^{\omega_{D}} \ln Z_{\omega}g(\omega)\diff \omega = - \frac{3V}{2\pi^{2}c_{s}^{3}}\int_{0}^{\omega_{D}} \omega^{2} \ln(1-e^{-\beta \hbar \omega})\diff \omega
	\end{equation*}
	
	\item The total energy in the phonon sound waves is (using the identity $\frac{e^{-x}}{1-e^{-x}} = \frac{1}{e^{x} -1}$)
	\begin{equation*}
		E = -\pdv{}{\beta} \ln Z = \frac{3V\hbar}{2\pi^{2}c_{s}^{3}} \int_{0}^{\omega_{D}}\frac{\omega^{3}}{e^{\beta\hbar\omega}-1} \diff \omega
	\end{equation*}
	There is important information in integrand: it is the phonon energy distribution, giving the amount of energy carried by photons with frequency between $ \omega $ and $ \omega + \diff \omega$. It has the same form as the Planck distribution for photons and reads
	\begin{equation*}
		E(\omega) \diff \omega = \frac{3V\hbar}{2\pi^{2}c_{s}^{3}}\frac{\omega^{3}}{e^{\beta \hbar \omega} - 1}\diff \omega
	\end{equation*}
	
	\item To actually solve the integral for phonon energy $ E $, we first change variables to $ x = \beta \hbar \omega $, so the upper limit becomes $ x_{D} = \beta \hbar \omega_{D} = \frac{T_{D}}{T} $. The energy is
	\begin{equation*}
		E = \frac{3V(k_{B}T)^{4}}{2\pi^{2}(\hbar c_{s})^{3}} \int_{0}^{T_{D}/T} \frac{x^{3}}{e^{x}-1}\diff x
	\end{equation*}
	The resulting parameter-dependent integral has no analytic solution; it is a function of $ \frac{T_{D}}{T} $. We satisfy ourselves with the limit scenarios $ T\ll T_{D} $ and $ T \gg T_{D} $, for which we can find analytic solutions.
	
	\item In the low-temperature limit $ T \ll T_{D} $ we have $ \frac{T_{D}}{T} \to \infty $, and we can use the tabulated integral\footnote{Of Planck distribution fame} $ \int_{0}^{\infty}\frac{x^{3}}{e^{x}-1}\diff x = \frac{\pi^{4}}{15}$ to get
	\begin{equation*}
		E = \frac{\pi^{2}Vk_{B}^{4}}{10 \hbar^{3} c_{s}^{3}}T^{4}
	\end{equation*}
	The low-temperature heat capacity $ C_{V} = \pdv{E}{T} $  is
	\begin{equation*}
		C_{V} = \frac{2\pi^{2}Vk_{B}^{4}}{5\hbar^{3} c_{s}^{3}T^{3}} \equiv \frac{12\pi^{4}}{5}Nk_{B}\left(\frac{T}{T_{D}}\right)^{3}
	\end{equation*}
	where the last equality uses the identity
	\begin{equation*}
		T_{D} = \frac{\hbar \omega_{D}}{k_{B}} = \frac{\hbar}{k_{B}} \left(\frac{6\pi^{2}N}{V}\right)^{\frac{1}{3}}c_{s}
	\end{equation*}
	The low-temperature behavior $ C \sim T^{3} $ agrees with experiment, as mentioned in Subsection \ref{sss:einstein_model} on the Einstein model.
\end{itemize}

\textbf{High-Temperature Limit aka Law of Dulong and Petit}
\begin{itemize}	
	\item In the high-temperature limit $ T \gg T_{D} $, all $ x $ in the integral $  \int_{0}^{T_{D}/T}\frac{x^{3}}{e^{x}-1}\diff x  $ will be very small. We can safely the expand the integral into a Taylor series to get
	\begin{equation*}
		\frac{x^{3}}{e^{x} - 1} \approx \frac{x^{3}}{(1 + x + \cdots )-1} = x^{2}
	\end{equation*}
	In this case, the integral is simply
	\begin{equation*}
		 \int_{0}^{T_{D}/T}\frac{x^{3}}{e^{x}-1}\diff x  \approx \int_{0}^{T_{D}/T} x^{2}\diff x  = \frac{1}{3}\left(\frac{T_{D}}{T}\right)^{3}
	\end{equation*}
	The energy and heat capacity are then 
	\begin{equation*}
		E = \frac{Vk_{B}^{4}T_{D}^{3}}{2\pi^{2}\hbar^{3}c_{s}^{3}}T = 3Nk_{B}T \qquad \text{and}\quad C_{V} = \frac{V k_{B}^{4}T_{D}^{3}}{2\pi^{2}\hbar^{3}c_{s}^{3}} = 3 N k_{B}
	\end{equation*}
	where both equalities use $ T_{D} = \frac{\hbar}{k_{B}} \left(\frac{6\pi^{2}N}{V}\right)^{\frac{1}{3}}c_{s} $.
	
	Note that high-temperature heat capacity $ C_{V} = 3Nk_{B} $ depends only on the amount $ N $ and makes no reference to the material's specific properties. The observation that high-temperature heat capacity is about the same for all solids had been known experimentally well before the Debye model. It is called the \textit{law of Dulong and Petit}. 
	
\end{itemize}


\subsection{Grand Canonical Ensemble}

\subsubsection{Common Questions}
\begin{enumerate}
    \item What is the grand canonical ensemble? Discuss how it is introduced and derived. Discuss and derive the thermodynamical significance of the grand potential.

    \item Discuss and derive the probability distributions (expected number of particles in a given energy state) in Bose-Einstein statistics.

    \item Discuss and derive the probability distributions (expected number of particles in a given energy state) in Fermi-Dirac statistics.

    \item Calculate the chemical potential of a gas of free electrons in a metal at absolute zero. Discuss why the result still applies acceptably even at room temperatures.

\end{enumerate}


\subsubsection{Basic Concepts: The Grand Canonical Ensemble}
\textit{What is the grand canonical ensemble? Discuss how it is introduced and derived. Discuss and derive the thermodynamical significance of the grand potential.}

\smallskip
\textbf{Chemical Potential}
\begin{itemize}	
	\item So far, we have constrained our systems by fixing the energy (microcanonical ensemble) and temperature (canonical ensemble). There is another important conserved quantity, particle number $ N $, that leads to another ensemble. The \textit{grand canonical ensemble} is used to describe systems at fixed temperature while allowing for variations in particle number $ N $.
	
	To make the system's dependence on particle number (along with energy and volume) explicit, we write
	\begin{equation*}
		\Omega = \Omega(E, V, N) \qquad \text{and} \qquad S(E, V, N) = k_{B}\ln \Omega(E, V, N)
	\end{equation*}
	
	\item Recall how entropy gives rise to temperature $ T $ and pressure $ p $ via the partial derivatives
	\begin{equation*}
		\frac{1}{T} = \pdv{S}{E} \qquad \text{and} \qquad p = T \pdv{S}{V}
	\end{equation*}
	We now have another option: to differentiate $ S $ with respect to $ N $. The resulting quantity is called the system's \textit{chemical potential}
	\begin{equation*}
		\mu = - T\pdv{S}{N}
	\end{equation*}
	Analogously to how two systems that can exchange energy are in equilibrium when both systems have equal temperature $ T $ and how two systems that can exchange volume are in equilibrium when both systems have equal pressure $ p $, two systems that can exchange particles are in equilibrium when both systems have equal chemical potential $ \mu $. This condition is called \textit{chemical equilibrium}, discussed also in the Subsection \ref{sss:liquid_gas_phase_equil}.
	
	\item To interpret chemical potential, we generalize the first law of thermodynamics to include the possibility of a change in particle number. First, we take the total derivative of $ S = S(E, V, N) $ and apply the definitions of $ T, p $ and $ \mu $.
	\begin{align*}
		\diff S &= \pdv{S}{E}\diff E + \pdv{S}{V}\diff V + \pdv{S}{N}\diff N = \frac{1}{T}\diff E + \frac{p}{T}\diff V - \frac{\mu}{T}\diff N
	\end{align*}
	Solving for $ \diff E $ leads to the generalized first law
	\begin{equation*}
		\diff E = T \diff S - p\diff V + \mu \diff N
	\end{equation*}
	The term $ \mu \diff N $ is the energy cost of changing the number of particles in the system. Thus the chemical potential $ \mu $ is measures the energy cost of adding an infinitesimal amount of particle $ \diff N $ to our system. Because for typical system $ N \sim 10^{23} $ is absolutely enormous, chemical potential can be safely viewed as the energy cost of adding adding one more particle to the system, since one is essentially infinitesimal in comparison to $ 10^{23} $.

	\item We defined have chemical potential as 
	\begin{equation*}
		\mu = -T \pdveval{S}{N}{E, V}
	\end{equation*}
	while differentiating at constant $ E $ and $ V $. However, the generalized differentials for energy and free energy
	\begin{equation*}
		\diff E = T \diff S - p\diff V + \mu \diff N \eqtext{and} \diff F = - S \diff T - p\diff V + \mu \diff N
	\end{equation*}
	shows chemical potential could also be expressed 
	\begin{equation*}
		\mu = \pdveval{E}{N}{S, V} \eqtext{and} \mu = \pdveval{F}{N}{T, V}
	\end{equation*}

	
\end{itemize}

\textbf{The Grand Canonical Ensemble}
\begin{itemize}
	\item When transitioning from the microcanonical ensemble to the canonical ensemble, we imagined a primary system and an auxiliary reservoir at fixed temperature $ T $ and allowed the system and reservoir to exchange energy.
	
	We can use the reservoir idea with any other conserved quantity, including the number of particles $ N $ in the system, and allow both energy and particles to flow between the primary system and reservoir. In this case, we require the reservoir has fixed temperature $ T $ \textit{and} fixed chemical potential $ \mu $. The corresponding probability distribution for such a system is called the \textit{grand canonical ensemble}.
	
	\item The probability of finding a system in a state $ \ket{n} $ depends on both the state's energy $ E_{n} $ and particle number $ N_{n} $, so the grand canonical partition function is
	\begin{equation*}
		\mathcal{Z}(T, \mu, V) = \sum_{n}e^{-\beta(E_{n} - \mu N_{n})}
	\end{equation*}
	Under the fundamental assumption that all states are equally likely, the probability for a system in the grand canonical ensemble to be in the state $ \ket{n} $ is
	\begin{equation*}
		p(n) = \frac{e^{-\beta(E_{n} - \mu N_{n})}}{\sum_{m}e^{-\beta(E_{m} - \mu N_{m})}} = \frac{e^{-\beta(E_{n} - \mu N_{n})}}{\mathcal{Z}}
	\end{equation*}
	
	\item As in the canonical ensemble, the majority quantities can be written in terms of the partition function. 
	\begin{itemize}
		\item The entropy of a system in the grand canonical ensemble is
		\begin{equation*}
			S = k_{B}\pdv{}{T}[T \ln \mathcal{Z}]
		\end{equation*}
		
		\item Average particle number $ \expval{N} $ is given by
		\begin{equation*}
			\expval{N} = \frac{1}{\beta} \pdv{}{\mu}\ln \mathcal{Z}
		\end{equation*}
		The variance in particle number $ \Delta N^{2} $ is given by
		\begin{equation*}
			\Delta N^{2} = \frac{1}{\beta^{2}}\pdv[2]{}{\mu} \ln \mathcal{Z} = \frac{1}{\beta}\pdv{\expval{N}}{\mu}
		\end{equation*}
		
		\item Notice the symmetry in the relationship between energy variance and temperature in the canonical ensemble and the relationship between particle number variance and chemical potential in the grand canonical ensemble. 
		
		Like energy in the canonical ensemble, the relative fluctuations of particle number $ \frac{\Delta N}{\expval{N}} $ scale with the number of particles in the system as $ \sim \frac{N}{\sqrt{N}} = \frac{1}{\sqrt{N}}$. In the thermodynamic limit $ N \to \infty $, the fluctuations $ \Delta N $ grow arbitrarily small compared to $ \expval{N} $ and the number of particles in the system grows arbitrarily close the average value $ \expval{N} $. We can thus safely drop the average notation and refer to average particle number as $ N $.
		
		\item Finally, average energy in the grand canonical ensemble is determined by the relationship
		\begin{equation*}
			\expval{E} - \mu \expval{N} = - \pdv{}{\beta}\ln \mathcal{Z}
		\end{equation*}
	\end{itemize}
	
\end{itemize}

\textbf{Grand Canonical Potential}
\begin{itemize}
	\item The \textit{grand canonical potential} $ \Phi $ is defined as
	\begin{equation*}
		\Phi = F - \mu N
	\end{equation*}
	Mathematically, $ \Phi $ is a Legendre transform of free energy $ F $ from the variable $ N $ to $ \mu $. This is seen by comparing the total differentials
	\begin{equation*}
		 \diff F = - S \diff T - p\diff V + \mu \diff N \eqtext{and} \diff \Phi = -S \diff T - p \diff V - N\diff \mu  
	\end{equation*}
	which shows that $ \Phi $ should be thought of as a function of temperature, volume and chemical potential, $ \Phi = \Phi(T, V, \mu) $. 
	
	Using the definitions of $ S $ and $ \expval{N} $ in terms of $ \mathcal{Z} $, we can write $ \Phi $ in terms of $ \mathcal{Z} $ as
	\begin{equation*}
		\Phi = - \frac{1}{\beta} \ln \mathcal{Z}
	\end{equation*}
	Notice the similarity to the relationship $ F = - \frac{1}{\beta} \ln Z $ in the canonical ensemble.
	
	\item Recall the interpretation $ \Phi = \Phi(T, V, \mu) $ where the grand canonical potential is a function of temperature, volume and chemical potential. Temperature and chemical potential are intensive quantities while volume is an extensive quantity. $ \Phi $ is thus a function of a single extensive variable $ V $ and must scale as 
	 \begin{equation*}
	 	\Phi(T, \lambda V, \mu) = \lambda \Phi(T, V, \mu)
	 \end{equation*}
	 where $ \lambda $ is an arbitrary scalar value. To satisfy this scaling relationship, $ \Phi $ must be proportional to $ V $. But from the total differential $ \diff \Phi = -S \diff T - p \diff V - N\diff \mu $, we see the proportionality of $ \Phi $ to $ V $ is precisely $ - p $. This means that $ \Phi $ can be expressed
	 \begin{equation*}
	 	\Phi(T, V, \mu) = -p(T, \mu) V
	 \end{equation*}
	 The relationship between grand canonical potential and temperature is a very useful way to calculate the pressure of various systems.

\end{itemize}


\subsubsection{Bose-Einstein Distribution}
\textit{Discuss and derive the probability distributions (expected number of particles in a given energy state) in Bose-Einstein statistics.}

\begin{itemize}
	\item First, a change of notation: we will label our quantum systems' single particle quantum states with $ \ket{i} $. We had previously used $ \ket{n} $ to denote single particle states, but $ n $ will be used in this section to denote the number of particles in a state $ \ket{i} $.
	
	With this notation, the single particle energies are $ E_{i} $. We will assume our particles are ideal and thus non-interacting. 
	
	\item Next, we come to an important point: quantum particles of a given type (e.g. electrons, protons, etc...) are indistinguishable. This means we have completely described our system by naming how many particles are in state $ \ket{0} $, how many are in state $ \ket{1} $, and in general how many are in state $ \ket{i} $. We will denote the number of particles in the state $ \ket{i} $  by $ n_{i} $. 
	
	If the particles were distinguishable, we would need to be more specific and identify which state particle $ a $ is in, which state particle $ b $ is in, and so on for all $ N $ particles. But because quantum particles are indistinguishable, there is no basis for the notion of particle $ a, b $, etc... and we simply identify the number of particles in each state. 
	

	
	\item If we chose to work in the canonical ensemble (which we won't), we would have to sum over all possible ways of partitioning the total system's $ N $ particles into sets $ \{n_{i}\} $ subject to the constraint $ \sum_{i}n_{i} = N $. The partition function would be
	\begin{equation*}
		Z = \sum_{\{n_{i}\}} e^{-\beta n_{i}E_{i}}
	\end{equation*}
	The factor $ n_{i} $ in the exponent accounts for the $ n_{i} $ particles in the state $ \ket{i} $. 
	
	The details of partitioning the $ N $ particles into sets $ \{n_{i}\} $ makes evaluating the partition function really hard! The complication is a consequence of the fact that we can at best describe our system by stating how many particles are in each state $ \ket{i} $, which occurs because quantum particles are indistinguishable.
	
	\item For indistinguishable particles, it turns out to be much easier to work in the grand canonical ensemble. We introduce a chemical potential $ \mu $ and allow the total number of particles $ N $ in our system of interest to fluctuate.  
	
	We consider each single particle state $ \ket{i} $ in turn. In the GCE, a given state $ \ket{i} $ can be occupied by an arbitrary number of particles $ n_{i} $. For the state $ \ket{i} $, the grand canonical partition function is
	\begin{equation*}
		\mathcal{Z}_{i} = \sum_{n_{i}=0}^{\infty} e^{-\beta n_{i}(E_{i} - \mu)}
	\end{equation*}
	
	\item Evaluating the sum is not too difficult. If we assume $ (E_{r} - \mu) > 0 $, so that the terms $ e^{-\beta n_{i}(E_{i} - \mu)} $ decrease with increasing $ n_{i} $, we can write
	\begin{equation*}
		\mathcal{Z}_{i} = \sum_{n_{i}=0}^{\infty} \left(e^{-\beta (E_{i} - \mu)}\right)^{n_{i}} = \frac{1}{1 - e^{-\beta (E_{i} - \mu)}}
	\end{equation*}
	were we have used geometric series $ \sum_{n} x^{n} = \frac{1}{1 - x} $ with $ x = e^{-\beta (E_{i} - \mu)} $. 
	
	\textit{Important:} the boson partition function only converges under the assumption that $ (E_{i} - \mu) > 0 $ holds for all single particle states $ \ket{i} $. If, as per convention, we set the ground state to have energy $ E_{0} = 0 $, the convergence condition implies 
	\begin{equation*}
		(E_{i} - \mu)\big |_{E_{0} = 0} > 0 \implies \mu < 0 
	\end{equation*}
	Since $ \mu $ is the same for all states $ \ket{i} $ \textit{the grand partition function for a Bose gas only makes sense if the system's chemical potential is negative}.
	
	\item Because the grand partition function is multiplicative for independent subsystems, the full grand partition function for a boson gas is simply the product over all $ \mathcal{Z}_{i} $
	\begin{equation*}
		\mathcal{Z} = \prod_{i} \mathcal{Z}_{i} = \prod_{i} \frac{1}{1 - e^{-\beta (E_{i} - \mu)}}
	\end{equation*}
	
	\item Next, the average number of particles in a Bose gas is
	\begin{equation*}
		\expval{N} = \frac{1}{\beta} \pdv{}{\mu} \ln \mathcal{Z} = \sum_{i}\frac{1}{e^{\beta(E_{i} - \mu)} - 1}
	\end{equation*}
	\textit{Derivation:} Plug in $ \mathcal{Z} $ and use $ \ln AB = \ln A + \ln B \implies  \ln \big(\prod_{i}x_{i} \big)= \sum_{i} \ln x_{i} $
	\begin{equation*}
		\ln \mathcal{Z} = \ln \prod_{i} \frac{1}{1 - e^{-\beta (E_{i} - \mu)}} = \sum_{i} \ln (1 - e^{-\beta (E_{i} - \mu)})^{-1}
	\end{equation*} 
	Differentiate with respect to $ \mu $ using $ \pdv{}{\mu}\ln u = \frac{1}{u} \pdv{u}{\mu} $ and simplify to get
	\begin{align*}
		\expval{N} = \frac{1}{\beta} \pdv{}{\mu} \left[\sum_{i} \ln (1 - e^{-\beta (E_{i} - \mu)})^{-1}\right] = \sum_{i} \frac{e^{-\beta(E_{i} - \mu)}}{1 - e^{-\beta(e_{i} - \mu)}}
	\end{align*}
	Multiplying the numerator and denominator by $ e^{\beta(E_{i} - \mu)} $ gives
	\begin{equation*}
		\expval{N} = \sum_{i}\frac{1}{e^{\beta(E_{i} - \mu)} - 1}
	\end{equation*}
	
	\item Naturally, the sum of all $ n_{i} $, the number of gas particles in each state $ \ket{i} $, should add up to the total number of particles $ N $ in the gas as a whole. We write this constraint as $ \expval{N} = \sum_{i}\expval{n_{i}} $. Comparing $ \expval{N} = \sum_{i}\expval{n_{i}} $ to the definition of $ \expval{N} $, we can define
	\begin{equation*}
		\expval{n_{i}} = \frac{1}{e^{\beta(E_{i} - \mu)} - 1}
	\end{equation*}
	This important equation is the \textit{Bose-Einstein} distribution. It gives the average number of particles in the state $ \ket{i} $ for an ideal monatomic boson gas. 
	
\end{itemize}

\subsubsection{Fermi-Dirac Distribution}
\textit{Discuss and derive the probability distributions (expected number of particles in a given energy state) in Fermi-Dirac statistics.}


\begin{itemize}
	\item Like bosons, fermions are indistinguishable, so it is best to work in the grand canonical ensemble. The exclusion principle makes the grand partition function for single particle state $ \ket{i} $ very simple. Since at most one fermion can occupy a given state, each state $ \ket{i} $ is either occupied or it is not (i.e. $ n \in \{0, 1\} $); there is no other option.
	\begin{equation*}
		\mathcal{Z}_{i} = \sum_{n=0}^{1}e^{-\beta n(E_{i}-\mu)} = 1 + e^{-\beta(E_{i} - \mu)}
	\end{equation*}
	
	\item The grand partition function for the whole system is the product of $ \mathcal{Z}_{i} $ over all states $ \ket{i} $:
	\begin{equation*}
		\mathcal{Z} = \prod_{i} \mathcal{Z}_{i} = \prod_{i} (1 + e^{-\beta(E_{i} - \mu)})
	\end{equation*}
	
	\item Next, the average number of particles in a fermion gas is
	\begin{equation*}
		\expval{N} = \frac{1}{\beta}\pdv{}{\mu} \ln \mathcal{Z} = \sum_{i} \frac{1}{1 + e^{\beta (E_{i} - \mu)}}
	\end{equation*}
	The derivation reads
	\begin{align*}
		\expval{N} &= \frac{1}{\beta}\pdv{}{\mu} \ln \left[\prod_{i} (1 + e^{-\beta(E_{i} - \mu)})\right] = \frac{1}{\beta} \pdv{}{\mu} \sum_{i} \ln(1 + e^{-\beta(E_{i} - \mu)})\\
		&=\frac{1}{\beta} \sum_{i} \frac{\beta e^{-\beta(E_{i}-\mu)}}{1 + e^{-\beta(E_{i} - \mu)}} = \sum_{i} \frac{e^{-\beta(E_{i}-\mu)}}{1 + e^{-\beta(E_{i} - \mu)}} = \sum_{i} \frac{1}{1 + e^{\beta (E_{i} - \mu)}}
	\end{align*}
	
	\item Just like for bosons, the average number of particles in each state $ \ket{i} $ must add up to the total number of particles $ N $ in the system, leading to $ \sum_{i}\expval{n_{i}} = N $. Comparing this constraint to the expression for $ \expval{N} $, we define
	\begin{equation*}
		\expval{n_{i}} =  \frac{1}{1 + e^{\beta (E_{i} - \mu)}}
	\end{equation*}
	This is the \textit{Fermi-Dirac distribution} and gives the average number of particles in the state $ \ket{i} $ in an ideal monatomic Fermi gas. It differs from the Bose-Einstein distribution only in the sign of the one in the denominator, but this difference is enough to produce very different behavior between bosons and fermions.
\end{itemize}

\subsubsection{Low-Temperature Chemical Potential of an Electron Gas}
\textit{Calculate the chemical potential of a gas of free electrons in a metal at absolute zero. Discuss why the result still applies acceptably even at room temperatures.}

\begin{itemize}
	\item In the low-temperature limit $ T \to 0 $, the Fermi-Dirac distribution simplifies to
	\begin{equation*}
		\lim_{T \to 0} \expval{n_{i}} = \lim_{T \to 0} \frac{1}{e^{\beta(E - \mu)} + 1} = 
		\begin{cases}
			1 & E < \mu\\
			0 & E > \mu
		\end{cases}
	\end{equation*}
	All states with energy less than $ \mu $ are filled and all states with energy less than $ \mu $ are empty. At $ T \to 0 $, all fermions we add to the system settle into the lowest available (unoccupied) energy state. The energy of the last filled state is called the \textit{Fermi energy} and is denoted by $ E_{F} $. Mathematically, $ E_{F} $ is the value of the chemical potential at $ T = 0 $. 
	
	\item When we fill energy states in an ideal fermion system with fermions, the energy states of free particles are localized in momentum space. Successive fermions occupy states of ever-increasing momentum, and the fermion system forms a sphere in momentum space called the \textit{Fermi sphere}. 
	
	The momentum of the last (highest energy) fermion is called the \textit{Fermi momentum}. The Fermi momentum is usually given as a wave vector via the relationship $ p = \hbar k_{F} $:
	\begin{equation*}
		p_{F} = \hbar k_{F} = \sqrt{2mE_{F}}
	\end{equation*}
	For quantum systems we typically work with the wave vector $ k $ instead of momentum. In this convention, all states in a fermion system with $ \abs{k} \leq k_{F} $  are filled an lie in the Fermi sphere. The states with $ \abs{k} = k_{F} $ lie on the edge of the Fermi sphere and form the Fermi surface. Keep in mind that the Fermi surface place a central role in many areas of quantum statistical and solid states physics, but most of these topics are beyond the scope of these notes.
	
	\item It best to define the Fermi energy in terms of the number of particles $ N $ in the fermion system. To do this, we take advantage of the fact that at $ T \to 0 $, only states with $ E \leq E_{F} $ are occupied. As mentioned in the first bullet, this means the Fermi-Dirac distribution $ \expval{n_{i}} = \frac{1}{e^{\beta(E - \mu)}+1}$ is one for $ E \in [0, E_{F}] $, greatly simplifying the integration. Using the density of states definition of $ N $ we have
	\begin{align*}
		N &= \int_{0}^{\infty}\expval{n_{i}}g(E) \diff E = \int_{0}^{E_{F}} (1) \cdot g(E) \diff E = \int_{0}^{E_{F}} \frac{g_{s}V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \sqrt{E} \diff E \\
		&= \frac{g_{s}V}{6\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} E_{F}^{3/2}
	\end{align*}
	Solving the equality for $ E_{F} $ gives
	\begin{equation*}
		E_{F} = \frac{\hbar^{2}}{2m} \left(\frac{6\pi^{2}N}{g_{s}V}\right)^{2/3}
	\end{equation*}
	
	\item The Fermi energy sets the energy scale for a Fermion system. We can define an corresponding temperature scale in terms of the \textit{Fermi temperature}
	\begin{equation*}
		T_{F} = \frac{E_{F}}{k_{B}}
	\end{equation*}
	Temperatures with $ T > T_{F} $ are considered high-temperature for fermion systems, while temperatures $ T < T_{F} $ are considered low-temperature. Note that Fermi temperatures are often quite high by everyday standards. For instance, $ T_{F} \sim \SI{e4}{\kelvin} $ for electrons in a metal.
	
	\item Because the Fermi temperature for typical materials (of order $ \SI{e4}{\kelvin} $) is much larger than room temperature, the low-temperature Fermi energy 
	\begin{equation*}
		E_{F} = \frac{\hbar^{2}}{2m} \left(\frac{6\pi^{2}N}{g_{s}V}\right)^{2/3}
	\end{equation*}
	is still a very good approximation for the Fermi energy of solids at room temperature. 
	
	\item Here is an more rigorous justification of the validity of the approximation at room temperature: the slope of the tangent line to the Fermi-Dirac distribution at $ E = \mu $ is
	\begin{equation*}
		\eval{\dv{\expval{n_{i}}}{E}}_{\mu} = \eval{\frac{\beta e^{-\beta(E - \mu)}}{(1 + e^{-\beta(E - \mu)})^{2}}}_{\mu} = - \frac{\beta}{4} = - \frac{1}{4k_{B}T}
	\end{equation*}
	and the equation of the tangent line is
	\begin{equation*}
		\expval{n_{i}}(E) = \frac{1}{2} - \frac{1}{4k_{B}T} (E - \mu)
	\end{equation*}
	The step function, zero-temperature limit of the Fermi-Dirac distribution crosses $ \expval{n_{i}} = 0 $ at $ E = \mu $, while the tangent line crosses $ \expval{n_{i}} = 0 $ at $ E = \mu + 2k_{B}T $, a difference of $ 2k_{B}T $. At room temperature, this discrepancy is of order $ \frac{1}{20} \si{\electronvolt} $, which is negligible compared to the room-temperature chemical potentials of everyday materials, of order $ \SI{10}{\electronvolt} $ for metals. This means replacing the room-temperature Fermi-Dirac distribution with the low-temperature, step function limit is a valid approximation.

	\item A final note: in the low temperature limit $ T \to 0 $ where $ E_{F} $ is the energy of the last occupied state the average energy of a fermion system is
	\begin{equation*}
		\expval{E} = \int_{0}^{E_{F}} E g(E) \diff E =  \frac{g_{s}V}{10\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} E_{F}^{5/2} = \frac{3}{5}NE_{F}
	\end{equation*}
	Using the relationship $ pV = \frac{2}{3}\expval{E} $, we can  write $ pV  = \frac{2}{5} N E_{F} $ for a fermion gas at $ T \to 0 $. This deserves a second look: even at zero temperature a Fermi gas has non-zero pressure! This remarkable result, sometimes called \textit{degeneracy pressure} is the result of the Pauli exclusion principle and is at completely at odds with the behavior of a classical ideal gas.
	
\end{itemize}


\subsection{Magnetism and Quantum Statistical Physics}

\subsubsection{Common Questions}
\begin{enumerate}
    \item Discuss the effect of an external magnetic field on atoms with nonzero angular momentum. Calculate the susceptibility of gas atoms with spin $ \frac{1}{2} $ in the high temperature limit. Discuss and derive Curie's law from a statistical physics approach.

    \item What is the Ising model? How to we analyze solids using the Ising model and the mean field approximation? What notable behavior does this analysis predict?

\end{enumerate}


\subsubsection{Aside: Polylogarithms and Fermi Gases} \label{sss:polylog}
\textit{Note}: this is an optional section. I added the discussion of polylogarithms because they are an elegant machinery for handling magnetism in the next section.
\begin{itemize}
	\item We will use something called a \textit{polylogarithm function} to concisely express the $ \expval{N} $ and $ \expval{E} $ for a Fermi gas. The polylogarithm functions are a family of parameter-dependent integrals that occur frequently in quantum statistics. The polylogarithms are given by
	\begin{equation*}
		\mathrm{Li}_{n}(z) = \frac{1}{\Gamma(n)} \int_{0}^{\infty} \frac{x^{n-1}}{z^{-1}e^{x}+1} \diff x
	\end{equation*}
	where $ \Gamma(n) $ is the \textit{Gamma function} 
	\begin{equation*}
		\Gamma(z) = \int_{0}^{\infty} x^{z-1}e^{-z}\diff z
	\end{equation*}
	For the purpose of this section, all we need to know about the Gamma function is $ \Gamma(\frac{3}{2}) = \frac{\sqrt{\pi}}{2} $ and $ \Gamma(\frac{5}{2}) = \frac{3\sqrt{\pi}}{4} $.

	Later, in the discussion of magnetism, we will use the small argument approximation
	\begin{equation*}
		\mathrm{Li}_{3/2}(z) \approx z \qquad (z \ll 1)
	\end{equation*}
	and, in the low-temperature limit, the large-argument approximation
	\begin{equation*}
		\mathrm{Li}_{n}(z) \approx \frac{(\ln z)^{n}}{\Gamma(n+1)} 
	\end{equation*}
	Both are given without derivation.
	
	\item In terms of the density of states, the average occupation number for a Fermi gas is
	\begin{equation*}
		\expval{N} = \int  \frac{g(E)}{1 + e^{\beta (E - \mu)}}  \diff E = \frac{g_{s}V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \int_{0}^{\infty}\frac{E^{1/2}}{e^{-\beta \mu} e^{\beta E} + 1} \diff E 
	\end{equation*}
	The last integral can be conveniently expressed as polylogarithm function
	\begin{equation*}
		\expval{N} = \frac{g_{s}V}{\lambda^{3}}\mathrm{Li}_{3/2}(e^{\beta \mu})
	\end{equation*}
	where $ \lambda $ is the familiar thermal de Broglie wavelength $ \lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}} $.
	
	\textit{Derivation}: checking the definition of $ \mathrm{Li}_{n}(z) $ for reference, we choose $ n = \frac{3}{2} $ and $ z = e^{\beta E} $. To create the necessary $ x = \beta E $, we multiply the integrand above and below by $ \beta^{1/2} $ and change integration via $ \diff E =  \frac{\diff[\beta E]}{\beta}  $, which generates the $ (k_{B}T)^{3/2} $ term needed for the thermal de Broglie wavelength $ \lambda $.
	
	Manipulating the outside constants from the density of states into the thermal de Broglie wavelength $ \lambda^{3} $ conveniently generates the necessary $ \frac{1}{\Gamma(3/2)} = \frac{2}{\sqrt{\pi}} $ to complete $ \mathrm{Li}_{3/2}(e^{\beta \mu}) $ with $ x = \beta E $.
	
	\item Following an analogous procedure, the average energy can be written
	\begin{equation*}
		\expval{E} = \frac{g_{s}V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \int_{0}^{\infty}\frac{E^{3/2}}{z^{-1}e^{\beta (E - \mu)} + 1} \diff E = \frac{3}{2}\frac{g_{s}V}{\lambda^{3}\beta}\mathrm{Li}_{5/2}(e^{\beta \mu})
	\end{equation*}
		

\end{itemize}

\subsubsection{Spin Coupling and Curie's Law}
\textit{Discuss the effect of an external magnetic field on atoms with nonzero angular momentum. Calculate the susceptibility of gas atoms with spin $ \frac{1}{2} $ in the high temperature limit. Discuss and derive Curie's law from a statistical physics approach.}

\begin{itemize}
	\item Consider an electron gas in an external magnetic field. The magnetic field introduces two important effects: the Lorentz force $ \bm{F} = \bm{v} \cross \bm{B} $ and the coupling of electron spin to the magnetic field. We will only discuss spin coupling in this section.
	
	\item An electron (a spin $ \frac{1}{2} $ fermion) can only have two spin states: spin up or spin down. We will label the spin up state $ s = 1 $ and the spin down state $ s = -1 $. In an external magnetic field, the electron's energy picks up the additional magnetic term
	\begin{equation*}
		E_{B} = \mu_{B} s B
	\end{equation*}
	from the interaction of the electron's magnetic moment with the magnetic field. The constant $ \mu_{B} = \frac{e_{0}\hbar}{2m} $ is the Bohr magneton and is unrelated to chemical potential.
	
	\item An important note: because the spin up and spin down electrons have different energies, their partition functions, which involve the term $ e^{-\beta E} $ will also be different, as will all the quantities derived from the partition function. 
	
	We separately consider the number of spin up and spin down particles, which we will label $ N_{\uparrow} $ and $ N_{\downarrow} $ and describe in terms of the polylogarithms. In the previous section (Subsection \ref{sss:polylog}) we said the average occupation number for a free fermion gas is
	\begin{equation*}
		\expval{N} = \frac{g_{s}V}{\lambda^{3}}\mathrm{Li}_{3/2}(e^{\beta \mu})
	\end{equation*}
	For electrons with spin coupling, we just have to account for the extra energy term and replace $ e^{\beta(E - \mu)} $ with $ e^{\beta(E + \mu_{B}sB- \mu) } $ in the Fermi-Dirac distribution. We get
	\begin{align*}
		&\expval{N_{\uparrow}} = \int  \frac{g(E)}{1 + e^{\beta (E + \mu_{B}B - \mu)}}  \diff E = \frac{V}{\lambda^{3}}\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right )\\
		&\expval{N_{\downarrow}} = \int  \frac{g(E)}{1 + e^{\beta (E - \mu_{B}B - \mu)}}  \diff E =  \frac{V}{\lambda^{3}}\mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )
	\end{align*}
	Note the I have plugged in $ g_{s} = 1 $ for the spin up and spin down states (an electron that can be in \textit{either} spin up or spin down has $ g_{s} = 2$, but the fixed up or down states themselves have $ g_{s} = 1 $).
	
	\item Next, we turn to the system's magnetization, which measures how the system's energy responds to an external magnetic field. Magnetization $ M $ is defined as
	\begin{equation*}
		M = - \pdv{E}{B}
	\end{equation*}
	Inserting the magnetic field energy $ E_{B} = \mu_{B}s B $ leads to $ M = - \mu_{B}s $ where $ s $ is the system's spin. For a system of $ N $ particles, $ s $ is the difference between the number of states with spin up and spin down, so the total magnetization is
	\begin{equation*}
		M = - \mu_{B}(N_{\uparrow} - N_{\downarrow}) = - \frac{\mu_{B}V}{\lambda^{3}} \left[\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right ) - \mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )\right]
	\end{equation*}
	 
	\item In the high temperature limit (for suitably low magnetic fields) $ e^{\beta(\mu \pm \mu_{B}B)} \ll 1$. We can then make the small-argument approximation $ \mathrm{Li}_{3/2}(z) \approx z $ to get
	\begin{equation*}
		M \approx - \frac{\mu_{B}V}{\lambda^{3}} \left(e^{\beta(\mu - \mu_{B}B)} - e^{\beta(\mu + \mu_{B}B)}\right) =  \frac{2\mu_{B}V}{\lambda^{3}}e^{\beta \mu} \sinh(\beta \mu_{B}B)
	\end{equation*}
	To get to an important result, we will re-write this equality in terms of total particle number $ N $. Making use of $ \mathrm{Li}_{3/2}(z) \approx z $, we have
	\begin{align*}
		N &= N_{\uparrow} + N_{\downarrow} = \frac{V}{\lambda^{3}}\left[\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right ) + \mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )\right]\\
		&\approx \frac{V}{\lambda^{3}}\left(e^{\beta(\mu - \mu_{B}B)} + e^{\beta(\mu + \mu_{B}B)} \right) = \frac{2V}{\lambda^{3}}e^{\beta \mu} \cosh(\beta\mu_{B}B)
	\end{align*}
	We can invert the equality to get $ e^{\beta \mu} = \frac{N\lambda^{3}}{2V} \frac{1}{\cosh(\beta\mu_{B}B)} $ from which follows
	\begin{equation*}
		M \approx \mu_{B} N \tanh(\beta\mu_{B}B)
	\end{equation*}
	
	\item As $ T \to \infty $ we have $ \beta \mu_{B}B \to 0 $, and we can safely make the small-argument approximation $ \tanh x \approx x $, which leads to
	\begin{equation*}
		M \approx \frac{N\mu_{B}^{2}}{k_{B}} \frac{B}{T} \qquad (\text{high } T)
	\end{equation*}
	The inverse proportionality of magnetization to temperature is called \textit{Curie's law} and is valid in the high temperature limit where quantum effects are not important. 
	
	\item Curie's law is often given in terms of magnetic susceptibility $ \chi $, defined as
	\begin{equation*}
		\chi = \pdv{M}{B}
	\end{equation*}
 	Magnetic susceptibility measures how easy it is to magnetize a substance. In terms of susceptibility, Curie's law reads
 	\begin{equation*}
	 	\chi \approx \frac{N\mu_{B}^{2}}{k_{B}T}
 	\end{equation*}
 	where the $ \frac{1}{T} $ proportionality is the characteristic feature.
 \end{itemize}

\textbf{Low-Temperature Limit}
 \begin{itemize}
 	\item In the low temperature limit, quantum effects become important in the analysis of fermion magnetism. Recall the general expression for magnetization:
	\begin{equation*}
		M = - \frac{\mu_{B}V}{\lambda^{3}} \left[\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right ) - \mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )\right]
	\end{equation*}
	Using the first-order large-argument approximation $ \mathrm{Li}_{n}(z) \approx \frac{(\ln z)^{n}}{\Gamma(n+1)} $ with $ n = \frac{3}{2} $ and $ z =  e^{\beta(\mu \pm \mu_{B}B)} $ and making the low-temperature approximation $ \mu \approx E_{F} $ leads to
	\begin{equation*}
		M \approx \frac{\mu_{B}V}{6\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2}\left[(E_{F} + \mu_{B}B)^{3/2} - (E_{F} - \mu_{B}B)^{3/2}  \right]
	\end{equation*}
	In the low magnetic field regime $ \mu_{B}B \ll E_{F} $, we can expand $ (E_{F} \pm \mu_{B}B)^{3/2} $ to first order in $ B $ using general first-order approximation $ f(x + \delta x) \approx x + f'(x) \delta x $ with $ x = E_{F} $ and $ \delta x = \pm \mu_{B}B $. This gives
	\begin{equation*}
		(E_{F} \pm \mu_{B} B)^{3/2} \approx E_{F} \pm \frac{3}{2} E_{F}^{1/2} \mu_{B}B
	\end{equation*}
	From which follows
	\begin{equation*}
		M \approx \frac{\mu_{B}^{2}V}{2\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} B E_{F}^{1/2}
	\end{equation*} 
	In terms of the electron density of states $ g(E) = \frac{V}{2\pi^{2}}\left(\frac{2m}{\hbar^{2}}\right)^{3/2}E^{1/2}$ (we've plugged in $ g_{s} = 2 $) the magnetization reads
	\begin{equation*}
			M \approx \mu_{B}^{2} g(E_{F}) B
	\end{equation*}
	Note that at low temperatures the Curie's law $ \frac{1}{T} $ proportionality is gone. Likewise, magnetic susceptibility saturates to a constant value
	\begin{equation*}
		\chi = \pdv{M}{B} = \mu_{B}^{2} g(E_{F})
	\end{equation*}
	As a final note, materials with $ \chi > 0 $ are called \textit{paramagnetic}; their magnetism increases in the presence of a magnetic field. 
\end{itemize}

\subsubsection{Ising Model}
\textit{What is the Ising model? How to we analyze solids using the Ising model and the mean field approximation? What notable behavior does this analysis predict?}

\begin{itemize}
	\item \`{A} la Tong: the Ising model consists of $ N $ sites in a $ d $-dimensional lattice. On each lattice site sits an atom with quantum spin that can sit in one of two states: spin up or spin down. Well call the eigenvalue of the spin on the $ i $th lattice site $ s_{i} $. If the spin is up, $ s_{i} = +1 $; if the spin is down, $ s_{i} = -1 $. Note that $ s $ is a dimensionless quantity. 
	
	Importantly, each atom experiences a magnetic interaction with each of its nearest neighbors. In the presence of an external magnetic field $ B $, the energy lattice's is
	\begin{equation*}
		E = - J\sum_{\expval{i, j}}s_{i}s_{j} - B \mu_{\text{lattice}} \sum_{i}s_{i}
	\end{equation*}
	where $ \mu_{\text{lattice}} $ represents a lattice site's magnetic moment, while the notation $ \expval{i, j} $ denotes summation over all nearest neighbor pairs in the lattice; the number of such pairs depends both on the dimension $ d $ and the type of lattice. We'll denote the number of nearest neighbors by $ q $. 
	
	The factor $ J $, which has units of energy, is sometimes called an exchange in integral and represents the coupling between neighboring spins. If $ J > 0 $, neighboring spins prefer to be aligned ($ \uparrow \uparrow  $ or $ \downarrow \downarrow $); such a system is ferromagnetic. For $ J < 0 $, neighboring spins tend to anti-align ($ \downarrow\uparrow  $ or $  \uparrow \downarrow $), and the system is anti-ferromagnetic. $ J = 0 $ corresponds to a paramagnet.
	
	To avoid repeatedly writing the factor $ B \mu_{\text{lattice}} $, we'll work in terms of an ``effective magnetic field'' $ H \equiv \mu_{\text{lattice}} B $, which has units of energy, just like $ J $. In terms of $ H $, the lattice's energy is
	\begin{equation*}
		E = - J\sum_{\expval{i, j}}s_{i}s_{j} - H \sum_{i}s_{i}
	\end{equation*}	

	\item We'll work in the canonical ensemble and use the partition function
	\begin{equation*}
		Z = \sum_{\{s_{i}\} }e^{-\beta E(s_{i})} 
	\end{equation*}
	where we sum over all lattice sites $ s_{i} $. The main goal of the section is finding the system's average spin $ m $, a dimensionless quantity between $ -1 $ and $ 1 $ (corresponding to the range of $ s \in \{-1, 1\} $).
	\begin{equation*}
		m = \frac{1}{N} \sum_{i} \expval{s_{i}} = \frac{1}{N \beta} \pdv{}{H} \ln Z
	\end{equation*}
	
	\item Next, a cool interpretation: we treat the lattice as a gas, thinking of gas particles hopping between lattice sites, where no more than one particle can occur at a given site. We introduce the variable $ n_{i} \in \{0, 1\} $ to specify whether the $ i $th lattice site is occupied $ (n_{i} = 1) $ or empty $ (n_{i} = 0) $.
	
	The energy of the lattice gas is 
	\begin{equation*}
		E = - 4J \sum_{\expval{i j}}n_{i}n_{j} - \mu \sum_{i}n_{i}
	\end{equation*}
	where $ \mu $ is the chemical potential determining the overall particle number while the term $ - 4J \sum_{\expval{i j}}n_{i}n_{j} $, which makes it energetically favorable for particles to occupy neighboring lattice sites, models an attractive force between particles. 
	
	But this energy is the same as the Ising model is we make the substitution
	\begin{equation*}
		s_{i} = 2n_{i} - 1 \implies s_{i} \in \{-1, 1\}
	\end{equation*}
	The chemical potential $ \mu $ plays the role of the external magnetic field.
	
	\item Next, we use an approximate solution to evaluate $ Z $ called \textit{mean field theory}. We write the interaction between neighboring spins $ s_{i}s_{j} $ in terms of the the deviation from the average spin
	\begin{align*}
		s_{i}s_{j} &= \big[(s_{i} - m) + m\big]\big[(s_{j} - m) + m\big]\\
		&=(s_{i}-m)(s_{j}-m) + m(s_{j}-m) + m(s_{i}-m) + m^{2}\\
		&\approx m(s_{j}-m) + m(s_{i}-m) + m^{2}\\
		&= m(s_{i}+s_{j}) - m^{2}
	\end{align*}
	where we assume in the middle line that the fluctuations from the average spin are small, allowing us to neglect the term $ (s_{i}-m)(s_{j}-m) $, which is second order in $ (s_{ij}-m) $. We can then write the total energy as
	\begin{align*}
		E &= - J\sum_{\expval{i, j}}s_{i}s_{j} - H \sum_{i}s_{i} \approx  -J \sum_{\expval{i, j}}[m(s_{i}+s_{j}) - m^{2}] - H \sum_{i}s_{i}\\
		&=J\sum_{\expval{i, j}} m^{2} - J m \sum_{\expval{i, j}}(s_{i}+s_{j}) - H \sum_{i}s_{i} = J\sum_{\expval{i, j}} m^{2} - J mq \sum_{i}s_{i} - H \sum_{i}s_{i}\\
		&=\frac{1}{2}JNqm^{2} - (Jqm + H)\sum_{i}s_{i}
	\end{align*}
	where the term $ \frac{Nq}{2} $ is the total number of nearest neighbor pairs; the factor $ \frac{1}{2} $ comes from counting pairs and not individual sites. 
	
	\item Notice that the mean field approximation removes the interactions; there is no more sum over $ \expval{i j} $. The spins become independent, and the effect of the neighbors spins simplifies to a contribution $ Jqm $ to an effective magnetic field
	\begin{equation*}
		\heff = H + Jqm
	\end{equation*}
	In terms of $ \heff $, the lattice's energy is 
	\begin{equation*}
		E = \frac{1}{2}JNqm^{2} + \heff \sum_{i}s_{i}
	\end{equation*}
	which is a two-state system with $ s_{i} \in {-1, 1} $, analogous to the system in Subsection \ref{sss:two_state_mce}. The partition function for a one-particle lattice with $ N = 1 $ is
	\begin{equation*}
		Z_{1} = \sum_{\{s_{i}\}} e^{-\beta E(s_{i})} = e^{-\frac{1}{2}\beta Jqm^{2}} \left(e^{+\beta \heff } + e^{-\beta \heff} \right)
	\end{equation*}
	because the spins are non-interacting, the total partition function for $ N $ sites is simply $ Z_{1}^{N} $
	\begin{align*}
		Z &= Z_{1}^{N} = e^{-\frac{1}{2}\beta JNqm^{2}} \left(e^{+\beta \heff } + e^{-\beta \heff} \right)^{N}\\
		&=e^{-\frac{1}{2}\beta JNqm^{2}}2^{N} \cosh^{N}(\beta \heff)
	\end{align*}
	To complete the answer, we still have to find an expression for the average spin $ m $. With partition function in hand, we can go back to the earlier definition and find
	\begin{equation*}
		m = \frac{1}{N \beta }\pdv{}{H} \ln Z = \tanh(\beta(H + Jqm))
	\end{equation*}
	Here's a quick derivation, using $ \ln xy = \ln x + \ln y $, $ \ln e^{x} = x $ and $ \ln x^{N} = N \ln x $
	\begin{align*}
		m &= \frac{1}{N \beta }\pdv{}{H} \ln Z = m = \frac{1}{N \beta }\pdv{}{H}\left[-\frac{1}{2}\beta JNqm^{2} + \ln 2^{N} + N \ln \cosh^{N}(\beta \heff) \right]\\
		&= \frac{1}{\beta }\pdv{}{H} \ln \cosh(\beta(H + Jqm)) = \frac{\sinh(\beta(H + Jqm))}{\cosh(\beta(H + Jqm))} = \tanh(\beta(H + Jqm))
	\end{align*}
\end{itemize}	

\textbf{Solution with $ \bm{H} = 0 $}
\begin{itemize}	
	\item In the absence of an external magnetic field, the average spin simplifies to
	\begin{equation*}
		m = \tanh(\beta J q m) = m \tanh \frac{m}{\tau}
	\end{equation*}
	where we've defined a reduced temperature $ \frac{1}{\tau} = \beta Jq  $. The equation does not have an analytic solution, so we proceed graphically by plotting the dimensionless left hand and right hand sides of the equation on the same axes and looking for the points of intersection.  
	\begin{itemize}
		\item When $ \tau > 1 $ ( corresponding to high temperatures when $ k_{b}T > J q $ ) the only solution is the trivial solution $ m = 0 $. In other words, there is no zero average spin---corresponding to no net magnetization--at high temperatures; the entropy associated with temperature fluctuations dominates the energetically preferred but highly ordered state of aligned spins. The system is paramagnetic.
		
		\item When $ \tau < 1 $ (corresponding to low temperatures when $ k_{b}T < J q $ ), there are non-zero solutions at $ m = \pm m $, corresponding to a non-zero average spin---and a non-zero net magnetization---at low temperatures. One solution correspond to spin up and one to spin down. The system is ferromagnetic.
		
		\item The critical temperature separating the two cases occurs when $ \tau = 1 $ or $ k_{B}T_{c} = Jq $. Importantly, the transition is not smooth (i.e. $ m $ does not slowly approach zero as $ T \to \infty $). Instead, average spin and net magnetization abruptly vanishes at a finite value $ T = T_{c} $, with no magnetization at all for higher temperatures. This corresponds to a \textit{phase transition} between the ferromagnetic and paramagnetic states. 
		
		\item We can be more mathematically precise: If we work close to the phase transition in the limit $ m \approx 0 $ and $ \tau \approx 1 $, we can Taylor expand $ \tanh \frac{m}{\tau} $ to get
		\begin{equation*}
			m \approx \frac{m}{\tau} - \frac{m^{3}}{3\tau^{3}} \eqtext{or} m^{2} = 3 \tau^{3}\left(\frac{1}{\tau} - 1\right) = 3\tau^{2}(1 - \tau)
		\end{equation*}
		We can then write the behavior near the phase transition as
		\begin{equation*}
			m = 
			\begin{cases}
				\sqrt{3\tau^{2}(1 - \tau)} & \tau < 1 \qquad (\text{ferromagnetic})\\
				0	&  \tau > 1	 \qquad (\text{paramagnetic})
			\end{cases}
		\end{equation*}
	\end{itemize}
	
\end{itemize}	

\iffalse
\textbf{Ziherl}
\begin{itemize}	
	\item Because of its spin, each lattice site represents a magnetic dipole. The total energy of the lattice is the sum of the magnetic dipole energy of each lattice site in the magnetic field of its neighbors. 
	\begin{equation*}
		E = - J \sum s_{i} \cdot \sum_{j} s_{j} = - \sum \gamma \hbar s_{i} B_{i}
	\end{equation*}
	where $ i $ runs over all lattice sites and $ j $ over each lattice site's nearest neighbors. The field $ B_{i} $ at each lattice site is
	\begin{equation*}
		B_{i} = \frac{J}{\gamma \hbar}\sum_{j} s_{j}
	\end{equation*}
	where the sum runs over the $ i $th site's nearest neighbors.
	
	\item To simplify the sum, we approximate the each neighbor's spin $ s_{j} $ with the average spin $ \bar{s} $ of all the nearest neighbors. This approximation is called the \textit{mean field approximation}, and leads to an average magnetic field
	\begin{equation*}
		\bar{B} = \frac{J}{\gamma \hbar}\sum_{j} \bar{s} = \frac{J \bar{s} q}{\gamma \hbar}
	\end{equation*}
	where $ q $ denotes the number of nearest neighbors at each lattice site. 
	\begin{equation*}
		\bar{B}  = \frac{J \bar{s} q}{\gamma \hbar} = \frac{J q}{2\mu_{B}} \frac{\bar{\mu}}{2\mu_{B}}
	\end{equation*}
	where $ \bar{\mu}_{z} = \bar{s} \mu_{B} $ is the average magnetic moment of a lattice site and $ \mu_{B}  $ is the Bohr magneton equal to $ \mu_{B} \equiv \frac{e_{0}\hbar}{2m_{e}} = \frac{\gamma \hbar}{2} $.
	
	\item We now consider a system with magnitude of spin $ \abs{S} = \frac{1}{2} $, so the projection $ s $ in the $ z $ direction can take on values $ \pm \frac{1}{2} $.
	
	The lattice then behaves as a two-state system, depending on the value of $ s = \pm \frac{1}{2} $. If we treat the mean field $ B_{i} $ as an external field, we can reuse the results for a paramagnetic two-state system:
	\begin{equation*}
		\bar{\mu}_{z} = \mu_{B}\tanh(\beta \bar{B} \mu_{B})
	\end{equation*}
	which leads to the equation of state
	\begin{equation*}
		\frac{\bar{\mu}_{z}}{\mu_{B}} = \tanh(\beta \bar{B}\mu_{B} ) = \tanh(\frac{\beta \mu_{B}J q \bar{\mu}_{z}}{4 \mu_{B}^{2}}) = \tanh(\frac{\beta J q }{4} \frac{\bar{\mu}_{z}}{\mu_{B}})
	\end{equation*}
	
	\item In terms of reduced temperature $ \frac{1}{\mathcal{T}} = \frac{Jq\beta}{4} $ and reduced magnetization $ \mathcal{M} = \frac{\bar{\mu}_{z}}{\mu_{B}} $, the equation of state reads
	\begin{equation*}
		\mathcal{M} = \tanh \frac{\mathcal{M}}{\mathcal{T}}
	\end{equation*}
	This equation does not have an analytic solution, so we solve it graphically. Different regimes of reduced temperature correspond to
	\begin{itemize}
		\item $ \mathcal{T} > 1$ is solved only for $ \mathcal{M} = 0 $, corresponding to a paramagnetic phase. 
		\item $ \mathcal{T} < 1$ is solved for $ \mathcal{M} \neq 0 $, corresponding to a ferromagnetic phase. 
		\item $ \mathcal{T} = 1$ corresponds to a phase transition between the two phases.
	\end{itemize}
	If we work in the limit $ \mathcal{M} \to 0 $ close to the phase transition where $ \mathcal{T} \approx 1 $, we can Taylor expand $ \tanh \frac{\mathcal{M}}{\mathcal{T}} $ to get
	\begin{equation*}
		\mathcal{M} \approx \frac{\mathcal{M}}{\mathcal{T}} - \frac{\mathcal{M}^{3}}{3\mathcal{T}^{3}} \eqtext{or} \mathcal{M}^{2} = 3 \mathcal{T}^{3}\left(\frac{1}{\mathcal{T}}-1\right) = 3\mathcal{T}^{2}(1 - \mathcal{T})
	\end{equation*}

\end{itemize}
\fi


\subsection{Kinetic Theory of Gases}

\subsubsection{Common Questions}
\begin{enumerate}
    \item Discuss and derive the Boltzmann distribution.

    \item How is the mean speed of gas molecules defined in the kinetic theory of gases? What is a typical order of magnitude? How is the mean free path of gas molecules defined in the kinetic theory of gases? On what quantities does the mean free path depend, and what is a typical order of magnitude?

    \item Derive the ideal gas equation using kinetic theory.

    \item State and derive the diffusion constant of an ideal gas.

    \item What is the microscopic interpretation of the viscosity of a gas in terms of kinetic theory? How do we estimate the magnitude the viscosity in planar geometry? Discuss the behavior of viscosity in the limit in which container separation is much smaller than mean free path.

    \item Derive the expression for the thermal conduction of an ideal gas. In what regime of density does the expression apply? What is the thermal conductivity of a thin layer of a low-density gas in which the mean free path is larger than distance between the hot and cold reservoirs?

\end{enumerate}


\subsubsection{Boltzmann Distribution}
\textit{Discuss and derive the Boltzmann distribution.}
\begin{itemize}
	\item Recall the single-particle classical partition for a monatomic gas in the canonical ensemble
	\begin{equation*}
		Z_{1} = \frac{1}{(2\pi \hbar)^{3}}\int e^{-\beta p^{2}/2m}\diff^{3}\bm{r} \diff^{3} \bm{p} = \frac{V}{(2\pi \hbar)^{3}}\int e^{-\beta p^{2}/2m}\diff^{3} \bm{p} 
	\end{equation*} 
	Changing to an integral over velocity via $ \bm{p} = m \bm{v} $ and $\diff^{3}\bm{p} = m^{3} \diff^{3}\bm{v} $ gives
	\begin{equation*}
		Z_{1} = \frac{m^{3}V}{(2\pi\hbar)^{3}}\int e^{-\beta m v^{2}/2} \diff^{3}\bm{v}
	\end{equation*}
	Finally, taking advantage of rotational symmetry, changing to spherical coordinates and integrating over solid angle using $ \diff^{3}\bm{v} = 4\pi v^{2}\diff v $ gives
	\begin{equation*}
		Z_{1} = \frac{4\pi m^{3}V}{(2\pi\hbar)^{3}}\int v^{2} e^{-\beta m v^{2}/2} \diff v
	\end{equation*}
	
	\item Because the original partition function is the sum/integral over a probability distribution (of energy states), here, too, the partition function must be an integral over a distribution, this time over speeds. Namely, the probability that a gas atom has speed between $ v $ and $ v + \diff v $ is the integrand plus a normalization factor $ \mathcal{N} $
	\begin{equation*}
		f(v) \diff v = \mathcal{N} v^{2} e^{-\beta mv^{2}/2} \diff v = \mathcal{N} v^{2} e^{-mv^{2}/2k_{B}T} \diff v
	\end{equation*}
	
	\item The normalization factor is found from the usual condition that the probabilities sum to one, i.e. $ \int_{0}^{\infty}f(v)\diff v = 1$. The integral is evaluated with integration by parts, the general form to use is $ \int x^{2} e^{-x^{2}} \diff x = \int x x e^{-x^{2}} \diff x$ with $ u = x $ and $ \diff v =  x e^{-x^{2}} \diff x$, which is itself evaluated with a simpler integration by parts. The substitution e.g. $ s = \sqrt{\frac{m}{2k_{B}T}} v $ is helpful to simplify the integration. The result is
	\begin{equation*}
		\mathcal{N} = 4\pi \left(\frac{m}{2\pi k_{B}T} \right)^{3/2}
	\end{equation*}
	
	\item The distribution $ f(v) \diff v $ is called the \textit{Maxwell-Boltzmann distribution}. With $ \mathcal{N} $ plugged in, it reads
	\begin{equation*}
		f(v) \diff v = 4\pi \left(\frac{m}{2\pi k_{B}T} \right)^{3/2} v^{2} e^{-mv^{2}/2k_{B}T} \diff v
	\end{equation*}
	In vector notation (if we had not integrated over solid angle) the distribution reads
	\begin{equation*}
		f(v) \diff^{3} \bm{v} = \left(\frac{m}{2\pi k_{B}T} \right)^{3/2} e^{-mv^{2}/2k_{B}T} \diff^{3} \bm{v}
	\end{equation*}
	The Maxwell distribution gives the probability that an ideal gas atom has speed between $ v $ and $ v + \diff v $. Remarkably, the distribution holds for other gases, including in the presence of inter-molecular interactions.
	
\end{itemize}



\subsubsection{Mean Speed and Mean Free Path}
\textit{How is the mean speed of gas molecules defined in the kinetic theory of gases? What is a typical order of magnitude? How is the mean free path of gas molecules defined in the kinetic theory of gases? On what quantities does the mean free path depend, and what is a typical order of magnitude?} 
\begin{itemize}

	\item The distribution can be used to find the mean square speed of atoms in a gas. The result is
	\begin{equation*}
		\expval{v^{2}} = \int_{0}^{\infty}v^{2} f(v) \diff v = \frac{3k_{B}T}{m}
	\end{equation*}
	This result agrees with equipartition of energy with three degrees of freedom, which predicts the average energy (all kinetic; there is no potential) of a gas particle to be $ \frac{3}{2}k_{B}T $. If we compare this to average kinetic energy derived from $ \expval{v^{2}} $ as given by the Maxwell-Boltzmann distribution we see
	\begin{equation*}
		E = \frac{1}{2}m \expval{v^{2}} = \frac{1}{2}m \left(\frac{3k_{B}T}{m}\right) = \frac{3}{2}k_{B}T
	\end{equation*}
	
	\item Meanwhile, the mean speed $ \expval{v} $, using the substitution $ u = \frac{mv^{2}}{2k_{B}T}$, is
	\begin{align*}
		\expval{v} &= \int_{0}^{\infty}v f(v) \diff v = \int_{0}^{\infty}4\pi \left(\frac{m}{2\pi k_{B}T}\right)^{3/2}v^{3}e^{-\frac{mv^{2}}{2k_{B}T}}\diff v \\
		&=\sqrt{\frac{8k_{B}T}{\pi m}} \int_{0}^{\infty}ue^{-u}\diff u =\sqrt{\frac{8k_{B}T}{\pi m}}
	\end{align*}
	A typical magnitude is $ \expval{v} \approx \SI{450}{\meter \cdot \second^{-1}} $ at room temperature.

	\item Gas particles are constantly colliding with each other as they whiz around in their container. The \textit{mean free path} is the average distance a gas particle travels between collisions. As a simple model to estimate of mean free path, consider a single moving gas particle with characteristic dimension $ \sigma $ and thus cross section $ \pi \sigma^{2} $. The volume traced by the moving particle between collisions is $ V = \pi \sigma^{2} \ell $ where $ \ell $ is the mean free path. If $ n $ is the number density of particles in the volume, our simple one-particle model has $ n V = 1 $, since their is only one particle! We can rearrange to get $ \ell = \frac{1}{\pi n \sigma^{2}} $, but this answer is off by a factor $ \sqrt{2} $, since we ignored other particles. The correct answer is
	\begin{equation*}
		\ell = \frac{1}{\sqrt{2}\pi n \sigma^{2}} 
	\end{equation*}
	where $ n = n(p, T) $ is the number density of the gas particles and is dependent on pressure and temperature. A typical order of magnitude for $ \ell $ is $ \SI{100}{\nano \meter} $ at standard temperature and pressure. This might seem small, but on the scale of molecular radii, which are of the order $ \SI{0.1}{\nano \meter} $, it is an enormous value: it tells us that in standard conditions inter-particle collisions are quite rare indeed.

\end{itemize}

\subsubsection{Ideal Gas Equation with Kinetic Theory}
\textit{Derive the ideal gas equation using kinetic theory.}

\begin{itemize}
	\item The results from the Maxwell-Boltzmann distribution can be used to derive the ideal gas law $ pV = Nk_{B}T $ using the interpretation of pressure arsing from the collisions of gas particles with the walls of an enclosure. Consider a cubic enclosure with sides of length $ L $ containing an ideal gas. Collisions between gas particles and the enclosure walls are essentially elastic, since the mass of the enclosure in any everyday scenario would be vastly larger than the mass of a particle.
	
	A gas particle with mass $ m $ traveling with velocity $ v_{x} $ in the $ x $ direction will bounce off a wall and return with $ x $ velocity $ - v_{x} $, changing its momentum by $ 2 m v_{x}$. The force on the wall, corresponding to the impulse $ F \Delta t = \Delta p_{x}  $ is
	\begin{equation*}
		F = \frac{\Delta p_{x}}{\Delta t} = 2mv_{x}\cdot \frac{v_{x}}{2L}= \frac{mv_{x}^{2}}{L}
	\end{equation*}
	where $ \Delta t = \frac{2L}{v_{x}} $ is the average time for the particle to hit the next wall. 
	
	\item Summing over all $ N $ gas particles, the force on the wall is
	\begin{equation*}
		F = N \frac{m \expval{v_{x}^{2}}}{L}
	\end{equation*}
	where $ \expval{v_{x}^{2}} $ is the average velocity in the $ x $ direction. 
	
	\item By rotational symmetry, the $ x $, $ y $ and $ z $ direction must be equivalent, so $ \expval{v_{x}^{2}} = \expval{v_{y}^{2}} = \expval{v_{z}^{2}} $. It follows  $ \expval{v^{2}} = 3 \expval{v_{x}^{2}} $, and so the force on the wall can be written
	\begin{equation*}
		F = N \frac{m \expval{v^{2}}}{3L}
	\end{equation*}
	
	\item We can now substitute in the earlier result $ \expval{v^{2}} = \frac{3k_{B}T}{m}$ to get
	\begin{equation*}
		F = N \frac{m}{3L} \frac{3k_{B}T}{m} = \frac{Nk_{B}T}{L}
	\end{equation*}
	Pressure is force per area; using $ p = \frac{F}{L^{2}} $ and substituting $ L^{3} = V $ gives
	\begin{equation*}
		p = \frac{F}{L^{2}} = \frac{Nk_{B}T}{L^{3}} = \frac{Nk_{B}T}{V} \implies pV = Nk_{B}T
	\end{equation*}
\end{itemize}

\subsubsection{Diffusion and Kinetic Theory}
\textit{State and derive the diffusion constant of an ideal gas.}
\begin{itemize}
	\item First, we find the current density of incident particles. Consider a flux of gas particles with speed $ v $ incident at an angle $ \theta $ on a hypothetical area $ A $, where $ A $ is oriented normal to the $ x $ direction. A particle's $ x $ component of velocity is then $ v_{x} = v \cos \theta $. The number of particles $ \Delta N $ incident on the area in the time $ \Delta t $ is
	\begin{equation*}
		\Delta N = nV = n A \expval{v_{x}} \Delta t =  n A \expval{v} \cos \theta \Delta t
	\end{equation*}
	where $ n $ is the number density of particles. We rearrange and take the limit $ \Delta t \to 0 $ to get the particle current density $ j $:
	\begin{equation*}
		\diff j = \lim_{\Delta t \to 0} \frac{\Delta N}{A \Delta t} = \frac{n \expval{v} \cos \theta}{4\pi} \diff \Omega = \frac{n\expval{v}}{2} \cos \theta \sin \theta \diff \theta
	\end{equation*}
	where $ \diff \Omega  = 2\pi \sin \theta \diff \theta$ is the element of solid angle over which the particles are incident. Integrating $ \theta $ over $ 0 $ to $ \frac{\pi}{2} $ (the range of incident angle) gives
	\begin{equation*}
		j = \int \diff j = \int_{0}^{\frac{\pi}{2}} \frac{n\expval{v}}{2} \cos \theta \sin \theta \diff \theta = \frac{n\expval{v}}{2} \int_{0}^{1} u \diff u = \frac{n\expval{v}}{4}
	\end{equation*}
	which is the current density of incident particles.
	
	\item To derive the diffusion equation, consider a hypothetical area $ A $ inside a linear particle density gradient
	\begin{equation*}
		n(x) = n_{0} + x \dv{n}{x}
	\end{equation*}
	If there is current density $ j_{in} $ incident on $ A $ in the positive $ x $ direction and $ j_{out} $ in the negative $ x $ direction, the net current density is
	\begin{equation*}
		j_{net} = (j_{in} - j_{out}) 
	\end{equation*}
	From the previous bullet point we know $ j = \frac{n\expval{v}}{4} $. If we assume only particles within the mean free path cross through $ A $ in the time $ \Delta t $, we can take $ n $ to be $ n(\pm \ell) $ and get
	\begin{equation*}
		j_{net} = (j_{in} - j_{out}) = \frac{\expval{v}}{4}\left(n(-\ell) - n(\ell)\right)
	\end{equation*}
	Using the linear density gradient $ n(x) = n_{0} + x \dv{n}{x} $ gives
	\begin{equation*}
		j_{net} = \frac{\expval{v}}{4}\left[(n_{0} -\ell \dv{n}{x}) - (n_{0} + \ell \dv{n}{x})\right] = -\frac{\expval{v}\ell}{2} \dv{n}{x}
	\end{equation*}
	
	\item Because of rotational symmetry, we can make an analogous argument for the $ y $ and $ z $ directions. The net result is
	\begin{equation*}
		\bm{j}_{net} = -\frac{\expval{v}\ell}{2} \left(\dv{n}{x} + \dv{n}{y} + \dv{n}{z} \right) = - \frac{\expval{v}\ell}{2} \nabla n(\bm{r})
	\end{equation*}
	which is the diffusion equation for an ideal gas. The diffusion constant is $ D = \frac{\expval{v}\ell}{2} $ and is approximately equal to $ 0.6 \expval{v} \ell $ in experiment---our simplified model is off by a slight factor.
\end{itemize}


\subsubsection{Viscosity and Kinetic Theory}
\textit{What is the microscopic interpretation of the viscosity of a gas in terms of kinetic theory? How do we estimate the magnitude the viscosity in planar geometry? Discuss the behavior of viscosity in the limit in which container separation is much smaller than mean free path.}

\begin{itemize}
	\item In a microscopic interpretation, viscosity is the result of momentum transfer between a fluid and the container walls. As a simple model, we assume a gas is bounded between two large plates. The plates are separated vertically along the $ z $ axis with the midpoint at $ z = 0 $. The top plate moves in the positive $ x $ direction with velocity $ +\frac{u}{2} $ and the bottom plate moves in the negative $ x $ direction with velocity $ -\frac{u}{2} $. 
	
	The gas particles between the plates experience shear forces as the plates move relative to each other and the gas between them, which leads to a gas velocity gradient varying linearly with height $ z $:
	\begin{equation*}
		\expval{v}(z) = z \dv{}{z}\expval{v}
	\end{equation*}
	The gas particles at the top plate have average velocity $ +\frac{u}{2} $, particles at the bottom plate have average velocity $ -\frac{u}{2} $, and particles in the middle have $ \expval{v} = 0 $.
	
	\item If a particle crosses from the lower to the upper region its average velocity will change from $ -\expval{v} $ to $ \expval{v} $. If we assume only particles within a mean free path of the midpoint will cross, the corresponding change in momentum is
	\begin{equation*}
		\Delta p_{1} = m\left(\expval{v}(\ell) - \expval{v}(-\ell) \right) = m \left(\dv{u}{z}\ell - (-\ell) \dv{u}{z}\right) = 2 ml \dv{u}{z}
	\end{equation*}
	The total shear force from all the particles together can be written in terms of the current density $ j $ and the single-particle change in momentum $ \Delta p_{1} $:
	\begin{equation*}
		F = \frac{\Delta p_{tot}}{\Delta t} = A j \Delta p_{1}
	\end{equation*}
	If we write $ j = \frac{n \expval{v}}{4} $, where $ \expval{v} $ would be the mean particle speed in the absence of shear forces, we have
	\begin{equation*}
		F = A j \Delta p_{1} = A \left(\frac{n \expval{v}}{4}\right) \left(2 ml \dv{u}{z}\right) = \frac{A\rho \expval{v}\ell}{2} \dv{u}{z}
	\end{equation*}
	where $ \rho = n m $ is the particle density. Dividing through by plate area $ A $ gives an estimate for the viscosity $ \eta $:
	\begin{equation*}
		\frac{F}{A} = \frac{\rho \expval{v}\ell}{2} \dv{u}{z} = \eta \dv{u}{z} \eqtext{or} \eta = \frac{\rho \expval{v}\ell}{2}
	\end{equation*}
	Note that viscosity is proportional to $ \rho, \expval{v} $ and $ \ell $.
\end{itemize}



\subsubsection{Thermal Conductivity of an Ideal Gas}
\textit{Derive the expression for the thermal conduction of an ideal gas. In what regime of density does the expression apply? What is the thermal conductivity of a thin layer of a low-density gas in which the mean free path is larger than distance between the hot and cold reservoirs?}

\begin{itemize}
	\item Following the pattern from the previous question, consider a volume of gas bounded between two large plates of area $ A $, separated vertically along the $ z $ axis. Let the upper plate be at temperature $ T_{u} $ and the lower plate at temperature $ T_{l} $, and assume a linear temperature gradient
	\begin{equation*}
		T(z) = T_{0} + z \dv{T}{z}
	\end{equation*}
	A particle crossing from the upper region to the lower region will change its mean speed $ \expval{v} $, and thus its kinetic energy $ E $, in response to the change in temperature. This transfer of energy as particles move across the temperature gradient is the microscopic interpretation of heat. If we assume only particles within the mean free path cross the barrier, the heat transferred by a single particle is
	\begin{align*}
		Q_{1} &= \expval{E}_{l} - \expval{E}_{u} = \frac{3}{2}k_{B} (T(-\ell) - T(\ell)) = \frac{3}{2}k_{B}  \left[\left(T_{0} -\ell \dv{T}{z}\right) - (T_{0} + \ell \dv{T}{z})\right] \\
		&= -3 k_{B} \ell \dv{T}{z}
	\end{align*}
	
	\item The total heat transferred by all the particles in the time $ \Delta t $ can be expressed in terms of the particle current density and single-particle heat as
	\begin{equation*}
		Q_{tot} = (j A \Delta t) Q_{1} = \frac{n \expval{v} A \Delta t}{4}Q_{1} = - \frac{3}{4}  k_{B} \ell n \expval{v} A \Delta t\dv{T}{x}
	\end{equation*}
	The heat current density $ j_{Q} $ is then
	\begin{equation*}
		j_{Q} = \frac{Q_{tot}}{A \Delta T} = - \frac{3}{4}  k_{B} \ell n \expval{v} \dv{T}{z}
	\end{equation*}
	
	\item As before, we can use rotational symmetry to make an analogous argument in the $ x $ and $ y $ directions. In three dimensions, the heat current density is then
	\begin{equation*}
		\bm{j}_{Q} = - \frac{3}{4}  k_{B} \ell n \expval{v} \left(\dv{T}{x} + \dv{T}{y} + \dv{T}{z}\right) = - \frac{3}{4}  k_{B} \ell n \expval{v} \nabla T(\bm{r})
	\end{equation*}
	The associated coefficient of thermal conductivity is
	\begin{equation*}
		\lambda = \frac{3}{4}  k_{B} \ell n \expval{v}
	\end{equation*}
	
	\item The derivation holds only if the mean free path between collisions is much smaller than the separation of the container plates. In this case, there are many more collisions between gas particles than collisions between a particle and the container, which allows for energy transfer through the gas.
	
	If the distance between the containers is much smaller than the mean free path (e.g. $ \ell \gg h $), the particles bounce back and forth between the walls much more often than they collide with each other, and we have to proceed differently. In this case, particles from the upper plate have kinetic energy $ E = \frac{3}{2}k_{B}T_{u} $ and particles from the lower plate have $  E = \frac{3}{2}k_{B}T_{l} $, and the heat transferred per particle is
	\begin{equation*}
		Q_{1} = \frac{3}{2}k_{B}\left(T_{l} - T_{u} \right)
	\end{equation*}
	As before, the total heat, in terms of $ Q_{1} $ and particle current density $ j $, transferred in the time $ \Delta t $ is
	\begin{equation*}
		Q_{tot} = A \Delta t j Q_{1} = A \Delta t \left(\frac{n \expval{v}}{4} \right)\frac{3}{2}k_{B}\left(T_{u} - T_{l} \right)
	\end{equation*}
	while the heat current density is
	\begin{equation*}
		j_{Q} = \frac{Q}{A \Delta t} = \frac{3}{8} n \expval{v} k_{B} (T_{u} - T_{l})
	\end{equation*}
	In this case, the heat current density is no longer a function of the temperature gradient; it depends simply on the temperature difference between the plates. 
	
\end{itemize}


\end{document}



