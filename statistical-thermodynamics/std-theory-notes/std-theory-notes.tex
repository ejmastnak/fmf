\documentclass[11pt, a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage{mwe}
\usepackage[margin=3.5cm]{geometry}
\usepackage{fancyhdr}
\usepackage{truncate}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{bm} % for bold vectors in math mode
\usepackage{physics} % many useful physics commands
\usepackage[separate-uncertainty=true]{siunitx} % for scientific notation and units
\usepackage{xcolor}  % to color hyperref links
\usepackage[colorlinks = true, linkcolor = blue, urlcolor  = blue, citecolor = blue, anchorcolor = blue]{hyperref}

\setlength{\parindent}{0pt} % to stop indenting new paragraphs
\newcommand{\eqtext}[1]{\qquad \text{#1} \qquad}

\newcommand{\diff}{\mathop{}\!\mathrm{d}} % differential
\newcommand{\dbar}{{\mathchar '26\mkern -11mu \mathrm{d}}} % for improper differentials

\newcommand{\schro}{Schr\"{o}dinger\xspace}

\pdfinfo{
	/Title (Statistical Thermodynamics Theory Notes)
	/Author (Elijan Mastnak)
	/Subject (Physics)
}

% begin header configuration
\fancypagestyle{headerstyle}{
\fancyhf{}  % clear default settings from header and footer; create blank slate
\fancyhead[R]{\href{https://github.com/ejmastnak/fmf}{\small{\texttt{github.com/ejmastnak/fmf}}}}  % link in upper right
\fancyhead[L]{\textit{\truncate{0.65\headwidth}{\rightmark}}}  % subsection name; truncate long names
\fancyfoot[C]{\thepage}  % centered page number in footer
\renewcommand{\headrulewidth}{0.1pt}
}
% end header configuration


\begin{document}
\title{Statistical Thermodynamics Theory Notes}
\author{Elijan Mastnak}
\date{2019-2020 Academic Year}
\maketitle

\begin{center}
\textbf{About These Notes}
\end{center}
These notes are my plebeian interpretation of Professor David Tong's excellent lecture notes on \href{http://www.damtp.cam.ac.uk/user/tong/statphys.html}{Statistical Physics}, generously made freely available on \href{http://www.damtp.cam.ac.uk/user/tong/teaching.html}{\underline{his website}}. I wrote my own copy while studying for the similar course \textit{Statistical Thermodynamics} at the University of Ljubljana because I found actively typing up notes---instead of just passively reading someone else's---supercharged my understanding and retention. 

I'm making these notes available in the hope they might help others, but I enthusiastically recommend that you first check out Professor Tong's notes instead. I doubt my version does anything that Tong's notes don't already do better.

\vspace{2mm}
\textit{Navigation}: For easier document navigation, the table of contents is ``clickable'', meaning you can jump directly to a section by clicking the colored section names in the table of contents. Unfortunately, \textit{the clickable links do not work in most online or mobile PDF viewers}; you have to download the file first.

\vspace{2mm}
\textit{On Authorship:} All credit for the content in these notes goes entirely to Professor David Tong, and I make no claim whatsoever to originality or authorship. If you're not already familiar with them, I encourage you to check out \href{http://www.damtp.cam.ac.uk/user/tong/teaching.html}{Professor Tong's notes}. They're awesome.

\newpage

\tableofcontents

\newpage

\pagestyle{headerstyle}
\section{Statistical Physics}

\subsection{The Microcanonical Ensemble}

\subsubsection{Some Groundwork for the Microcanonical Ensemble}
\begin{itemize} 
	\item 	Statistical mechanics is about taking a system of many (e.g. $\sim 10^{23} $) particles predicting something useful about its macroscopic behavior given the particle's microscopic properties and interactions. It is about deriving the diverse macroscopic behavior of matter from a few fundamental principles (Newtonian mechanics, quantum mechanics, and statistics) applied to the matter's microscopic constituents. 
	
	\item The \textit{energy microstates} of an isolated system of particles at energy $ E $ with $ N \sim 10^{23} $ degrees of freedom are the energy eigenstates of the \schro equation 
	\begin{equation*}
		\hat{H}\ket{\psi} = E\ket{\psi}
	\end{equation*}
	For such a large number of degrees of freedom, the eigenstates are complicated and not useful in practice.
	
	\item Instead of working in terms of individual quantum states, statistical mechanics describe a system as a \textit{probability distribution} over all possible quantum microstates.
	
	\item For a system of fixed energy $ E $, there is a non-zero probability only for states with energy $ E $. We denote the basis of the allowed states by $ \ket{n} $ and the probability that the system is in a given state as $ p(n) $. In this case (within this probability distribution), the expectation value of any operator $ \hat{O} $ is 
	\begin{equation*}
		\langle \hat{\mathcal{O}} \rangle = \sum_{n}p(n)\bra{n}\hat{\mathcal{O}}\ket{n}
	\end{equation*}
	
	\item What type of probability distribution is appropriate for large systems? For now, we consider systems only in a macroscopically steady state of energy and momentum. In this case, the probability distribution, and thus expectation values of operators, are time-independent. If a system is in macroscopically steady state of energy and momentum, it is said to be in \textit{equilibrium}.
	
	\item Here is a fundamental assumption of statistical mechanics:
	\begin{quote}
		For an isolated system in equilibrium, all accessible microstates are equally likely.
	\end{quote}
	The word accessible is used intentionally (instead of e.g. allowed) to imply a bit of flexibility. Accessible microstates can include states that could be reached with small perturbations of the system. For now, for an isolated system of fixed energy in equilibrium, accessible means all states with the fixed energy $ E $.
	
	\item Notation: The number of states in the system with energy $ E $ is denoted by $ \Omega(E) $ and is usually at least of the order $ \sim 10^{23} $.
	\begin{equation*}
		\Omega(E) = \text{number of states with energy $ E $}
	\end{equation*}
	for a quantum system with two spin degrees of freedom, the total number of microstates is of the order $ \sim 2^{10^{23}} $. This is a very large number!
	
	Formally, because energy levels are discrete in quantum systems, $ \Omega(E) $ measured the number of states with energy between $ E $ and $ \delta E $ where $ \delta E $ is small compared to the accuracy of the measuring instrument but large compared to the energy level spacing. Of course, the energy levels, with spacing of the order $ \sim 2^{-10^{23}} $, are effectively continuous.
	
	
	\item The probability an isolated system of fixed energy $ E $ is in a the microstate $ \ket{n} $ is 
	\begin{equation*}
		p(n) = \frac{1}{\Omega(E)}
	\end{equation*}
	
	\item \textbf{Note:} when system is used in this section without further specification, it means an isolated system of many particles at fixed energy $ E $.
	
	Also, microstates, energy states and quantum states and simply states refer to the same concept of microstates unless explicitly stated otherwise.
\end{itemize}

\subsubsection{Entropy}
\begin{itemize}
	\item The \textit{entropy} of a system at energy $ E $ is defined as
	\begin{equation*}
		S(E) = k_{B} \ln \Omega(E)
	\end{equation*}
	where $ \Omega(E) $ is the number of states at energy $ E $ and $ k_{B} = \SI{1.381e-23}{\joule \, \kelvin^{-1}} $.
	
	\item Entropy is additive across non-interacting systems. For two non-interacting systems with energies $ E_{1} $ and $ E_{2} $ the total number of states for both system considered as a whole is 
	\begin{equation*}
		\Omega(E_{1}, E_{2}) \propto \Omega_{1}(E_{1})\Omega_{2}(E_{2})
	\end{equation*}
	while, because of the logarithm $ S \sim \ln \Omega $, the total entropy is
	\begin{equation*}
		S(E_{1}, E_{2}) \propto S_{1}(E_{2}) + S_{2}(E_{2})
	\end{equation*}
\end{itemize}

\subsubsection{On the Second Law}
\begin{itemize}
	\item Consider taking two systems with energies $ E_{1} $ and $ E_{2} $ and placing them together so that they can exchange energy. Assume the systems exchange energy in a way that the energy levels of each system shift negligibly; the only relevant interaction is transfer of energy between the systems. The energy of the combined system is $ E_{tot} = E_{1} + E_{2} $.
	
	\item  What happens to the energies of the individual systems after energy transfer? After energy transfer, the energy of the first system can be \textit{any} energy $ E \leq E_{tot} $, which means the energy of the second system is $ E_{tot} - E $. 
	
	Technically, the first system can only take on discrete energies $ E_{i} $ that are energy eigenvalues of its Hamiltonian. In this case, the first system can have energies $ E_{i} \leq E_{tot} $, so the second system can have energies $ E_{tot} - E_{i} $. 
	
	\textit{Technicality:} As quantum systems, both system 1 and 2 have discrete energy levels. This is why we had to restrict the energies system 1 could take on to the discrete energies $ E_{i} $. We said system 1 could take on the energies $ E_{i} $ and then assume the energies of system 2 were the discrete energies $ E_{tot} - E_{i} $. However, there is no reason to know a priori that the discrete energies $ E_{tot} - E_{i} $ are allowed energy eigenvalues of the second system! In practice, the resolution of this dilemma is to ignore it. Because the energy levels are so closely spaced, they are effectively continuous, and the quantum considerations of discreteness are not important.
	
	\item Recall from before that for two \textit{non-interacting} systems considered as a whole, the number of states is 
	\begin{equation*}
		\Omega(E_{1}, E_{2}) \propto \Omega_{1}(E_{1})\Omega_{2}(E_{2})
	\end{equation*}
	In our case, the systems can exchange energies and $ E_{1} $ can take on any energies $ E_{i} $ in the range $ [0, E_{tot}] $, while $ E_{2} $ can take on energies $ E_{tot} - E_{i} $, so must sum $ \Omega(E_{tot}) $ over all possible energies $ E_{i} $. The result is 
	\begin{equation*}
		\Omega(E_{tot}) = \sum_{\{E_{i}\}}\Omega_{1}(E_{i})\Omega_{2}(E_{tot} - E_{i})
	\end{equation*}
	where $ i $ runs over all the allowed energy eigenvalues $ E_{i} $ of the first system, which fall in the range $ [0, E_{tot}] $.
	
	\item Inverting the definition of entropy $ S(E) = k_{B}\ln \Omega(E) $, produces $ \Omega(E) = \exp \frac{S(E)}{k_{B}} $, which leads to 
	\begin{align*}
		\Omega(E_{tot}) &= \sum_{\{E_{i}\}} \exp(\frac{S_{1}(E_{i})}{k_{B}}) \exp(\frac{S_{1}(E_{tot} - E_{i})}{k_{B}}) \\
		& = \sum_{\{E_{i}\}} \exp(\frac{S_{1}(E_{i})}{k_{B}} + \frac{S_{1}(E_{tot} - E_{i})}{k_{B}})
	\end{align*}
	

	
	\item After interaction, the combined system has the fixed energy $ E_{tot} $, so it rests in a microcanonical ensemble with distribution $ p(n) = \frac{1}{\Omega(E_{tot})}$. Because the microstates of the original systems are a subset of the total number of possible states of the combined system, the entropy $ S(E_{tot}) $ of the combined system is greater than or equal to the entropy $ S_{1}(E_{1}) + S_{2}(E_{2}) $ of the original system in which the two systems $ 1 $ and $ 2 $ were separate and non-interacting.

	Or, more simply, because there are at least as many possible states in the combined system as in the separated isolated systems, the entropy of the combined systems is at least as large as the entropy of the isolated systems considered separately. In any case,
	\begin{equation*}
		S_{E_{tot}} \geq S_{1}(E_{1}) + S_{2}(E_{2})
	\end{equation*}
	
	\item \textit{Important approximation:} Consider $ S_{E_{tot}} \equiv k_{B}\ln \Omega(E_{tot}) \geq S_{1}(E_{1}) + S_{2}(E_{2}) $ when the number of particles $ N $ is very large. 
	
	Recall the expression for the total number of allowed microstates in the combined system is 
	\begin{equation*}
		\Omega(E_{tot}) = \sum_{i} \exp(\frac{S_{1}(E_{i})}{k_{B}} + \frac{S_{2}(E_{tot} - E_{i})}{k_{B}})
	\end{equation*}	
	Entropy scales as $ S \sim N $, so $ \Omega $ scales as a sum of exponential of $ N $. But $ N \sim 10^{23} $ is itself an exponential, so $ \Omega $ scales as a sum of exponentials of exponentials! Because of how rapidly exponentials grow with increasing arguments, the \textit{sum of exponentials with exponentially increasing arguments is completely dominated by the maximum term}. In our particular case, the system's total number of allowed microstates is very well approximated by the single term
	\begin{equation*}
		\Omega(E_{tot}) \approx \exp(\frac{S_{1}(E_{max})}{k_{B}} + \frac{S_{2}(E_{tot} - E_{max})}{k_{B}})
	\end{equation*}	
	And the total entropy of the system is likewise very well approximated by 
	\begin{equation*}
		S_{E_{tot}} \approx S_{1}(E_{max}) + S_{2}(E_{tot} - E_{max})
	\end{equation*}		
 The approximation may not be totally intuitive because we do not encounter such large numbers in everyday life, but it is completely mathematically valid to approximate the sum by its largest term. As a example, assume reasonably that some large $ E_{max} $ is slightly larger than all other energies and the exponent expression $ \sim e^{E_{max}} $ is twice as large as for any other $ E $. In this case, the term in the sum for $ \Omega_{E_{tot}} $ corresponding to $ E_{max} $ will be larger by all other terms by factor $ e^{N} \sim e^{10^{23}} $. All other terms are completely negligible.

	\item The maximum value $ E_{max} $ occurs at the point satisfying the condition 
	\begin{equation*}
		\pdv{S_{1}}{E} (E_{max}) - \pdv{S_{2}}{E} (E_{tot} - E_{max}) = 0
	\end{equation*}
	
	\item Note there is no fundamental a priori physical reason why the first system should have the fixed energy $ E_{max} $ once it transfers energy with the second system. (Recall that all energies $ E_{i} $ are possible). Instead, it is just \textit{overwhelmingly likely} to be found with energy $ E_{max} $, (the energy maximizing the number of microstates of the combined system), because of the aforementioned behavior of exponentials causing the maximum term in the sum for $ \Omega(E_{tot}) $ to utterly dominate the other terms. \textit{Because there are so many more states available at energy $ E_{max} $ than at any other energy (by a factor $ \sim e^{10^{23}} $), it makes sense the first system would occupy a state with energy $ E_{max} $.} 
	
	More so, once the first system has energy $ E_{max} $, it is likewise overwhelmingly unlikely to be found in a state with energy other than $ E_{max} $.
	
	Note: The key concept at this point is not calculating the exact value of $ E_{max} $ for a given system. We are satisfied knowing $ E_{max} $ exists, that it is the largest energy, and thus the energy maximizing the number $ \Omega $ of available states.
	
	\item It is the \textit{overwhelming probability} of the first system having energy $ E_{max} $ (rather than physical certainty) that is responsible for the second law of thermodynamics and the principle that entropy increases in all physical processes. When two systems are brought together, energy is all statistically all but certain to be transferred in such a way that the total number of available for the combined systems is vastly enlarged compared to when the constituent systems where separate.
	
\end{itemize}

\subsubsection{Temperature}
\begin{itemize}
	\item In statistical mechanics the temperature $ T $ of a system is defined as
	\begin{equation*}
		\frac{1}{T} = \pdv{S}{E}
	\end{equation*}
	A fundamental property in statistical mechanics is: \textit{If two separate systems in equilibrium, both at the same temperature $ T $, are placed in contact, nothing happens}! The systems stay the same and no energy is transferred between them.
	
	\item How to explain this? We showed earlier by statistical arguments how two such systems placed in contact will (are overwhelmingly likely to) maximize their entropy (i.e. maximize the number of available microstates $ \Omega $), which occurs when the first system takes on the energy $ E_{max} $ and the second system takes on the energy $ E_{tot} - E_{max} $, where $ E_{max} $ is defined by
	\begin{equation*}
		\pdv{S_{1}}{E} (E_{max}) - \pdv{S_{2}}{E} (E_{tot} - E_{max}) = 0
	\end{equation*}
	If nothing happens when the systems are placed in contact, the first system must already have been at energy $ E_{max} $. This means that before being placed in contact, $ E_{1} \equiv E_{max} $ and $ E_{2} \equiv E_{tot} - E_{max} $. In this case, the equation defining $ E_{max} $ reads
	\begin{equation*}
		\pdv{S_{1}}{E} (E_{1}) = \pdv{S_{2}}{E} (E_{2})
	\end{equation*}
	which, from the definition $ \frac{1}{T} = \pdv{S}{E} $, occurs precisely when the system's temperatures $ T_{1} $ and $ T_{2} $ were already equal! The condition that no energy is transferred between the systems leads directly to $ T_{1} = T_{2} $.
	
	\item What happens when two systems at slightly different temperatures are placed in contact? The systems will exchange energy, and conservation of energy ensures the changes obey $ \delta E_{1} = -\delta E_{2} $. 
	
	Because entropy is additive, the entropy of the combined system is $ S(E_{tot}) = S_{1}(E_{1}) + S_{2}(E_{2}) $. For small changes in entropy $ \delta S $, a differential approximation and the identity $ \delta E_{1} = -\delta E_{2}  $ (conservation of total energy) leads to
	\begin{align*}
		\delta S &= \pdv{S_{1}(E_{1})}{E}\delta E_{1} + \pdv{S_{2}(E_{2})}{E}\delta E_{2} = \left( \pdv{S_{1}(E_{1})}{E} - \pdv{S_{2}(E_{2})}{E} \right)\delta E_{1}\\
		&=\left(\frac{1}{T_{1}} - \frac{1}{T_{2}}\right) \delta E_{1}
	\end{align*}
	This is an important result. The second law states that $ \delta S $ is always positive. This implies that energy always flows from the system with higher temperature to the system with lower temperature. For example, if $ \delta E_{1} $ is positive, meaning energy flowed into system 1 $ T_{2} > T_{2} $ to satisfy $ \delta S > 0 $.
	
	Mathematically, the energy flows from higher to lower temperature because $ \pdv{S}{E} $ is a monotonically decreasing function of temperature.
\end{itemize}

\subsubsection{Heat Capacity}
\begin{itemize}
	\item A system's heat capacity $ C $ is defined as
	\begin{equation*}
		C = \pdv{E}{T}
	\end{equation*}
	where $ E $ is the energy added to system as heat (which we haven't properly defined yet). Heat capacity measures the amount of heat energy added to a system needed to produce a given change in temperature. It is particularly useful because it is defined in terms of energy and temperature, which are both easily measurable quantities (as opposed to entropy).
	
	\item An alternate expression for heat capacity comes from viewing energy as a function of temperature and applying the chain rule and definition of temperature $ \frac{1}{T} = \pdv{S}{E} $.
	\begin{equation*}
		\pdv{S}{T} = \pdv{S}{E} \pdv{E}{T} = \left (\frac{1}{T}\right )(C) = \frac{C}{T} \implies C = T \pdv{S}{T}
	\end{equation*}
	Solving the expression $ C = T \pdv{S}{T} $ for $ S $ via integration leads to
	\begin{equation*}
		\int_{S_{1}}^{S_{2}}\diff S = \Delta S = \int_{T_{1}}^{T_{2}}\frac{C(T)}{T}\diff T
	\end{equation*}
	This is a useful, experimentally measurable expression for a system's entropy change during a process involving a temperature change.
	
	\item A system's heat capacity is proportional to the number $ N $ of particle's in the system; the more particles, the larger the heat capacity. A system's \textit{specific heat capacity} is the heat capacity divided (normalized) by the system's mass (or sometimes by $ N $), and is independent of $ N $. 
	
	\item Differentiating the definition of temperature $ \frac{1}{T} = \pdv{S(E)}{E} $ with energy viewed as a function of temperature leads to 
	\begin{equation*}
		-\frac{1}{T^{2}} = \pdv[2]{S}{E} \cdot \pdv{E}{T} = C \pdv[2]{S}{E}  \implies  \pdv[2]{S}{E}  = -\frac{1}{CT^{2}}
	\end{equation*}
	Now, $ \frac{1}{T^{2}} $ is always positive, so as long as $ C > 0 $, $ \pdv[2]{S}{E} < 0 $, meaning that all extrema of $ S(E) $ are maxima by the second derivative test. The fact that all extrema of $ S(E) $ are maxima is important! Why? It means that entropy is indeed maximized (and not e.g. minimized) at energies $ E_{max} $ satisfying the condition
	\begin{equation*}
		\pdv{S_{1}}{E} (E_{max}) = \pdv{S_{2}}{E} (E_{tot} - E_{max})
	\end{equation*}
	which was important in the discussion of two systems placed in contact. Systems with $ C > 0 $ (for which extrema of $ S(E) $ are guaranteed to be maxima) are called \textit{thermodynamically stable}.
	
\end{itemize}

\subsubsection{Example: The Two State System in the Microcanonical Ensemble}
\begin{itemize}
	\item Our so-called two state system is a system of $ N $ non-interacting particles, each fixed in position so that the system's volume is constant. Each individual particle can occupy one of two possible states, e.g. spin up $ \ket{\uparrow} $ or spin down $ \ket{\downarrow} $. 
	
	\item The states have energy $ E_{\downarrow} = 0$ and $ E_{\uparrow} = \epsilon $, so it is energetically favorable for the particles to be in the spin down position, i.e. the position with lower energy. 
	
	Suppose there are $ N_{\uparrow} $ particles with spin up and $ N_{\downarrow} = N - N_{\uparrow}$ particles with spin down, so the energy of the system is 
	\begin{equation*}
		E = N_{\uparrow} \epsilon
	\end{equation*}
	
	\item The number of states $ \Omega(E) $ of the total system at the energy $ E $ is the number of ways to choose $ N_{\uparrow} $ particles from a total of $ N $, which from basic combinatorics is 
	\begin{equation*}
		\Omega(E) = \frac{N!}{N_{\uparrow}!(N - N_{\uparrow}!)}
	\end{equation*}
	The two-state system's entropy is thus
	\begin{equation*}
		S(E) = k_{B}\ln \Omega(E) = k_{B} \ln(\frac{N!}{N_{\uparrow}!(N - N_{\uparrow}!)})
	\end{equation*}
	
	\item Using Stirling's approximation $ \ln N! \approx N\ln N - N $ and canceling common terms, the system's entropy is very nearly
	\begin{equation*}
		S(E) = -k_{B}\big[ (N-N_{\uparrow})\ln(N-N_{\uparrow}) + N_{\uparrow}\ln N_{\uparrow} - N \ln N \big]
	\end{equation*}
	Adding and subtracting $ N_{\uparrow}\ln N $ from the right side of the equation, applying logarithm properties and substituting in the energy expression $ E = N_{\uparrow} \epsilon $ gives
	\begin{align*}
		S(E) &= -k_{B} \left[(N - N_{\uparrow})\ln(\frac{N - N_{\uparrow}}{N}) + N_{\uparrow} \ln(\frac{N_{\uparrow}}{N}) \right]\\
		&= -k_{B}N \left[\left (1 - \frac{E}{N\epsilon}\right )\ln(1 - \frac{E}{N\epsilon}) + \frac{E}{N\epsilon} \ln(\frac{E}{N\epsilon}) \right]
	\end{align*}
	Note that the entropy vanishes for $ E = 0 $ (all spins down) and $ E = N\epsilon $ (all spins up), because there is only one microstate for these energies. The microstates and entropy are maximized for $ E = \frac{N\epsilon}{2} $ (half spin up and half spin down) when $ S = N k_{B} \ln 2 $.
	
	\item If the system has energy $ E $, its temperature is
	\begin{equation*}
		\frac{1}{T} = \pdv{S}{E} = \frac{k_{B}}{\epsilon}\ln(\frac{N\epsilon}{E} - 1)
	\end{equation*}
	From this expression, the fraction of particles with spin up at temperature $ T $ is
	\begin{equation*}
		\frac{N_{\uparrow}}{N} = \frac{E}{N\epsilon} =  \frac{1}{e^{\epsilon/k_{B}T} + 1} 
	\end{equation*}
	As $ T \to \infty $, the fraction approaches $ \frac{N_{\uparrow}}{N} = \frac{1}{2} $, the configuration of maximum entropy.
	
	\item For energies $ E > \frac{N\epsilon}{2} $ where $ \frac{N_{\uparrow}}{N} > \frac{1}{2}$, the temperature as defined by $ \frac{1}{T} = \pdv{S}{E} $ is negative, which can be interpreted as hotter than infinity. In systems with negative temperature, the entropy/number of microstates decreases with increasing energy. Such systems can be momentarily achieved in laboratory conditions by instantaneously inverting all particle spins.
	
	\item From the relationship $ E = \frac{N\epsilon}{e^{\epsilon/k_{B}T} + 1} $, the system's heat capacity $ C = \dv{E}{T} $ is 
	\begin{equation*}
		C = \frac{N \epsilon^{2}}{k_{B}T^{2}} \frac{e^{\epsilon/k_{B}T}}{(e^{\epsilon/k_{B}T} + 1)^{2}} 
	\end{equation*}
	As expected, $ C \sim N $. Note that $ C $ increases with temperature, reaches a maximum near the characteristic temperature $ T \sim \frac{\epsilon}{k_{B}} $, then decreases. As $ T \to 0 $, $ C $ approaches zero exponentially as $ C \sim e^{-1/T} $.
\end{itemize}

\subsubsection{Volume, Pressure, and The First Law of Thermodynamics}
\begin{itemize}
	\item So far, we have considered systems described by their energy $ E $, number of particles $ N $, and temperature $ T $. The next parameter to consider is the system's volume $ V $. With volume introduced, the system's number of microstates $ \Omega $ and entropy $ S $ are functions of both energy and volume:
	\begin{equation*}
		S = S(E, V) \qquad \Omega = \Omega(E, V), \qquad S(E, V) = k_{B}\ln \Omega(E, V)
	\end{equation*}
	
	\item The system's temperature is still defined as $ \frac{1}{T} = \pdv{S}{E} $, where the partial derivative with respect to $ E $ implies that volume $ V $ is fixed during the differentiation. 
	
	Differentiating entropy with respect to $ V $, however, gives rise to a new quantity: pressure, defined as
	\begin{equation*}
		p = T \pdv{S}{V}
	\end{equation*}
	
	\item Recall that when two separate systems are placed in contact, their energies stay the same (i.e. no energy is transferred) if the systems' initial temperatures are equal. Analogously, if two separate systems with volumes $ V_{1} $ and $ V_{2} $ are placed in contact, their volumes with stay the same if the systems' initial pressures are equal. The argument is analogous, treating energy as fixed and using conservation of total volume $ V_{1} + V_{2} = V_{tot} $, considering the extreme volume $ V_{max} $ instead of the extreme energy $ E_{max} $ at which entropy is maximized.
	
	\item Note that physical meaning of pressure is not directly tied to entropy as closely as temperature. Roughly speaking, the partial derivative $ \partial S $ in the definition of pressure cancels with the proportionality of $ T $ to $ \frac{1}{\partial S} $. 
	
	In more mathematically precise language, for a system with entropy $ S(E, V) $ that undergoes a small change in energy and volume, the corresponding change in entropy is
	\begin{equation*}
		\diff S = \pdv{S}{E}\diff E + \pdv{S}{V}\diff V
	\end{equation*}
	using the definitions of temperature $ \frac{1}{T} = \pdv{S}{E} $ and pressure $ p = T\pdv{S}{V} $, the expression becomes
	\begin{equation*}
		\diff E = T\diff S - p \diff V
	\end{equation*}
	
	\item The relationship $ \diff E = T\diff S - p \diff V $ is important. It governs the relationship between a system's energy, temperature, etc... in any processes involving changes in these quantities.
	
	How to interpret? The term $ \diff E $ represents the change in the system's energy. 
	
	The term $ p \diff V $ is the work done \textit{on} the system. The minus sign is just a way of saying that the system's energy increases when work is done on the system. To see this, note that when work is done on the system, its volume decreases, so $ \diff V < 0 $. The minus sign ensures that $ \diff E > 0 $ (i.e. energy increases) when work is done on the system and $ \diff V < 0 $. 
	
	The term $ T \diff S $ also represents an energy. Without proof (which will come later), it is the \textit{heat} the system absorbs from its surroundings. 
	
	The equation represents a form of conservation of energy for a thermodynamic system and is called the \textit{first law of thermodynamics}.

\end{itemize}

\subsubsection{Heat Capacity at Constant Volume and Pressure}
\begin{itemize}
	\item Recall the original definition of heat capacity $ C = \pdv{E}{T} \equiv \pdv{Q}{T} $ where energy $ E $ should really be heat, which we avoided using earlier because we hadn't defined it. The introduction of new quantities (e.g. volume and pressure) gives rise to different heat capacities, depending on which of the system's variables (e.g. volume and pressure) is constant. 
	
	\item Both heat capacity at constant volume and heat capacity at constant pressure are commonly used. They are defined, respectively, as
	\begin{equation*}
		C_{V} = \pdv{Q}{T} \bigg |_{V} \qquad \text{and} \qquad C_{p} = \pdv{Q}{T} \bigg |_{p}
	\end{equation*}
	where $ Q $ is the energy added to the system as heat.
	
	\item When a system's volume $ V $ is constant, $ \diff V = 0 $ and the first law of thermodynamics reads $ \diff E = T \diff S $. Differentiating with respect to temperature at constant volume produces
	\begin{equation*}
		\pdv{E}{T}\bigg |_{V} \equiv C_{V} = T \pdv{S}{T}\bigg |_{V}
	\end{equation*}
	Analogously, heat capacity at constant pressure can be expressed as
	\begin{equation*}
		C_{p} = T \pdv{S}{T}\bigg |_{p}
	\end{equation*}
	
	These expressions show how heat capacity measures a system's ability to absorb energy as heat $ T \diff S $ (as opposed to other ways, like work).
	
\end{itemize}

\subsection{The Canonical Ensemble}
\begin{itemize}
	\item Recall that the microcanonical ensemble describes systems with fixed energy $ E $, from which we can deduce the system's equilibrium temperature $ T $. 
	
	It is often more convenient (and physically applicable) to describe a system at fixed temperature $ T $, since in practice, a system's energy constantly fluctuates as it interacts with its environment. The \textit{canonical ensemble} describes a system at fixed temperature $ T $, from which it is possible to deduce and average (equilibrium) energy.
	
	\item We model the canonical ensemble, we consider the system of interest in contact with a second system called a \textit{heat reservoir} at an equilibrium temperature $ T $. The reservoir has very large energy compared to the system of interest, so any flow of energy from the system to the reservoir has no appreciable effect on the reservoir's energy or temperature. The idea is that the reservoir has such large energy compared to the primary system that energy flow between system and reservoir has no effect on the reservoir.
	
	\item How are the system's energy levels populated? We label the system's states as $ \ket{n} $ and assign each of the primary system's states the energy $ E_{n} $. The number of microstates of the combined system and reservoir is given by the sum over all of the primary system's states, since the reservoir is assumed to be in a constant-energy state.
	\begin{equation*}
		\Omega(E_{tot}) = \sum_{n}\Omega_{R}(E_{tot} - E_{n}) = \sum_{n} \exp(\frac{S_{R}(E_{tot}- E_{n})}{k_{B}})
	\end{equation*}
	Note that the sum is over all of $ S $'s states and \textit{not} all of $ S $'s energy levels. If we summed over energy levels we would have to include a degeneracy factor $ \Omega_{S}(E_{n}) $ since each energy $ E_{n} $ occurs many times because of the degeneracy of states with energy $ E_{n} $.
	
	\item Because of the reservoir's large energy, $ E_{n} \ll E_{tot}$. A first-order differential approximation of $ S_{R} $ for small deviations $ E_{n} $ from $ E_{tot} $ is
	\begin{equation*}
		S_{R}(E_{tot} - E_{n}) \approx S_{R}(E_{tot}) - E_{n}\pdv{S_{R}}{E_{tot}}
	\end{equation*}
	which is analogous to the approximation $ f(x + \delta x) \approx f(x) + \delta x \pdv{f}{x} $. The number of microstates becomes
	\begin{equation*}
		\Omega(E_{tot}) \approx \sum_{n} \exp(\frac{S_{R}(E_{tot}) }{k_{B}} - \frac{E_{n}}{k_{B}}\pdv{S_{R}}{E_{tot}}) = \exp(\frac{S_{R}(E_{tot})}{k_{B}}) \sum_{n} \exp(- \frac{E_{n}}{k_{B}}\pdv{S_{R}}{E_{tot}})
	\end{equation*}
	By introducing temperature $ \frac{1}{T} = \pdv{S_{R}}{E_{tot}} $, the expression becomes
	\begin{equation*}
		\Omega(E_{tot}) = e^{S_{R}(E_{tot})/k_{B}} \sum_{n} e^{-E_{n}/k_{B}T}
	\end{equation*}
		
	\item All accessible energy states of the combined primary system and reservoir are equally likely (the fundamental principle of statistical mechanics), so all $ \Omega(E_{tot}) $ are equally likely. The number of single particle states for which the system occurs in a given microstate $ \ket{n} $ is
	\begin{equation*}
		\Omega_{n}(E_{tot}) = e^{S_{R}/k_{B}} e^{-E_{n}/k_{B}T}
	\end{equation*}
	The probability the system occurs in the state $ \ket{n} $ is the ratio of this value to the total number of states
	\begin{equation*}
		p(n) = \frac{\Omega_{n}}{\Omega} = \frac{e^{S_{R}/k_{B}} e^{-E_{n}/k_{B}T}}{e^{S_{R}(E_{tot})/k_{B}} \sum_{m} e^{-E_{m}/k_{B}T}} = \frac{ e^{-E_{n}/k_{B}T}}{\sum_{m} e^{-E_{m}/k_{B}T}}
	\end{equation*}
	This distribution is called the \textit{Boltzmann distribution} or the \textit{canonical ensemble}. 
	
	\item Notice that the term $ S_{R} $ corresponding to the details of the heat reservoir cancel and do not occur in the canonical ensemble. All that remains is the reservoir's temperature $ T $.
	
	Because of the exponential proportionality $ p(n) \propto  e^{-E_{n}/k_{B}T} $, states with $ E_{n} \gg k_{B}T $ are highly unlikely to be occupied, while states with $ E_{n} \leq k_{B}T $ have a good change of being occupied. As $ T \to 0 $, $ p(n) \to 0 $ for $ E_{n} \neq 0 $ and the system is forced into its ground state with energy $ E_{0} = 0$.
	
\end{itemize}

\subsubsection{Notation and the Partition Function}
\begin{itemize}
	\item Some shorthand for notation that will come up over and over again. The inverse factor of temperature is conventionally denoted by
	\begin{equation*}
		\beta = \frac{1}{k_{B}T}
	\end{equation*}
	and is sometimes called the thermodynamic beta.
	
	\item The normalization factor in the denominator of the Boltzmann probability distribution is denoted by
	\begin{equation*}
		Z = \sum_{n} e^{-\beta E_{n}}
	\end{equation*}
	In terms of $ Z $, the probability to find a canonical system in the state $ \ket{n} $ is
	\begin{equation*}
		p(n) = \frac{e^{-\beta E_{n}}}{Z}
	\end{equation*}

	\item The term $ Z $ is called the \textit{partition function}. It counts the total number of microstates $ \Omega $ in a system and has important applications throughout statistical mechanics. To begin, note that the partition function is multiplicative for independent systems that don't interact with each other, e.g. $ Z_{tot} = Z_{1}Z_{2} $.
	
	To probe this, consider two systems with energies $ E^{(1)} $ and $ E^{(2)} $ and total energy $ E_{tot} =  E^{(1)} + E^{(2)} $. The total partition function is 
	\begin{align*}
		Z_{tot} &= \sum_{j} e^{-\beta E^{(tot)}_{j}} = \sum_{n, m}e^{-\beta (E^{(1)}_{n} + E^{(2)}_{m} )} = \sum_{n, m}e^{-\beta E^{(1)}_{n}}e^{-\beta E^{(2)}_{m}}\\
		&=\sum_{n}e^{-\beta E^{(1)}_{n}} \sum_{m}e^{-\beta E^{(2)}_{m}} = Z_{1}Z_{2}
	\end{align*}
\end{itemize}

\subsubsection{Average Energy and Energy Fluctuations in the Canonical Ensemble}
\begin{itemize}
	\item Recall that in general the expectation/average value $ \langle \hat{\mathcal{O}}\rangle $ for the discrete probability distribution $ p(n) $ is $  \langle \hat{\mathcal{O}}\rangle = \sum_{n}p(n)\mathcal{O}_{n} $ where $ \mathcal{O}_{n} $ are the operator's eigenvalues. 
	
	Applied to energy and the canonical ensemble, we see the average energy $ \expval{E} $ of a system in the canonical ensemble is
	\begin{equation*}
		\expval{E} = \sum_{n}p(n)E_{n} = \sum_{n} \frac{E_{n}e^{-\beta E_{n}}}{Z}
	\end{equation*}
	The average energy can be neatly written in terms of the partition function as
	\begin{equation*}
		\expval{E} = - \pdv{}{\beta} \ln Z
	\end{equation*}
	We verify this with a direct computation of the partial derivative
	\begin{equation*}
		- \pdv{}{\beta} \ln Z = - \pdv{}{\beta} \ln(\sum_{n}e^{-\beta E_{n}}) = - \frac{\sum_{n}-E_{n}e^{-\beta E_{n}}}{\sum_{n}e^{-\beta E_{n}}} = + \frac{\sum_{n}E_{n}e^{-\beta E_{n}}}{Z} = \expval{E}
	\end{equation*}
	
	\item The spread $ \Delta E^{2} $ of energy about the mean value $ \expval{E} $, which represents energy fluctuations in the canonical ensemble, is precisely the variance
	\begin{equation*}
		\Delta E^{2} = \expval{\left(E - \expval{E}\right)^{2}} = \langle E^{2}\rangle - \expval{E}^{2}
	\end{equation*}
	The energy variance can be neatly expressed both in terms of the partition function and average energy as
	\begin{equation*}
		\Delta E^{2} = \pdv[2]{}{\beta}\ln Z = -\pdv{\expval{E}}{\beta}
	\end{equation*}
	This is verified with a straightforward if rather tedious application of the quotient rule:
	\begin{align*}
		\Delta E^{2} &= \pdv{}{\beta}\left[\pdv{}{\beta} \ln Z\right] = \pdv{}{\beta}\left[- \frac{\sum_{n}-E_{n}e^{-\beta E_{n}}}{\sum_{n}e^{\beta E_{n}}}\right]\\
		&= \frac{\sum_{n}E_{n}^{2}e^{-\beta E_{n}}\sum_{n}e^{-\beta E_{n}} - \left(\sum_{n}E_{n}e^{-\beta E_{n}}\right)^{2}}{\left(\sum_{n}e^{-\beta E_{n}}\right)^{2}}\\
		&=\frac{\sum_{n}E_{n}^{2}e^{-\beta E_{n}}}{Z} - \left(\frac{\sum_{n}E_{n}e^{-\beta E_{n}}}{Z}\right)^{2} =  \expval{E^{2}} - \expval{E}^{2}
	\end{align*}
	where the last line follows from the general identity $  \langle \hat{\mathcal{O}}\rangle = \sum_{n}p(n)\mathcal{O}_{n} $.
	
	\item Recall the definition of heat capacity at constant volume for the microcanonical ensemble $ C_{V} = \pdv{E}{T}\big |_{V} $. For the canonical ensemble, where the energy is not fixed, the appropriate definition is
	\begin{equation*}
		C_{V} = \pdv{\expval{E}}{T}\bigg |_{V}
	\end{equation*} 
	The energy variance $ \Delta E^{2} $ can also be expressed in terms of heat capacity $ C_{V} $ as
	\begin{equation*}
		\Delta E^{2} = k_{B}T^{2} C_{V}
	\end{equation*}
	This expression comes from the identity $ \Delta E^{2} = -\pdv{}{\beta} \expval{E} $ and changing from differentiation with respect to $ \beta $ to differentiation with respect to $ T $, i.e. $ \pdv{}{\beta} = - k_{B}T^{2} \pdv{}{T} $.
	\begin{equation*}
		\Delta E^{2} = -\pdv{}{\beta} \expval{E} = + k_{B}T^{2} \pdv{\expval{E} }{T} = k_{B}T^{2}C_{V}
	\end{equation*}
	
	\item The relationship $ \Delta E^{2} = k_{B}T^{2} C_{V} $ is important. How to interpret? $ \Delta E^{2} $ represents probabilistic fluctuations of the system's energy about the mean value $ \expval{E} $ while $ C_{V} $ represents the system's ability to absorb energy.
	
	If $ C_{V} $ is large, the system can absorb relatively large amounts of energy without appreciable change in temperature, which allows relatively large energy fluctuations while maintaining a steady temperature, as required in the canonical ensemble. 
	
	This equation hints that energy fluctuations are related to a system's ability to absorb and dissipate heat, described more generally in the so-called \textit{fluctuation-dissipation theorem}.
	
	\item Secondly, because $ C_{V} \sim N $, the equation $ \Delta E^{2} = k_{B}T^{2} C_{V} $ shows that $ \Delta E^{2} \sim N $, implying $ \Delta E \sim \sqrt{N} $. Because the system's energy typically grows as $ E \sim N $, the \textit{relative size} of the fluctuations scales as
	\begin{equation*}
		\frac{\Delta E}{E} \sim \frac{\sqrt{N}}{N} = \frac{1}{\sqrt{N}}
	\end{equation*}
	In the limit of many particles $ N \to \infty $, the size of the fluctuations relative to the system's total energy is negligible, i.e. $ \frac{\Delta E}{E} \to 0 $. The limit $ N \to \infty $ is called the \textit{thermodynamic limit}.
	
	In the thermodynamic limit, as $ \frac{\Delta E}{E} \to 0 $ we have $ E \to \expval{E} $, and the system's energy becomes arbitrarily close to the mean value $ \expval{E} $. What does this mean? \textit{In the thermodynamic limit of large $ N $, the canonical ensemble behaves as a microcanonical ensemble with fixed energy $ \expval{E} $. }
	
	Because in practice $ N $ is at least $ \sim 10^{23} \approx \infty$, we can consider most systems well within the thermodynamic limit. For this reason, we can safely approximate a canonical ensemble's energy $ E $ with the average energy $ \expval{E} $.

\end{itemize}

\subsubsection{Example: The Two State System with the Canonical Ensemble}		
\begin{itemize}
	\item For a two state system with energy levels $ 0 $ and $ \epsilon $, the sum over $ n $ in the partition function is easily evaluated. For a single particle, $ Z = Z_{1} $ is
	\begin{equation*}
		Z_{1} = \sum_{n=0}^{1} e^{-\beta E_{n}} = e^{0} + e^{-\beta \epsilon} = 1 + e^{-\beta \epsilon} = 2e^{-\beta \epsilon/2} \cosh(\beta \epsilon /2 )
	\end{equation*}
	
	\item For a two-state system of $ N $ (non-interacting) particles, (recalling that the partition function is multiplicative), the partition function $ Z_{tot} $ is
	\begin{equation*}
		Z_{tot} = \prod_{i=1}^{N}Z_{1} = Z_{1}^{N} = 2e^{-N\beta \epsilon/2} \cosh^{N}(\beta\epsilon/2)
	\end{equation*}
	
	\item The average energy $ \expval{E} $, found using the logarithm identity $ ln (AB) = \ln A + \ln B $ and the exponential definition of $ \cosh $ and $ \tanh $, is
	\begin{equation*}
		\expval{E} = - \pdv{}{\beta} \ln Z = \frac{N\epsilon}{2} \left (1 - \tanh(\frac{\beta \epsilon}{2})\right )
	\end{equation*}
	This answer is algebraically equivalent to the earlier result $ E = \frac{N\epsilon}{e^{\beta \epsilon} + 1} $ found with the microcanonical ensemble.
	
	\item The system's heat capacity at constant volume is
	\begin{equation*}
		C_{V} = \pdv{\expval{E}}{T} = \frac{-1}{k_{B}T^{2}}\pdv{\expval{E}}{\beta} = \frac{N\epsilon^{2}}{4k_{B}T^{2}} \sech(\frac{\beta \epsilon}{2})
	\end{equation*}
	which is algebraically equivalent to the earlier result	$ C = \frac{N \epsilon^{2}}{k_{B}T^{2}} \frac{e^{\epsilon/k_{B}T}}{(e^{\epsilon/k_{B}T} + 1)^{2}}  $.
	
\end{itemize}

\subsubsection{Entropy in the Canonical Ensemble}
\begin{itemize}
	\item Recall that entropy was defined in the microcanonical ensemble in terms of the number of states with fixed energy. For the canonical ensemble, which has a probability distribution over states with different energies, we have to be more resourceful in our definition. 
	
	\item We consider the primary system and heat reservoir together and use a trick. Instead of one copy of the primary system, assume we have a large number $ W $ of identical copies, with each system occurring in a sate $ \ket{n} $. If $ W $ is very large, the number of primary systems in a state $ \ket{n} $ is $ W p(n) $.
	
	To determine the entropy in the canonical ensemble, we treat the collection of $ W $ primary systems as occurring in a microcanonical ensemble to which we can apply the familiar definition $ S = k_{B} \ln \Omega$. 
	
	\item From combinatorics, number of ways of putting $ W p(n) $ systems into the state $ \ket{n} $ for each state $ \ket{n} $ is
	\begin{equation*}
		\Omega = \frac{W!}{\prod_{n}(p(n)W)!}
	\end{equation*}
	so the entropy for the $ W $ primary systems is
	\begin{align*}
		S_{W} &= k_{B} \ln \Omega = k_{B}\ln(\frac{W!}{\prod_{n}(p(n)W)!})=k_{B}\ln W! - k_{B}\ln  \big[\textstyle \prod_{n}(p(n)W)! \big]\\
		&=k_{B}\ln W! - k_{B} \sum_{n} \ln\big[(p(n)W)!\big]
	\end{align*}
	For large $ W $, we can use Stirling's formula $ \ln N! \approx N \ln N - N $ to get
	\begin{equation*}
		S_{W} = - k_{B} W \sum_{n}p(n)\ln p(n)
	\end{equation*}
	
	\item \textit{Derivation:} I'll divide the $ k_{B} $ to remove unnecessary clutter. 
	\begin{itemize}
		\item Apply Stirling's formula to $ \frac{S_{W}}{k_{B}} = \ln W! - \sum_{n} \ln\big[(p(n)W)!\big] $ to get
		\begin{equation*}
			\frac{S_{W}}{k_{B}} = W \ln W - W - \sum_{n}\left[p(n)W \ln (p(n)W) -p(n)W \right]
		\end{equation*}
	
		\item The key is simplifying the sum, which we'll call $ \mathcal{S} =  \sum_{n}[\ldots]$. First, factor into
		\begin{equation*}
			\mathcal{S} \equiv \sum_{n}\left[p(n)W \ln (p(n)W) -p(n)W \right] = \sum_{n} p(n)W \ln (p(n)W) - \sum_{n} p(n)W 
		\end{equation*}
		
		\item Factor out the constant term $ W $ and apply $ \sum_{n}p(n) = 1 $ to get
		\begin{equation*}
			\mathcal{S} = W \sum_{n} p(n) \ln (p(n)W) - W \sum_{n} p(n) =W \sum_{n} p(n) \ln (p(n)W) - W 
		\end{equation*}
		
		\item Split up the remaining sum using $ \ln (p(n) W) = \ln p(n) + \ln W $ to get
		\begin{align*}
			\mathcal{S} &= W \sum_{n} p(n)(\ln p(n) + \ln W) - W \\
			&= W \sum_{n} p(n) \ln p(n) + W \sum_{n} p(n)\ln W - W\\
			&= W \sum_{n} p(n) \ln p(n) + W \ln W \sum_{n} p(n) - W 
		\end{align*}
		
		\item Finally, apply $ \sum_{n}p(n) = 1 $ to get
		\begin{equation*}
			\mathcal{S} = W \sum_{n} p(n) \ln p(n) + W\ln W - W 
		\end{equation*}	
		
		\item The hard work is done. Returning to $ \frac{S_{W}}{k_{B}} $, we have
		\begin{align*}
			\frac{S_{W}}{k_{B}} &= W \ln W - W - \mathcal{S} \\
			&= W \ln W - W - \left(W \sum_{n} p(n) \ln p(n) + W\ln W - W \right) \\
			&= - W \sum_{n} p(n) \ln p(n)
		\end{align*}
		from which follows $ S_{W} = - k_{B} W \sum_{n}p(n)\ln p(n) $
	\end{itemize}

	\item The expression $ S_{W} = - k_{B} W \sum_{n}p(n)\ln p(n) $ is the entropy for all $ W $ copies of the primary system. Because entropy additive, we get the entropy $ S $ for a single system by dividing by $ W $, giving
	\begin{equation*}
		S = \frac{S_{W}}{W} = - k_{B} \sum_{n}p(n)\ln p(n) 
	\end{equation*}
	This powerful formula is called the \textit{Gibbs entropy formula} and applies to any system in which the energy states are distributed as $ p(n) $.
	
	\item Note that for the microcanonical distribution $ p(n) = \Omega(E) $, the Gibbs formula recovers the formula $ S(E) = k_{B} \ln \Omega (E) $. Plugging in $ p(n) = \Omega(E) $ gives
	\begin{equation*}
		S = - k_{B} \sum_{n}p(n)\ln p(n) = - k_{B} \sum_{n} \frac{1}{\Omega(E)} \ln \frac{1}{\Omega(E)}
	\end{equation*}
	Factor the constant terms $  \frac{1}{\Omega(E)} \ln \frac{1}{\Omega(E)} $ from the sum and apply $ \sum_{n} 1 = \Omega(E) $ to get
	\begin{equation*}
		S = - \frac{k_{B}}{\Omega(E)} \ln \frac{1}{\Omega(E)} \sum_{n} 1 = -\left(\ln \frac{1}{\Omega(E)}\right) \frac{k_{B}}{\Omega(E)} \Omega(E) = -k_{B} \ln \frac{1}{\Omega(E)}
	\end{equation*}
	Finally, apply the logarithm identity $ p \ln x = \ln x^{p} $ with $ p = -1 $ to get
	\begin{equation*}
		S = -k_{B} \ln \frac{1}{\Omega(E)} = k_{B} \ln \left(\frac{1}{\Omega(E)}\right)^{-1} = k_{B} \ln \Omega (E)
	\end{equation*}
	
	\item Returning to the canonical ensemble with $ p(n) = \frac{e^{-\beta E_{n}}}{Z} $, the entropy is
	\begin{equation*}
		S = \frac{k_{B}\beta}{Z}\sum_{n} E_{n}e^{-\beta E_{n}} + k_{B}\ln Z
	\end{equation*}
	\textit{Derivation:} Apply basic logarithm properties and $ \sum_{n} \frac{e^{-\beta E_{n}}}{Z} \equiv \sum_{n} p(n) = 1 $ to get
	\begin{align*}
		S &= - \frac{k_{B}}{Z} \sum_{n}e^{-\beta E_{n}}\ln \frac{e^{-\beta E_{n}}}{Z}\\
		&= - \frac{k_{B}}{Z} \sum_{n}e^{-\beta E_{n}}  \left (\ln e^{-\beta E_{n}} -\ln Z\right )\\
		&= \frac{k_{B}}{Z} \sum_{n} \beta E_{n} e^{-\beta E_{n}} +  \frac{k_{B}}{Z} \ln Z  \sum_{n}  e^{-\beta E_{n}}\\
		&= \frac{k_{B}\beta}{Z} \sum_{n} E_{n} e^{-\beta E_{n}} + k_{B} \ln Z  \sum_{n}  \frac{e^{-\beta E_{n}}}{Z}\\
		&=\frac{k_{B}\beta}{Z}\sum_{n} E_{n}e^{-\beta E_{n}} + k_{B}\ln Z
	\end{align*}
	
	\item Entropy in the canonical ensemble can be neatly expressed in terms of the partition function $ Z $ as
	\begin{equation*}
		S = k_{B} \pdv{}{T}[T \ln Z]
	\end{equation*}
	Note that the a canonical system's entropy is determined entirely by its temperature $ T $.
	
\end{itemize}

\subsubsection{Energy States, Energy Levels and the Partition Function}
\begin{itemize}
	\item The partition function $ Z = \sum_{n}e^{-\beta E_{n}} $ is a sum over all of a system's energy states $ \ket{n} $. 
	
	The partition function can be written as a sum over a system's \textit{energy levels} $ E_{i} $ by including a degeneracy factor $ \Omega(E_{i}) $ to account for multiple states with the same energy level $ E_{i} $. 
	\begin{equation*}
		Z = \sum_{\{E_{i}\}} \Omega(E_{i})e^{-\beta E_{i}}
	\end{equation*}
	
	\item The degeneracy factor $ \Omega(E) $ is typically a rapidly increasing function of $ E $ (e.g. from the Boltzmann formula $ \Omega(E) = \exp \frac{S(E)}{k_{B}} $) while the term $ e^{-\beta E} $ is a rapidly decreasing function of $ E $. 
	
	In both cases, the exponent is proportional to the number of particles $ N \sim 10^{23} $, which is itself exponentially large. Analogously to the situation with entropy and the second law, the sum of exponentials of exponentials in the equation $ Z = \sum_{\{E_{i}\}} \Omega(E_{i})e^{-\beta E_{i}} $ is completely dominated by the term with the largest energy $ E_{max} $, defined implicitly by:
	\begin{equation*}
		\pdv{}{E}\left[\Omega(E)e^{-\beta E}\right]_{E = E_{max}} = 0
	\end{equation*}
	The partition function is well approximated by the largest term
	\begin{equation*}
		Z \approx \Omega(E_{max})e^{-\beta E_{max}}
	\end{equation*}
	
	\item In this (very accurate) approximation, the system's most likely energy $ \expval{E} $ and the maximum energy coincide : $ \expval{E} = E_{max} $, which is easily verified by
	\begin{equation*}
		\expval{E} = -\pdv{}{\beta}\ln Z = - \pdv{}{\beta}\ln( \Omega(E_{max})e^{-\beta E_{max}}) = \frac{\Omega(E_{max})E_{max}e^{-\beta E_{max}}}{\Omega(E_{max})e^{-\beta E_{max}}} = E_{max}
	\end{equation*}
	
	\item Finally, making use of the result $ \expval{E} = E_{max} $, we can derive a Boltzmann-style entropy formula for the canonical ensemble
	\begin{equation*}
		S = k_{B}\ln \Omega(E_{max})
	\end{equation*}
	This can be derived from $ S = k_{B}\pdv{}{T} (T \ln Z) $ by recalling $ \beta = \frac{1}{k_{B}T} $
	\begin{align*}
		S &= k_{B}\pdv{}{T} (T \ln Z) = k_{B}\pdv{}{T} \left[T \ln( \Omega(E_{max})e^{-\beta E_{max}}) \right]\\
		&=k_{B}\pdv{}{T} \left[T \ln \Omega(E_{max})\right] + k_{B}\pdv{}{T}\left[T \ln e^{-E_{max}/k_{B}T}\right]\\
		&= k_{B} \ln \Omega(E_{max}) + k_{B} \pdv{}{\beta} \left[T \frac{-E_{max}}{k_{B}T}\right]\\
		&=k_{B} \ln \Omega(E_{max}) + k_{B} \pdv{}{\beta} \left[ \frac{-E_{max}}{k_{B}}\right]\\
		&=k_{B} \ln \Omega(E_{max}) + k_{B} \cdot 0 = k_{B} \ln \Omega(E_{max})
	\end{align*}
	
\end{itemize}

\subsubsection{Cool Interpretation of the Canonical and Microcanonical Ensembles}
The Gibbs entropy formula can be used to define both the canonical and microcanonical ensembles.
\begin{itemize}
	\item The microcanonical ensemble can be viewed as the probability distribution maximizing entropy $ S = - k_{B} \sum_{n} p(n) \ln p(n) $ subject to the constraint that energy $ E $ is fixed, and of course conservation of probability $ \sum_{n}p(n) = 1 $.
	
	\item For the microcanonical ensemble, we maximize $ S $ while satisfying the constraints by introducing the Lagrange multiplier $ \alpha $ and maximizing $ S + \alpha k_{B}(\sum_{n}p(n) -1) $ by requiring
	\begin{equation*}
		\pdv{}{p(n)} \left[-\sum_{n} p(n) \ln p(n) + \alpha \sum_{n}p(n) - \alpha \right] \equiv 0
	\end{equation*}
	Evaluating the derivative and placing all terms under one sum gives
	\begin{equation*}
		- \sum_{n} \left(\ln p(n) + 1 - \alpha\right) = 0
	\end{equation*}
	which is satisfied only if $ \ln p(n) + 1 - \alpha = 0 $, leading to the condition $ p(n) = e^{\alpha - 1} $, i.e. \textit{the probability distribution is constant in the microcanonical ensemble}.
	
	\item The canonical ensemble can be viewed as the probability distribution maximizing entropy $ S = - k_{B} \sum_{n} p(n) \ln p(n) $ subject to the constraint that $ \sum_{n}p(n) = 1 $ and that \textit{average} energy $ \expval{E} $ is fixed, i.e. $ \expval{E} = \sum_{n}p(n)E_{n} $. 
	
	\item In other words, both ensembles can be viewed as probability distributions maximizing entropy as defined by the Gibbs formula. The only difference is the imposed constraints under which entropy is maximized.
	
\end{itemize}

\subsubsection{Free Energy}
\begin{itemize}
	\item An important quantity in the canonical ensemble is the \textit{free energy}
	\begin{equation*}
		F = \expval{E} - TS
	\end{equation*}
 	In fact, there are a few quantities across physics referred to as free energies. In such cases, the quantity $ F = \expval{E} - TS$ is called the \textit{Helmholtz} free energy to avoid ambiguity. In this course, free energy refers to Helmholtz free energy unless explicitly stated otherwise.
	
	\item In e.g. Newtonian mechanics, we are used to systems doing everything they can to minimize their energy. However, in statistical mechanics with its large number of particles, minimizing energy and maximizing energy play a sort of balancing act. In practice, free energy captures the competition between minimizing energy and maximizing entropy in a system at \textit{fixed temperature}. 
	
	Take the two-state system as an example. In practice, such a system does not reside in the lowest possible energy state with all spins down (even though this would minimizes energy) because such a state minimizes entropy. Instead, the system is much more likely found in a state half spins up and half spins down, which maximizes entropy at the expense of higher energy.
	
	\item The total differential $ \diff F $ of free energy is
	\begin{equation*}
		\diff F = \diff \expval{E} - T \diff S - S \diff T
	\end{equation*}
	by inserting the first law of thermodynamics $ \diff E = T\diff S - p \diff V $ for $ \diff \expval{E} $ we get
	\begin{equation*}
		\diff F = - S \diff T - p \diff V
	\end{equation*}
	with the differentials $ \diff T $ and $ \diff V $ portray $ F $ as a function of temperature and volume $ F = F(T, V) $. 
	
	\item Mathematically, free energy is a Legendre transform of energy $ E $. Using $ \diff F = - S \diff T - p \diff V $, entropy and pressure are recovered with 
	\begin{equation*}
		S = -\pdv{F}{T}\bigg |_{V} \qquad \text{and} \qquad p = - \pdv{F}{V}\bigg |_{T}
	\end{equation*}
	
	\item Free energy is related to the partition function $ Z $ by
	\begin{equation*}
		F = - k_{B} T \ln Z
	\end{equation*}
	The expression follows directly from the partition function definitions of $ \expval{E} $ and $ Z $:
	\begin{align*}
		F &= \expval{E} - TS = - \pdv{}{\beta} \ln Z - k_{B} T \pdv{}{T}[T \ln Z]\\
		&= + k_{B}T^{2}\pdv{}{T} \ln Z - k_{B}T^{2} \pdv{}{T} \ln Z - k_{B}T \ln Z\\
		&=- k_{B}T \ln Z
	\end{align*}
	
	\item I stress again that free energy is useful for describing systems at \textit{fixed temperature}. This will come up later in the thermodynamics section.
	
\end{itemize}



\subsection{Grand Canonical Ensemble and Chemical Potential}
So far, we have constrained our systems by fixing the energy (microcanonical ensemble) and temperature (canonical ensemble). There is another important conserved quantity that leads to another ensemble. This is the number of particles $ N $ in the system, which leads to the so-called \textit{grand canonical ensemble}.

\subsubsection{Chemical Potential}
\begin{itemize}	
	\item In the microcanonical and canonical ensembles, we consider states with a fixed value of $ N $. For example, the analysis of the two state system depends explicitly on the number of particles $ N $ being conserved. 
	
	\item To make the dependence of entropy on particle number (along with energy and volume) explicit, we write
	\begin{equation*}
		\Omega = \Omega(E, V, N) \qquad \text{and} \qquad S(E, V, N) = k_{B}\ln \Omega(E, V, N)
	\end{equation*}
	
	\item Recall how entropy gives rise to temperature $ T $ and pressure $ p $ via the partial derivatives
	\begin{equation*}
		\frac{1}{T} = \pdv{S}{E} \qquad \text{and} \qquad p = T \pdv{S}{V}
	\end{equation*}
	We now have another option: to differentiate $ S $ with respect to $ N $. The resulting quantity is called the system's \textit{chemical potential}
	\begin{equation*}
		\mu = - T\pdv{S}{N}
	\end{equation*}
	
	\item Analogously to how two systems that can exchange energy are in equilibrium when both systems have equal temperature $ T $ and how two systems that can exchange volume are in equilibrium when both systems have equal pressure $ p $, two systems that can exchange particles are in equilibrium when both systems have equal chemical potential $ \mu $. This condition is called \textit{chemical equilibrium}.
	
	\item How to interpret chemical potential? To do this, we generalize the first law of thermodynamics to include the possibility of a change in particle number. As in the simpler case $ S = S(E, V) $ we take the total derivative of $ S = S(E, V, N) $ and apply the definitions of $ T, p $ and $ \mu $.
	\begin{align*}
		\diff S &= \pdv{S}{E}\diff E + \pdv{S}{V}\diff V + \pdv{S}{N}\diff N = \frac{1}{T}\diff E + \frac{p}{T}\diff V - \frac{\mu}{T}\diff N
	\end{align*}
	Solving for $ \diff E $ leads to the generalized first law
	\begin{equation*}
		\diff E = T \diff S - p\diff V + \mu \diff N
	\end{equation*}
	The term $ \mu \diff N $ is the energy cost of changing the number of particles in the system. Thus the chemical potential $ \mu $ is measures the energy cost of adding an infinitesimal amount of particle $ \diff N $ to our system. Because for typical system $ N \sim 10^{23} $ is absolutely enormous, chemical potential can be safely viewed as the energy cost of adding adding one more particle to the system, since one is essentially infinitesimal in comparison to $ 10^{23} $.

	\item We defined have chemical potential as 
	\begin{equation*}
		\mu = -T \pdv{S}{N} \bigg|_{E, V}
	\end{equation*}
	while differentiating at constant $ E $ and $ V $. However, the generalized first law 
	\begin{equation*}
		\diff E = T \diff S - p\diff V + \mu \diff N
	\end{equation*}
	shows chemical potential could also be expressed $ \mu = \displaystyle{\pdv{E}{N}\Big|_{S, V}} $. We can reconcile these to expressions with a general formula for partial derivatives. For any three variables $ x, y, z $ with a single constraint between them,
	\begin{equation*}
		\pdv{x}{y} \bigg |_{z} \pdv{y}{z}\bigg |_{x} \pdv{z}{x}\bigg |_{y} = -1
	\end{equation*}
	Applying this formula to $ E, S $ and $ N $ with the constraint of fixed volume $ V $ shows
	\begin{equation*}
		\pdv{E}{N}\bigg |_{S, V} = - \pdv{E}{S}\bigg |_{N, V} \pdv{S}{N}\bigg |_{E, V} = - T \pdv{S}{N}\bigg |_{E, V} = \mu
	\end{equation*}
	
	\item Finally, if we work at constant temperature instead of constant energy, the best quantity to work with is the free energy $ F(T, V, N) = E - TS $. Recalling the generalized first law $ \diff E = T \diff S - p\diff V + \mu \diff N $, the total differential $ \diff F $ is
	\begin{equation*}
		\diff F = - S \diff T - p\diff V + \mu \diff N
	\end{equation*}
	From here we get another definition for chemical potential, namely
	\begin{equation*}
		\mu = \pdv{F}{N}\bigg|_{T, V}
	\end{equation*}
	
\end{itemize}

\subsubsection{The Grand Canonical Ensemble}
\begin{itemize}
	\item When transitioning from the microcanonical ensemble to the canonical ensemble, we relaxed the restriction that a system's energy must be constant. We imagined a primary system and an auxiliary reservoir with fixed temperature $ T $ and allowed the system and reservoir to exchange energy so that the primary system's energy could fluctuate about an average value $ \expval{E} $.
	
	\item We can use the reservoir idea with any other conserved quantity, for instance the number of particles $ N $ in the system, and allow both energy and particles to flow between the primary system and reservoir. In this case, we require the reservoir has fixed temperature $ T $ \textit{and} fixed chemical potential $ \mu $. The corresponding probability distribution for such a system is called the \textit{grand canonical ensemble}.
	
	\item The probability of finding a system in a state $ \ket{n} $ depends on both the state's energy $ E_{n} $ and particle number $ N_{n} $. The partition function for the grand canonical ensemble is
	\begin{equation*}
		\mathcal{Z}(T, \mu, V) = \sum_{n}e^{-\beta(E_{n} - \mu N_{n})}
	\end{equation*}
	Under the fundamental assumption that all states are equally likely, the probability for a system in the grand canonical ensemble to be in the state $ \ket{n} $ is
	\begin{equation*}
		p(n) = \frac{e^{-\beta(E_{n} - \mu N_{n})}}{\sum_{m}e^{-\beta(E_{m} - \mu N_{m})}} = \frac{e^{-\beta(E_{n} - \mu N_{n})}}{\mathcal{Z}}
	\end{equation*}
	
	\item As in the canonical ensemble, the majority quantities can be written in terms of the partition function. 
	
	\begin{itemize}
		\item The entropy of a system in the grand canonical ensemble is
		\begin{equation*}
			S = k_{B}\pdv{}{T}[T \ln \mathcal{Z}]
		\end{equation*}
		
		\item Average particle number $ \expval{N} $ is given by
		\begin{equation*}
			\expval{N} = \frac{1}{\beta} \pdv{}{\mu}\ln \mathcal{Z}
		\end{equation*}
		The variance in particle number $ \Delta N^{2} $ is given by
		\begin{equation*}
			\Delta N^{2} = \frac{1}{\beta^{2}}\pdv[2]{}{\mu} \ln \mathcal{Z} = \frac{1}{\beta}\pdv{\expval{N}}{\mu}
		\end{equation*}
		
		\item Notice the symmetry in the relationship between average energy and temperature in the canonical ensemble and the relationship between average particle and chemical potential in the grand canonical ensemble. 
		
		Like energy in the canonical ensemble, the relative fluctuations of particle number $ \frac{\Delta N}{\expval{N}} $ scale with the number of particles in the system as $ \sim \frac{N}{\sqrt{N}} = \frac{1}{\sqrt{N}}$. In the thermodynamic limit $ N \to \infty $, the fluctuations $ \Delta N $ grow arbitrarily small compared to $ \expval{N} $ and the number of particles in the system grows arbitrarily close the average value $ \expval{N} $. We can thus safely drop the average notation and refer to average particle number as $ N $.
		
		\item Finally, average energy in the grand canonical ensemble is determined by the relationship
		\begin{equation*}
			\expval{E} - \mu \expval{N} = - \pdv{}{\beta}\ln \mathcal{Z}
		\end{equation*}
	\end{itemize}
	
\end{itemize}

\subsubsection{Grand Canonical Potential}
\begin{itemize}
	\item The \textit{grand canonical potential} $ \Phi $ is defined as
	\begin{equation*}
		\Phi = F - \mu N
	\end{equation*}
	Mathematically, $ \Phi $ is a Legendre transform of free energy $ F $ from the variable $ N $ to $ \mu $. This is seen from the total differential 
	\begin{equation*}
		\diff \Phi = -S \diff T - p \diff V - N\diff \mu	
	\end{equation*}
	which shows that $ \Phi $ should be thought of as a function of temperature, volume and chemical potential, $ \Phi = \Phi(T, V, \mu) $.
	
	As an aside, the total differential $ \diff \Phi  $ is derived from
	\begin{align*}
		\diff \Phi &= \diff F - \mu \diff N - N \diff \mu = (\diff E - T \diff S - S \diff T) - \mu \diff N - N \diff \mu\\
		&=(T \diff S - p\diff V + \mu \diff N) - T \diff S - S \diff T - \mu \diff N - N \diff \mu\\
		&=-S \diff T - p \diff V - N\diff \mu
	\end{align*}
	
	\item Using the definitions of $ S $ and $ \expval{N} $ in terms of $ \mathcal{Z} $, we can write $ \Phi $ in terms of $ \mathcal{Z} $ as
	\begin{equation*}
		\Phi = - k_{B}T \ln \mathcal{Z}
	\end{equation*}
	
	\item Recall the interpretation $ \Phi = \Phi(T, V, \mu) $ where the grand canonical potential is a function of temperature, volume and chemical potential. Temperature and chemical potential are intensive quantities while volume is an extensive quantity.
	
	 $ \Phi $ is thus a function of a single extensive variable $ V $ and must scale as 
	 \begin{equation*}
	 	\Phi(T, \lambda V, \mu) = \lambda \Phi(T, V, \mu)
	 \end{equation*}
	 where $ \lambda $ is an arbitrary scalar value. To satisfy this scaling relationship, $ \Phi $ must be proportional to $ V $. 
	 
	 But from the total differential $ \diff \Phi = -S \diff T - p \diff V - N\diff \mu $, we see the proportionality of $ \Phi $ to $ V $ is precisely $ - p $. This means that $ \Phi $ can be expressed as
	 \begin{equation*}
	 	\Phi(T, V, \mu) = -p(T, \mu) V
	 \end{equation*}
	 The relationship between grand canonical potential and temperature is a very useful way to calculate the pressure of various systems.

\end{itemize}

\section{Classical Gases}
First, we introduce the classical partition function for a single particle. In general, we will analyze classical gases with the canonical ensemble. 
\begin{itemize}
	\item The Hamiltonian for a single classical particle of mass $ m $ moving in three dimensions in the potential $ V(\bm{r}) $ is
	\begin{equation*}
		H(\bm{r}) = \frac{\bm{p}^{2}}{2m} + V(\bm{r})
	\end{equation*}
	
	\item In the introduction to statistical mechanics, we defined the partition function as the sum over all of the system's quantum states. In classical mechanics we integrate over the system's phase space. The \textit{classical partition function} for a single particle is
	\begin{equation*}
		Z_{1} = \frac{1}{h^{3}}\int e^{-\beta H(\bm{p}, \bm{q})} \diff^{3}\bm{r}\diff^{3}\bm{p}
	\end{equation*}
	How to interpret the factor $ \frac{1}{h^{3}} $ before the integral? Practically, since Planck's constant $ h $ has dimensions (length $ \cross $ momentum), the factor ensures that $ Z $ is dimensionless, just like the partition functions in statistical mechanics. The best way to interpret the presence of $ h $, however, is a hint that the classical partition function is derived from quantum considerations. I am not showing the derivation here; I consider it beyond the scope of these notes. 
	
\end{itemize}

\subsection{Ideal Gases}
\begin{itemize}
	\item The standard model for a so-called \textit{ideal gas} is a system of $ N $ non-interacting particles contained in a volume $ V $. Here non-interacting there is no potential between the particles and any collisions are elastic. 
	
	If the particles have no internal structure and thus no rotational or vibrational degrees of freedom, the gas is called an \textit{monatomic ideal gas}. The Hamiltonian for a monatomic ideal gas particle is simply the kinetic energy
	\begin{equation*}
		H = \frac{\bm{p}^{2}}{2m}
	\end{equation*}
	
	\item The partition function for a monatomic ideal gas particle is
	\begin{equation*}
		Z_{1}(V, T) = \frac{1}{h^{3}} \int \exp(\frac{\beta }{2m}\bm{p}^{2}) \diff^{3}\bm{r}\diff^{3}\bm{p}
	\end{equation*}
	The integral over position is simply the volume of the box: $ \int \diff^{3}\bm{r} = V $. The integral over momentum factors by components into integrals over $ p_{x}, p_{y} $ and $ p_{z} $ of the form
	\begin{equation*}
		\int_{-\infty}^{\infty} \exp(\frac{\beta }{2m}p_{x,y,z}^{2})\diff p_{x,y,z}
	\end{equation*}
	This is a standard Gaussian-type integral with the general solution
	\begin{equation*}
		\int_{-\infty}^{\infty} e^{-ax^{2}} \diff x = \frac{\pi}{a}
	\end{equation*}
	Putting the position and momentum pieces together, the single-particle partition function is
	\begin{equation*}
		Z_{1} = V \frac{1}{(2\pi \hbar)^{3}} \left(\frac{2\pi m}{\beta}\right)^{3/2} = V \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3/2}
	\end{equation*}
	
	\item The combination of factors in the parentheses occurs often enough to warrant a special name, the \textit{thermal de Broglie wavelength} $ \lambda $
	\begin{equation*}
		\lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}}
	\end{equation*}
	In terms of $ \lambda $, the partition function for a monatomic ideal gas particle is
	\begin{equation*}
		Z_{1} = \frac{V}{\lambda^{3}}
	\end{equation*}
	$ \lambda $ has dimensions of length (as expected for a wavelength) so $ Z_{1} $ is dimensionless, as it should be. $ \lambda $ can be thought of as the average de Broglie wavelength of a particle at temperature $ T $, while the partition function counts how many of these thermal wavelengths can fit in the volume $ V $.
	
	\item $ Z_{1} $ is partition function for a single particle. Recall the partition function is multiplicative for non-interacting systems, so the natural candidate for the partition function for a $ N $-particle monatomic ideal gas (which is non-interacting by definition) is $ Z_{1}^{N} $. It turns out the correct expression has an extra normalization factor:
	\begin{equation*}
		Z(N, V, T) = \frac{1}{N!}\frac{V^{N}}{\lambda^{3N}} = \frac{V^{N}}{N!} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2}
	\end{equation*}
	The $ N! $ occurs because the gas particles are indistinguishable. This is discussed in more detail in a later section.
	
\end{itemize}

\subsubsection{Pressure and the Ideal Gas Law}
\begin{itemize}
	\item In the introduction to statistical mechanics, we could derive many of a system's useful macroscopic quantities from the partition function. The same formulas hold for the classical partition function.
	
	\item To derive pressure, we start with free energy $ F = - k_{B}T \ln Z $ and calculate pressure using the initially intimidating (but actually quite simple) expression
	\begin{align*}
		p &= -\pdv{F}{V} = \pdv{}{V}\left( k_{B}T \ln\left [\frac{V^{N}}{N!} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2}\right ]\right)  \\
		&= k_{B} T\pdv{}{V} N \ln V = \frac{k_{B}T N}{V}
	\end{align*}
	This equation is the familiar ideal gas law $ pV = k_{B}T N $, which you may also have seen in the form $ pV = nRT $ where $ R $ is the ideal gas constant and $ n $ is the number of moles of particles in the system.
	
	Equations linking a system's pressure, volume, temperature and certain other quantities are called \textit{equations of state}. We will meet many more in future sections.
	
	\item The ideal gas law is an excellent description of gases at low densities (so that interactions between particles are negligible). At higher densities, gases deviate from the behavior predicted by the ideal gas law.
\end{itemize}

\subsubsection{Average Energy, Equipartition of Energy and Heat Capacity}
\begin{itemize}
	\item We calculate the average energy of an ideal gas (using the large $ N $ approximation $ \expval{E} \approx E $) with the familiar equation
	\begin{equation*}
		E = - \pdv{}{\beta} \ln Z = - \pdv{}{\beta}  \ln\left [\frac{V^{N}}{N!} \left(\frac{m}{2\pi \hbar^{2} \beta}\right)^{3N/2}\right ] = \frac{3N}{2\beta} = \frac{3}{2}Nk_{B}T
	\end{equation*}
	Note that each particle contributes an energy $ E_{1} = \frac{3}{2}k_{B}T $ to the average energy.
	
	\item The is an important, more general relationship called \textit{equipartition of energy} behind the formula $ E_{1} = \frac{3}{2}k_{B}T $ when we generalize our analysis to $ D $ dimensions. Because $ Z $ is multiplicative, the partition function generalizes to
	\begin{equation*}
		Z_{D} = \frac{V^{N}}{N!} \left(\frac{m}{2\pi \hbar^{2} \beta}\right)^{DN/2}
	\end{equation*}
	The corresponding average energy and energy per particle are
	\begin{equation*}
		E = - \pdv{}{\beta} \ln Z_{D} = \frac{D}{2}Nk_{B}T \qquad \text{and} \qquad E_{1} = \frac{D}{2}k_{B}T
	\end{equation*}
	The lesson is that in $ D $ dimensions, each particle contributes energy $ E_{1} = \frac{D}{2}k_{B}T $ to the average energy.
	
	\item More generally, for $ D $ degrees of freedom, each particle in a non-interacting classical systems contributes energy $ \frac{D}{2}k_{B}T $ to the average energy. 
	
	Equipartition of energy states that the average energy of each degree of freedom in a non-interacting classical system at temperature $ T $ is $ \frac{1}{2} k_{B}T $. If follows that the average energy of a system of $ N $ particles in three dimensions is 
	\begin{equation*}
		E = 3N \cdot \frac{1}{2} k_{B}T = \frac{3}{2}Nk_{B}T
	\end{equation*}
	as predicted above for a three-dimensional monatomic ideal gas.
	
	\item Finally, the heat capacity of a monatomic ideal gas in three dimensions is simply
	\begin{equation*}
		C_{V} = \pdv{E}{T}\bigg|_{V} = \frac{3}{2}N k_{B}
	\end{equation*}
	and each particle contributes $ \frac{3}{2} k_{B} $ to the system's total heat capacity.
\end{itemize}

\subsubsection{Entropy and Gibbs' Paradox}
\begin{itemize}
	\item Recall that the partition function for a classical gas in the canonical ensemble is 
	\begin{equation*}
		 Z = \frac{V^{N}}{N!} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2}
	\end{equation*}
	and not the $V^{N} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2} $ without the factor $ N! $ obtained by raising $ Z_{1} = V \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3/2} $ to the $ N $th power. The $ N! $ is necessary because quantum particles are indistinguishable. Thus, swapping the position of two identical particles does not create a new microstate of the system, and the expression $ Z_{1}^{N} $ over-counts the system's states by $ N! $, the number of ways to permute $ N $ particles.
	
	\item Note that adding an arbitrary constant factor $ \alpha $ to $ Z $ does not change a system's pressure or energy, since the constant could vanish when differentiated $ \ln \alpha $ using the pressure and energy equations in terms of $ Z $. However, the term $ N! $ is relevant to entropy, expressed (in general) in terms of $ Z $ as:
	\begin{equation*}
		S = \pdv{}{T} (k_{B}T \ln Z)
	\end{equation*}
	Entropy includes the factor $ \ln Z $ without a derivative, so the constant factor $ N! $ remains in the equation. 
	
	\item By inserting the proper expression $ Z = \frac{V^{N}}{N!} \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3N/2} $ and applying Stirling's formula for logarithms of large factorials, the entropy of a classical monatomic ideal gas is
	\begin{equation*}
		S = N k_{B} \left[\ln(\frac{V}{N\lambda^{3}}) + \frac{5}{2}\right]
	\end{equation*}
	This expression is called the \textit{Sackur-Tetrode} equation for the entropy of an ideal gas. Note, however, that only entropy \textit{changes} are directly measurable for classical gases using the expression
	\begin{equation*}
		\Delta S = \int_{T_{1}}^{T_{2}}\frac{C(T)}{T}\diff T
	\end{equation*}
\end{itemize}

\subsubsection{Ideal Gas in the Grand Canonical Ensemble}
\begin{itemize}
	\item In the grand canonical ensemble, the gas is free to exchange both energy and particle with an outside reservoir. Our system of interest can be modeled as a fixed sub-volume inside a much larger gas (the reservoir). We are interested in the average energy and average number of particles inside the sub-volume.
	
	\item The grand partition function for the ideal gas is 
	\begin{equation*}
		\mathcal{Z}_{ideal}(\mu, V, T)  = \exp(\frac{V e^{\beta \mu}}{\lambda^{3}})
	\end{equation*}
	The derivation applies the statistical-mechanical definition of $ \mathcal{Z} $ and then recognizes the resulting form of the power series $ \sum_{n = 0}^{\infty}\frac{x^{n}}{n!} = e^{x} $
	\begin{align*}
		\mathcal{Z}_{ideal}(\mu, V, T) &= \sum_{n} e^{-\beta(E_{n} - \mu N_{n})} = \sum_{N=0}^{\infty}e^{\beta \mu N}Z_{ideal}(N, V, T) \\
		&=   \sum_{N = 0}^{\infty} e^{\beta \mu N} \frac{V^{N}}{N!\lambda^{3N}} = \sum_{N = 0}^{\infty} \frac{1}{N!}\left(\frac{V e^{\beta \mu}}{\lambda^{3}}\right)^{N} = \exp(\frac{V e^{\beta \mu}}{\lambda^{3}})
	\end{align*}
	
	\item From the partition function we can find the average number of particles in the system
	\begin{equation*}
		\expval{N} = \frac{1}{\beta}\pdv{}{\mu}\ln \mathcal{Z} = \frac{Ve^{\beta \mu}}{\lambda^{3}} \approx N 
	\end{equation*}
	Rearranging gives an expression for the chemical potential $ \mu $
	\begin{equation*}
		\mu = k_{B}T \ln(\frac{\lambda^{3}N}{V})
	\end{equation*}
	
	
	\item How to interpret this expression? If $ \lambda^{3} < \frac{V}{N}$ then chemical potential is negative. Recall $ \lambda $ can be thought of as the average de Broglie wavelength of each particle, while $ \frac{V}{N} $ is the average volume taken up by each particle. 
	
	For low-density ideal gas conditions, we must have $ \lambda^{3} \ll \frac{V}{N} $, otherwise the particles' de Broglie wavelength is comparable to inter-particle separation and the gas deviates from ideal behavior. 
	
	We are forced to conclude chemical potential is \textit{negative} for ideal gases (because $ \lambda^{3} < \frac{V}{N} $). 
	
	\item A negative chemical potential is strange at first glance, given our earlier interpretation of $ \mu $ as the energy cost of adding an extra particle to the system (which one might expect to be positive).  The explanation lies in the definition of chemical potential, 
	\begin{equation*}
		\mu = \pdv{E}{N} \bigg |_{S, V}
	\end{equation*}
	namely the requirement of differentiation \textit{at constant entropy.} Normally, by adding a particle to the system, we give the system more ways to share its energy, increasing its microstates and thus its entropy. If we insist on keeping entropy fixed according to the definition of chemical potential, we must reduce the system's energy when adding a particle to balance what would normally result in an increase in entropy, thus $ \mu $ is negative. Compensating for entropy increase with energy decrease is the reason $ \mu < 0 $ for an ideal gas.
	
	In situations with strong repulsive interactions between particles and in low-temperature fermion systems (discussed later) chemical potential is positive.
	
	\item An ideal gas's fluctuation (variance) of particle number in the GCE is, by definition
	\begin{equation*}
		\Delta N^{2} = \frac{1}{\beta^{2}} \pdv[2]{}{\mu}\ln \mathcal{Z} = \frac{Ve^{\beta \mu}}{\lambda^{3}} =  N
	\end{equation*}
	As expected, the relative size of particle number fluctuations $ \frac{\Delta N}{N} $ scales as $ \sim \frac{\sqrt{N}}{N} $, so $  \frac{\Delta N}{N} \to 0 $ in the thermodynamic limit $ N \to \infty $.
	
	\item Finally, we can easily derive the ideal gas $ pV = Nk_{B}T $ from the grand canonical potential $ \Phi = - k_{B}T \ln \mathcal{Z} $ using the convenient identity $ \Phi = - pV $. 
	\begin{equation*}
		pV = - \Phi = + k_{B}T \ln \mathcal{Z} = k_{B}T \frac{Ve^{\beta \mu}}{\lambda^{3}} =  N k_{B} T 
	\end{equation*}
\end{itemize}

\subsection{The Maxwell Distribution}

\subsubsection{Derivation from the Classical Canonical Ensemble}
\begin{itemize}
	\item Recall the single particle classical partition for a monatomic gas in the canonical ensemble
	\begin{equation*}
		Z_{1} = \frac{1}{(2\pi \hbar^{3})}\int e^{-\beta p^{2}/2m}\diff^{3}\bm{r} \diff^{3} \bm{p} = \frac{V}{(2\pi \hbar^{3})}\int e^{-\beta p^{2}/2m}\diff^{3} \bm{p} 
	\end{equation*} 
	Changing to an integral over velocity via $ \bm{p} = m \bm{v} $ and $\diff^{3}\bm{p} = m^{3} \diff^{3}\bm{v} $ gives
	\begin{equation*}
		Z_{1} = \frac{m^{3}V}{(2\pi\hbar)^{3}}\int e^{-\beta m v^{2}/2} \diff^{3}\bm{v}
	\end{equation*}
	Finally, taking advantage of rotational symmetry, changing to spherical coordinates and integrating over solid angle using $ \diff^{3}\bm{v} = 4\pi v^{2}\diff v $ gives
	\begin{equation*}
		Z_{1} = \frac{4\pi m^{3}V}{(2\pi\hbar)^{3}}\int v^{2} e^{-\beta m v^{2}/2} \diff v
	\end{equation*}
	
	\item Recall the original partition function is the sum (integral) over the states of the probability distribution for that state. Here, too, the partition function is a sum, this time over speeds. The integrand must therefore be a probability distribution over speeds. Namely, the probability that a gas atom has speed between $ v $ and $ v + \diff v $ is the integrand plus a normalization factor $ \mathcal{N} $
	\begin{equation*}
		f(v) \diff v = \mathcal{N} v^{2} e^{-\beta mv^{2}/2} \diff v = \mathcal{N} v^{2} e^{-mv^{2}/2k_{B}T} \diff v
	\end{equation*}
	
	\item The normalization factor is found from the usual condition that the probabilities sum to one, i.e. $ \int_{0}^{\infty} = 1$. The integral is evaluated with integration by parts, the general form to use is $ \int x^{2} e^{-x^{2}} \diff x = \int x x e^{-x^{2}} \diff x$ with $ u = x $ and $ \diff v =  x e^{-x^{2}} \diff x$, which is itself evaluated with a simpler integration by parts. The substitution e.g. $ s = \sqrt{\frac{m}{2k_{B}T}} v $ is helpful to simplify the integration. The result is.
	\begin{equation*}
		\mathcal{N} = 4\pi \left(\frac{m}{2\pi k_{B}T} \right)^{3/2}
	\end{equation*}
	
	\item The distribution $ f(v) \diff v $ is called the \textit{Maxwell-Boltzmann distribution}. With $ \mathcal{N} $ plugged in, it reads
	\begin{equation*}
		f(v) \diff v = 4\pi \left(\frac{m}{2\pi k_{B}T} \right)^{3/2} v^{2} e^{-mv^{2}/2k_{B}T} \diff v
	\end{equation*}
	In vector notation (if we had not integrated over solid angle) the distribution reads
	\begin{equation*}
		f(v) \diff^{3} \bm{v} = \left(\frac{m}{2\pi k_{B}T} \right)^{3/2} e^{-mv^{2}/2k_{B}T} \diff^{3} \bm{v}
	\end{equation*}
	In our derivation (which used the monatomic classical canonical ensemble) the Maxwell distribution gives probability that a gas atom has speed between $ v $ and $ v + \diff v $. Remarkable, the distribution holds for other gases, including in the presence of inter-molecular interactions.
	
	\item The distribution can be used to find the mean square speed of atoms in a gas. The result is
	\begin{equation*}
		\expval{v^{2}} \equiv \int_{0}^{\infty}v^{2} f(v) \diff v = \frac{3k_{B}T}{m}
	\end{equation*}
	This result agrees with equipartition of energy with three degrees of freedom, which predicts the average energy (all kinetic; there is no potential) of a gas particle to be $ \frac{3}{2}k_{B}T $. If we compare this to average kinetic energy derived from $ \expval{v^{2}} $ as given by the Maxwell-Boltzmann distribution we see
	\begin{equation*}
		E = \frac{1}{2}m \expval{v^{2}} = \frac{1}{2}m \left(\frac{3k_{B}T}{m}\right) = \frac{3}{2}k_{B}T
	\end{equation*}

\end{itemize}

\subsubsection{Maxwell-Boltzmann Distribution and Rotation Symmetry}
\begin{itemize}
	\item Finally, a note on rotational symmetry and the form of the distribution. We first consider the distribution of velocities in only one direction, e.g. $ x $, given by $ \phi (v_{x}) $. Rotational symmetry requires the distribution be the same for the $ x, y $ and $ z $ directions. 
	
	Intuitively, this should make sense; for a free ideal gas whizzing randomly around in a box, the is no absolute reference for what to define as the $ x $ axis; all directions are equivalent.
	
	Rotational invariance requires the distribution is isotropic and can't depend on the direction of velocity, only the speed $ v = \sqrt{v_{x}^{2} + v_{y}^{2} + v_{z}^{2}} $. 
	
	\item The conditions are satisfied for functions $ F(v) $ and $ \phi(v_{x, y, z}) $ for which
	\begin{equation*}
		F(v) \diff v_{x} \diff v_{y} \diff v_{z} = \phi (v_{x}) \phi (v_{y}) \phi (v_{z}) \diff v_{x} \diff v_{y} \diff v_{z}
	\end{equation*}
	In fact, the only solution to this equation are function of the form
	\begin{equation*}
		\phi(v_{x, y, z}) = A e^{-B v_{x, y, z}^{2}}
	\end{equation*}
	where $ A $ and $ B $ are constants. The distribution over speeds in all three directions is
	\begin{equation*}
		F(v) \diff v_{x} \diff v_{y} \diff v_{z} = \phi (v_{x}) \phi (v_{y}) \phi (v_{z}) \diff v_{x} \diff v_{y} \diff v_{z} = A^{3} e^{-B(v_{x}^{2} + v_{y}^{2} + v_{z}^{2})} \diff v_{x} \diff v_{y} \diff v_{z}
	\end{equation*}
	Applying $ \diff v_{x} \diff v_{z}\diff v_{y} \diff v_{z} = \diff^{3} \bm{v} $, $ v^{2} = v_{x}^{2} + v_{y}^{2} + v_{z}^{2} $ and rotational symmetry gives
	\begin{equation*}
		F(v) \diff^{3}\bm{v} = A^{3} e^{-B v^{2}}\diff^{3}\bm{v} = 4 \pi A^{3} v^{2}e^{-B v^{2}}\diff v
	\end{equation*}
	which is the general form of the Maxwell-Boltzmann distribution. 
	
	\item This derivation, which relies on rotation symmetry alone, produces only the general form of the distribution. Deriving the values of the coefficients $ B = \frac{m}{2k_{B}T} $ and $ A = \sqrt{\frac{m}{2\pi k_{B}T}} $ requires the use of statistical mechanics, such as in the derivation from the canonical ensemble given above.
\end{itemize}

\subsubsection{Pressure and the Ideal Gas Law}
The results from the Maxwell-Boltzmann distribution can be used to derive the ideal gas law $ pV = Nk_{B}T $ using the interpretation of pressure arsing from the collisions of gas particles with the walls of an enclosure.
\begin{itemize}
	\item Consider a cubic enclosure with sides of length $ L $ containing an ideal gas. Collisions between gas particles and the enclosure walls are essentially elastic, since the mass of the enclosure in any everyday scenario would be vastly larger than the mass of a particle.
	
	A gas particle with mass $ m $ traveling with velocity $ v_{x} $ in the $ x $ direction will bounce off a wall and return with $ x $ velocity $ - v_{x} $, changing its momentum by $ 2 m v_{x}$. The force on the wall, corresponding to the impulse $ F \Delta t = \Delta p_{x}  $ is
	\begin{equation*}
		F = \frac{\Delta p_{x}}{\Delta t} = 2mv_{x}\cdot \frac{v_{x}}{2L}= \frac{mv_{x}^{2}}{L}
	\end{equation*}
	where $ \Delta t = \frac{2L}{v_{x}} $ is the average time for the particle to hit the next wall. 
	
	\item Summing over all $ N $ gas particles, the force on the wall is
	\begin{equation*}
		F = N \frac{m \expval{v_{x}^{2}}}{L}
	\end{equation*}
	where $ \expval{v_{x}^{2}} $ is the average velocity in the $ x $ direction. 
	
	\item By rotational symmetry, the $ x $, $ y $ and $ z $ direction must be equivalent, so $ \expval{v_{x}^{2}} = \expval{v_{y}^{2}} = \expval{v_{z}^{2}} $. It follows  $ \expval{v^{2}} = 3 \expval{v_{x}^{2}} $, and so the force on the wall can be written
	\begin{equation*}
		F = N \frac{m \expval{v^{2}}}{3L}
	\end{equation*}
	
	\item We can now use the result from the Maxwell-Boltzmann distribution $ \expval{v^{2}} = \frac{3k_{B}T}{m}$ to get
	\begin{equation*}
		F = N \frac{m}{3L} \frac{3k_{B}T}{m} = \frac{Nk_{B}T}{L}
	\end{equation*}
	Pressure is force per area; using $ p = \frac{F}{L^{2}} $ and substituting $ L^{3} = V $ gives
	\begin{equation*}
		p = \frac{F}{L^{2}} = \frac{Nk_{B}T}{L^{3}} = \frac{Nk_{B}T}{V} \implies pV = Nk_{B}T
	\end{equation*}
\end{itemize}

\subsection{Diatomic Gases}
A monatomic gas particle has only three degrees of translation freedom. In a simple model of a diatomic gas, consisting of two point masses attached by a spring, we must also consider rotational and vibrational degrees of freedom. The diatomic particles' degrees of freedom are
\begin{itemize}
	\item Three translational degrees of freedom
	\item Two (not three) rotational degrees of freedom about the two axis perpendicular to the axis of symmetry (rotation about the axis of symmetry has negligible moment of inertia since diatomic gas is modeled by point particles).
	\item One vibrational degree of freedom corresponding to oscillation along the axis of symmetry.
\end{itemize}
In our simple model, we consider the rotational and vibrational degrees of freedom to be independent, so we can factor the partition function of a single molecule into the independent contributions from each degree of freedom
\begin{equation*}
	Z_{trans} = Z_{trans}Z_{rot}Z_{vib}
\end{equation*}

\subsubsection{Translation}
The translation degree of freedom is the same as for a monatomic ideal gas, which we know to be
\begin{equation*}
	Z_{1} = V \left(\frac{mk_{B}T}{2\pi \hbar^{2}}\right)^{3/2}
\end{equation*}
where $ m $ is the mass of the gas molecule

\subsubsection{Rotation}
\begin{itemize}
	\item The plan is to derive the rotational Hamiltonian $ H_{rot} $ from the Lagrangian $ L_{rot} $ using Hamiltonian mechanics, and use the Hamiltonian to calculate the partition function by integrating over $ e^{-\beta H_{rot}} $. 
	
	\item From classical mechanics, the Lagrangian for two rotational degrees of freedom with equal moments of inertia $ I $ is
	\begin{equation*}
		L_{rot} = \frac{1}{2} I\left(\dot{\theta}^{2} + \sin^{2}\theta \dot{\phi}^{2}\right)
	\end{equation*}
	The conjugate momenta (don't worry if this is unfamiliar; it comes from Hamiltonian dynamics but is largely irrelevant to the understanding of diatomic gases) are
	\begin{equation*}
		p_{\theta}  = \pdv{L_{rot}}{\dot{\theta}} = I \dot{\theta} \qquad \text{and} \qquad p_{\phi}  = \pdv{L_{rot}}{\dot{\phi}} = I \sin^{2}\theta \dot{\phi}
	\end{equation*}
	From the conjugate momenta, we get the Hamiltonian $ H_{rot} $ form
	\begin{equation*}
		H_{rot} = \dot{\theta}p_{\theta} + \dot{\phi}p_{\phi} - L_{rot} = \frac{p_{\theta}^{2}}{2I} + \frac{p_{\phi}^{2}}{2I\sin^{2}\theta}
	\end{equation*}
	
	\item The rotational contribution the partition function is thus
	\begin{align*}
		Z_{rot} &= \frac{1}{(2\pi \hbar)^{2}} \int e^{-\beta H_{rot}} \diff \theta \diff \phi \diff p_{\theta} \diff p_{\phi}\\
		& = \frac{1}{(2\pi \hbar)^{2}} \int \exp(-  \frac{\beta p_{\theta}^{2}}{2I} -  \frac{\beta p_{\phi}^{2}}{2I\sin^{2}\theta}) \diff \theta \diff \phi \diff p_{\theta} \diff p_{\phi}\\
		&= \frac{1}{(2\pi \hbar)^{2}}  \int_{0}^{\pi} \diff \theta  \int_{0}^{2\pi} \diff \phi \int_{-\infty}^{\infty} \exp(-  \frac{\beta p_{\theta}^{2}}{2I}) \diff p_{\theta} \int_{-\infty}^{\infty} \exp(-  \frac{\beta p_{\phi}^{2}}{2I\sin^{2}\theta}) \diff p_{\phi}\\
		&=\frac{1}{(2\pi \hbar)^{2}} \sqrt{\frac{2\pi I}{\beta}}  \int_{0}^{2\pi} \diff \phi  \int_{0}^{\pi} \sqrt{\frac{2\pi I \sin^{2}\theta}{\beta}} \diff \theta = \frac{2Ik_{B}T}{\hbar^{2}}
	\end{align*}
	Note the normalization factor is $\frac{1}{h^{2}}$ (and not $ \frac{1}{h^{2}} $ as in $ Z_{trans} $) because there are only two rotational degrees of freedom.
	
	\item The rotational contribution to average energy is, using the familiar $ E = -\pdv{}{\beta} \ln Z $
	\begin{equation*}
		E_{rot} = - \pdv{}{\beta} \ln(\frac{2I}{\hbar^{2} \beta}) = -\pdv{}{\beta} \ln \frac{1}{\beta} = \frac{1}{\beta} = k_{B}T
	\end{equation*}
	Similarly rotation contribution to heat capacity is simply
	\begin{equation*}
		C_{V_{rot}} = \pdv{E_{rot}}{T} \bigg |_{V} = k_{B}
	\end{equation*}
	Both results could be predicted from equipartition of energy; for 2 degrees of freedom, the average energy per particle is $ \frac{2}{2}k_{B}T = k_{B}T $.
	
\end{itemize}

\subsubsection{Vibration}
\begin{itemize}
	\item The Hamiltonian for a single vibrational degree of freedom is the familiar harmonic oscillator. If $ \xi $ is the displacement from the equilibrium and $ \omega $ is the vibration frequency, the Hamiltonian is		
	\begin{equation*}
		H_{vib} = \frac{p_{\xi}^{2}}{2m} + \frac{1}{2} m \omega^{2}\xi^{2}
	\end{equation*}
	By the way, the presence of a potential energy term $ \frac{1}{2} m \omega^{2}\xi^{2} $ means that our formulation of equipartition of energy does not apply to vibration. 
	
	\item From the Hamiltonian, we can compute the vibrational contribution to the partition function
	\begin{align*}
		Z_{vib} &= \frac{1}{2\pi \hbar} \int e^{-\beta H_{vib}} \diff \xi \diff p_{\xi} = \frac{1}{2\pi \hbar} \int \exp(-\beta \frac{p_{\xi}^{2}}{2m} -\beta  \frac{1}{2} m \omega^{2}\xi^{2}) \diff \xi \diff p_{\xi}\\
		&=\frac{1}{2\pi \hbar} \int_{-\infty}^{\infty} \exp(-\beta \frac{p_{\xi}^{2}}{2m})\diff p_{\xi} \int_{-\infty}^{\infty}\exp(-\beta  \frac{1}{2} m \omega^{2}\xi^{2}) \diff \xi \\
		&=\frac{1}{2\pi \hbar} \sqrt{\frac{2\pi m}{\beta}} \sqrt{\frac{2\pi}{\beta m \omega^{2}}} = \frac{k_{B}T}{\hbar \omega}
	\end{align*}
	
	\item The average vibrational energy of a gas molecule is $ E_{vib} = -\pdv{}{\beta} \ln Z  $
	\begin{equation*}
		E_{vib} = -\pdv{}{\beta} \ln(\frac{1}{\hbar \omega \beta}) = -\pdv{}{\beta} \ln(\frac{1}{\beta}) = \frac{1}{\beta} = k_{B}T
	\end{equation*}
	As for rotation, the contribution to heat capacity is
	\begin{equation*}
		C_{V_{vib}} = \pdv{E_{vib}}{T} \bigg |_{V} = k_{B}
	\end{equation*}
\end{itemize}

\subsubsection{Summary and Note on Quantum Effects}
\begin{itemize}
	\item Energy is additive for independent degrees of freedom, so the total energy of a single diatomic gas molecule is
	\begin{equation*}
		E = E_{trans} + E_{rot} + E_{vib} = \frac{7}{2}k_{B}T
	\end{equation*}
	Analogously, the total heat capacity is
	\begin{equation*}
		C_{V} = C_{V_{trans}} +	C_{V_{rot}} + C_{V_{vib}} = \frac{7}{2}k_{B}
	\end{equation*}
	which could just as well have been found from $ C_{V} = \pdv{E}{T} $.
	
	\item For ideal (non-interacting particles) we can trivially scale these results up to a gas of $ N $ particles to get
	\begin{equation*}
		E = \frac{7}{2}Nk_{B}T \qquad \text{and} \qquad C_{V} = \frac{7}{2} N k_{B}
	\end{equation*}
	
	\item The heat capacity differs from this value at low temperatures because of quantum effects; this is discussed in the section on quantum gases.
\end{itemize}


\subsection{Interacting Gases}
So far we have only considered ideal gases, in which the particles are non-interacting and thus independent. We now take a brief foray into interacting gases, where the physics and and particularly mathematics is much more complicated.

\subsubsection{Monatomic Interacting Gas}
\begin{itemize}
	\item We can consider the ideal gas law as the limit of no interactions between gas particles, and mentioned it is a good approximation when density $ \frac{N}{V} $ is small. The idea of the ideal gas law as a limit for small $ \frac{N}{V} $ forms the basis for the \textit{virial expansion}, the most general equation of state for a monatomic gas:
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} + B_{2}(T) \frac{N^{2}}{V_{2}} + B_{3}(T) \frac{N^{3}}{V_{3}} + \cdots
	\end{equation*}
	The temperature-dependent coefficients $ B_{j}(T) $ are call the \textit{virial coefficients}.
	
	\item The goal is to calculate the virial coefficients from a inter-particle potential $ U(r) $. Any inter-particle potential must have two key features: an attractive component scaling as $ \sim \frac{1}{r^{6}} $ and a rapidly rising, short-range repulsive component.
	
	\item The attractive $ \frac{1}{r^{6}} $ interaction arises from fluctuating electric dipoles and is sometimes called the \textit{van der Waals interaction}. Recall that two permanent dipole moments $ p_{1} $ and $ p_{2} $ have a potential energy which scales as $ \sim \frac{p_{1}p_{2}}{r^{3}} $. Although neutral particles don't have permanent dipoles, they can acquire a temporary dipole from quantum fluctuations. 
	
	If a first particle has a temporary dipole $ p_{1} $ it will induce an electric field $ E \sim \frac{p_{1}}{r^{3}} $ which will induce a second dipole $ p_{2} \sim E \sim \frac{p_{1}}{r^{3}} $ in a nearby particle. The resulting potential energy between particles scales as $ \frac{p_{1}p_{2}}{r^{3}} \sim \frac{1}{r^{6}}$
	
	The rapidly rising term arises from the Pauli exclusion principle, which prevents two particles from occupying the same space. We discuss this more in the quantum gas section. For now the details of the principle are immaterial; all that matters is knowing the Pauli exclusion principle requires a rapidly rising short-range potential.
	
	\item A conventional choice for the potential between monatomic gas particles is the \textit{Lennard-Jones potential}
	\begin{equation*}
		U(r) \sim \left(\frac{r_{o}}{r}\right)^{12} - \left(\frac{r_{o}}{r}\right)^{6}
	\end{equation*}
	A simpler (and also common) choice is the \textit{hard core repulsion}
	\begin{equation*}
		U(r) = 
		\begin{cases}
			\infty & r < r_{0}\\
			-U_{0}\left(\frac{r_{o}}{r}\right)^{6} & r \geq r_{0}
		\end{cases}
	\end{equation*}
\end{itemize}

\subsubsection{Mayer's $ f $ Function and Approximate Partition Function}
\begin{itemize}
	\item For an interacting monatomic gas of $ N $ particles with potential $ U(r) $, the Hamiltonian is
	\begin{equation*}
		H = \sum_{i=1}^{N} \frac{p_{i}^{2}}{2m} \sum_{i>j}U(r_{ij})
	\end{equation*}
	where $ r_{ij} = \abs{\bm{r}_{i} - \bm{r}_{j}}$ denotes the separation between gas particles. The restriction $ i > j $ in the sum over $ U(r_{ij}) $ ensures we counts each pair of particles exactly once.
	
	\item The classical partition function for $ N $ particles is then
	\begin{equation*}
		Z(N, V, T) = \frac{1}{N!}\frac{1}{(2\pi \hbar)^{3N}} \int \prod_{i}^{N}e^{-\beta H} \diff^{3} \bm{p}_{i} \diff^{3} \bm{r}_{i} 
	\end{equation*}
	By separating the two terms in $ H $ and using the identity $ e^{a + b} = e^{a}e^{b} $, the integral can be factored into a momentum and position term
	\begin{equation*}
		\textstyle{Z = \frac{1}{N!}\frac{1}{(2\pi \hbar)^{3N}} \Big[\int \prod_{i}^{N} \exp \Big(-\beta \sum_{j} \frac{p_{j}^{2}}{2m}\Big) \diff^{3} \bm{p}_{i}\Big] \Big[\int \prod_{i}^{N}\exp\Big(-\beta \sum_{j < k}U(r_{jk})\Big) \diff^{3} \bm{r}_{i} \Big]}
	\end{equation*}
	The momentum term together with the normalization factor $ \frac{1}{(2\pi \hbar)^{3N}} $ is just like the momentum term in the partition function for an ideal gas and evaluates to $ \frac{1}{\lambda^{3N}} $, where $ \lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}} $ is the thermal de Broglie wavelength. The partition function is then
	\begin{equation*}
		Z = \frac{1}{N!}\frac{1}{\lambda^{3N}} \int \prod_{i}^{N}\exp \bigg (-\beta \sum_{j < k}U(r_{jk})\bigg ) \diff^{3} \bm{r}_{i} 
	\end{equation*}
	
	\item There is no easy way to evaluate the position integral, since the potential term does not factor in an obvious way. At this point, we use approximation techniques to evaluate the integral. Note that a Taylor series expansion of the potential term $ \exp \big (-\beta \sum_{j < k}U(r_{jk})\big ) \diff^{3} \bm{r}_{i}  $ is ineffective. Why? The Taylor expansion would read
	\begin{equation*}
		\exp \bigg (-\beta \sum_{j < k}U(r_{jk})\bigg ) \diff^{3} \bm{r}_{i} = 1 -\beta \sum_{j < k}U(r_{jk}) + \frac{\beta^{2}}{2} \sum_{j < k}U(r_{jk}) \sum_{l < m}U(r_{lm}) \mp \cdots
	\end{equation*}
	By the nature of the inter-particle potential, $ U(r_{ij}) \to \infty $ as $ r_{ij} \to 0 $, so the higher order terms, containing products of $ U $, would be larger than the smaller order terms. But in a well-behaved expansion, we want successive terms to be smaller than preceding terms! 
	
	\item Instead of working directly with the sum over $ U(r_{ij}) $, we will use a quantity called the \textit{Mayer f-function}
	\begin{equation*}
		f(r) = e^{-\beta U(r)} -1
	\end{equation*}
	For large separations as $ r \to \infty $, $ f(r) \to 0$ (this is because as $ r \to \infty $, $ U(r) \to 0 $ so $ e^{-\beta U(r)} \to 1$). Meanwhile, as the particles come close together as $ r \to 0 $, $ f(r) \to -1 $ (since $ U(r) \to \infty$ and $ e^{-\beta U(r)} \to 0 $). 
	
	\item To construct a suitable expansion of the partition function in terms of $ f $, we define
	\begin{equation*}
		f_{ij} = f(r_{ij})
	\end{equation*}
	and write the partition function as
	\begin{align*}
		Z(N, V, T) &= \frac{1}{N! \lambda^{3N}} \int \prod_{i}^{N}\diff^{3}\bm{r}_{i} \prod_{j > k}(1 + f_{jk})\\
		&=\frac{1}{N! \lambda^{3N}} \int \prod_{i}^{N}\diff^{3}\bm{r}_{i}\left(1 + \sum_{j>k}f_{jk} + \sum_{j > k, l>m}f_{jk}f_{lm} + \cdots\right)
	\end{align*}
	The first term $  \prod_{i}^{N}\diff^{3}\bm{r}_{i} $ contributes a volume factor to each integral for a total of $ V^{N} $. 
	
	Each pair of particles contributes a factor of the form
	\begin{equation*}
		\int \prod_{i}^{N} \diff^{3}\bm{r}_{i} f_{12} = V^{N-2}\int \diff^{3}\bm{r}_{1} \diff^{3}\bm{r}_{2}f(r_{12}) = V^{N-1}\int \diff^{3}\bm{r} f(\bm{r}) \diff^{3}\bm{r}_{2}f(r_{12})
	\end{equation*}
	to the second term. In the last equality, we have changed from integration over $ \bm{r}_{1} $ and $ \bm{r}_{2} $ to integration over the center of mass $ \bm{R} = \tfrac{1}{2}(\bm{r}_{1} + \bm(r)_{2}) $ and separation $ (\bm{r}_{1} - \bm(r)_{2}) $.
	
	Formally, there are $ \frac{1}{2}N(N-1) $ such terms for each pair of particles, but for $ N \sim 10^{23} $ we can safely replace $ \frac{1}{2}N(N-1) $ with $ \frac{1}{2}N^{2} $. 
	
	\item Putting the pieces together, if we expand the partition function in the Mayer function $ f $ and ignore terms quadratic in $ f $ and higher, we get
	\begin{equation*}
		Z(N, V, T) = \frac{V^{N}}{N! \lambda^{3N}} \left(1 + \frac{N^{2}}{2V} \int \diff^{3}\bm{r} f(r) \right)
	\end{equation*}
	Following is a slight trick: we promote one power of $ N $ from the term $ \frac{N^{2}}{2V} $ to an overall exponent of the expansion. Together with recognizing $ \frac{V^{N}}{N! \lambda^{3N}} = Z_{ideal} $, this gives
	\begin{equation*}
		Z(N, V, T) = Z_{ideal} \left(1 + \frac{N}{2V} \int \diff^{3}\bm{r} f(r) \right)^{N}
	\end{equation*}
	Note that this is an expansion in terms of the gas density $ \frac{N}{V} $, just like the general virial expansion.
	
	\item In terms of $ Z(N, V, T) $ (using the identity $ \ln AB = \ln A + \ln B $), the free energy is
	\begin{align*}
		F &= -k_{B}T \ln Z = -k_{B}T\ln Z_{ideal} - k_{B}T\ln \left(1 + \frac{N}{2V} \int \diff^{3}\bm{r} f(r)\right)^{N} \\
		&=F_{ideal} - N k_{B}T \ln(1 + \frac{N}{2V} \int \diff^{3}\bm{r} f(r))
	\end{align*}
	Note that the free energy is proportional to $ N $, as expected.
\end{itemize}

\subsubsection{van der Waals Equation of State}
\begin{itemize}
	\item We can use our approximation for free energy to derive the \textit{van der Waals equation of state}, which models deviations from the ideal gas law in real gases. 
	
	First, we use the first-order Taylor expansion $ \ln x \approx 1 + x $ to write $ F $ as
	\begin{equation*}
		F \approx F_{ideal} - N k_{B}T \frac{N}{2V} \int \diff^{3}\bm{r} f(r)
	\end{equation*}
	The definition of pressure $ p = -\pdv{F}{V} $ and the identity $ -\pdv{}{V} F_{ideal} = \frac{Nk_{B}T}{V} $ gives
	\begin{equation*}
		p = -\pdv{F}{V} = \frac{Nk_{B}T}{V} - k_{B}T \frac{N^{2}}{2V^{2}} \int \diff^{3}\bm{r} f(r)
	\end{equation*}
	
	\item Manipulating the equation for $ p $ to match the virial expansion gives
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} - \frac{N^{2}}{2V^{2}} \int \diff^{3}\bm{r} f(r) + \cdots
	\end{equation*}
	Note that the second virial coefficient in this expansion is 
	\begin{equation*}
		B_{2}(T) = \frac{1}{2} \int \diff^{3}\bm{r} f(r)
	\end{equation*}
	To interpret this result further, we must evaluate $ \int \diff^{3}\bm{r} f(r) $ using our chosen potential.
	
	\item Suppose we have a repulsive interaction, e.g. $ U(r) > 0 $ for all $ r $ with $ U(r) \to 0 $ as $ r \to \infty $. In this case $ f = e^{-\beta U} - 1 < 0 $ for all finite $ r $  (since $ e^{-\beta U} = 1 $ only when $ U = 0 $ at infinite separation). The correction term $ - \frac{N^{2}}{2V^{2}} \int \diff^{3}\bm{r} f(r) $ is then positive, and the gas's pressure increases, which makes sense for a repulsive interaction.
	
	Analogously, if $ U(r) < 0 $, then $ f > 0 $ for all finite $ r $, the correction term is negative, and pressure decreases, which is logical for an attractive interaction.
	
	\item Next, we consider a more realistic interaction in which the potential is repulsive at short distances and attractive at long distances. We will use the hard core repulsion
	\begin{equation*}
		U(r) = 
		\begin{cases}
			\infty & r < r_{0}\\
			-U_{0}\left(\frac{r_{o}}{r}\right)^{6} & r \geq r_{0}
		\end{cases}
	\end{equation*}
	The integral of the Mayer function $ f = e^{-\beta U(r)} - 1$ then reads
	\begin{align*}
		\int f(r)\diff^{3}\bm{r} &= \int_{0}^{r_{0}} \left(e^{-\beta \cdot \infty} - 1\right)\diff^{3}\bm{r} + \int_{r_{0}}^{\infty} \left(e^{\beta U_{0}\left(\frac{r_{0}}{r}\right)^{6}} - 1\right)\diff^{3}\bm{r}\\
		&=\int_{0}^{r_{0}}(-1)\diff^{3}\bm{r} + \int_{r_{0}}^{\infty} \left(e^{\beta U_{0}\left(\frac{r_{0}}{r}\right)^{6}} - 1\right)\diff^{3}\bm{r}\\
	\end{align*}
	We will approximate the second integral with a first order Taylor expansion in the high-temperature limit where $ \beta U_{0} \ll 1$ and make use of spherical symmetry by integrating over solid angle in both integrals.
	\begin{align*}
		\int f(r)\diff^{3}\bm{r} &= - 4\pi \int_{0}^{r_{0}}r^{2}\diff r + 4\pi \int_{r_{0}}^{\infty} \beta U_{0}\left(\frac{r_{0}}{r}\right)^{6} r^{2}\diff r\\
		&=\frac{4\pi r_{0}^{3}}{3}\left(\frac{U_{0}}{k_{B}T}-1\right)
	\end{align*}
	
	\item Inserting the expression for $ \int f(r)\diff^{3}\bm{r} $ into our two-term virial expansion
	\begin{equation*}
		\frac{p}{k_{B}T} = \frac{N}{V} - \frac{N^{2}}{2V^{2}} \int \diff^{3}\bm{r} f(r),
	\end{equation*}
	performing some algebraic manipulations, and defining $ a = \frac{2\pi r_{0}^{3}U_{0}}{3} $ and $ b = \frac{2\pi r_{0}^{3}}{3} $ leads to the equation of state
	\begin{equation*}
		\frac{pV}{k_{B}T} = 1 -\frac{N}{V}\left(\frac{a}{k_{B}T} - b\right)
	\end{equation*}
	
	\item To get to the van der Waals equation, we write our equation of state in the form 
	\begin{equation*}
		k_{B}T = \frac{V}{N}\left(p + \frac{N^{2}}{V^{2}}a\right)\left(1 + \frac{N}{V}b\right)^{-1}
	\end{equation*}
	Because we are working in an expansion of small density, we can expand the term $ \left(1 + \frac{N}{V}b\right)^{-1} $ using $ \frac{1}{1+x} \approx 1 - x $ which leads to
	\begin{equation*}
		k_{B}T = \left(p + \frac{N^{2}}{V^{2}}a\right)\left(\frac{V}{N} - b\right)
	\end{equation*}
	This is the famous van der Waals equation. It models monatomic gases with inter-particle interaction. Notice that in the limit $ a, b \to 0 $ (representing an absence of interaction) the van der Waals equation recovers the familiar ideal gas equation.
	
	\item The van der Waals equation is valid only at low densities (because we used an expansion in small $ \frac{N}{V} $) and at high temperatures (because we made the approximation $ \beta U_{0} \ll 1 $) when evaluating the integral of the Mayer $ f $ function.
	
	Although our derivation used a hard core potential where the attractive component falls as $ \sim \frac{1}{r^{6}} $, the van der Waals equation successfully models other inter-atomic potentials that are repulsive at short range and attractive at long range, as long as the attractive potential falls as $ \frac{1}{r^{n}} $ where $ n \geq 4 $. Why $ n \geq 4 $? Evaluating the integral of the Meyer $ f $ function leads to an integral of the form $ \int \frac{r_{0}^{n}}{r^{n-2}} $, which converges for $ n \geq 4 $.
	
	In particular, the $ n \geq 4 $ restriction means the van der Waals equation does not apply to Coulomb interactions, which decrease as $ \frac{1}{r} $.
	
	\item A note on the parameters: $ a $ (which contains the factor $ U_{0} $ representing the depth of the potential well) models the attractive interaction between particles that takes effect at large distances. $ a $ reduces the pressure of the interacting gas compared to an ideal gas.
	
	The parameter $ b $ contains only $ r_{0} $ (the minimum distance two particles can approach) and models hard-core repulsion at short distances. $ b $ reduces the effective volume of the interacting gas compared to an ideal gas; more volume is taken up by the finite-size particles, so the gas's effective volume (the ``empty'' space) is reduces. 

\end{itemize}

\section{Quantum Gases}
The previous section described classical gases; this section discusses situations where quantum effects are important. The idea of quantum gas has wide applications in physics and, among other things, explains the phenomena of light and sound in solids.

\subsection{Density of States}
\begin{itemize}
	\item Consider an ideal gas contained in a cubical box with side lengths $ L $ and volume $ V = L^{3}$. We assume there are no interactions between particles, so the energy eigenstates are plane waves of the form
	\begin{equation*}
		\psi = \frac{1}{\sqrt{V}}e^{i\bm{k}\cdot \bm{r}}
	\end{equation*}
	We impose periodic boundary conditions, so the wave vector $ \bm{k} = (k_{1}, k_{2}, k_{3}) $  is quantized as
	\begin{equation*}
		k_{i} = \frac{2\pi n_{i}}{L} \qquad n_{i} \in \mathbb{Z}
	\end{equation*}
	Using the notation $ k = \abs{\bm{k}} $, the energy of each (free) particle is
	\begin{equation*}
		E_{\bm{n}} = \frac{\hbar^{2}k^{2}}{2m} = \frac{4\pi^{2}\hbar^{2}}{2mL^{2}}n^{2}
	\end{equation*}
	where $ n^{2} = (n_{1}^{2} + n_{2}^{2} + n_{3}^{2}) $.
	
	\item The quantum mechanical partition function for a single gas particle is the sum over all energy eigenstates
	\begin{equation*}
		Z_{1} = \sum_{\bm{n}}e^{-\beta \bm{n}}
	\end{equation*}
	How to evaluate the sum? Generally we approximate the sum with an integral, justified by the close (i.e. essentially continuous) spacing of the energy levels.
	
	\item How about a more formal justification? In terms of the thermal de Broglie wavelength $ \lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}} $, the exponents in the sum are of the form 
	\begin{equation*}
		\beta E_{\bm{n}} \sim \frac{\lambda^{2}n^{2}}{L^{2}}
	\end{equation*}
	up to a factor of $ \pi $. For a macroscopic size box, $ \lambda \lll L $. This means there are a great many states with $ E_{\bm{n}} \leq k_{B}T $ (for which $ e^{-\beta E_{n}} $ is non-negligible) which contribute appreciable to the sum, so we can safely approximate the sum with an integral. 
	
	\item We write change from summation to integration using the identity $ \diff n = \frac{L}{2\pi} \diff k $ and integrating $ k $ over solid angle.
	\begin{equation*}
		\sum_{\bm{n}} \approx \sum \diff^{3}n = \frac{V}{(2\pi)^{3}}\int \diff^{3} k = \frac{4\pi V}{(2\pi)^{3}}\int_{0}^{\infty}k^{2} \diff k
	\end{equation*}
	It is conventional to change to integration over $ E $, which turns out to be more useful for future applications. If we assume a quadratic dispersion relation we have
	\begin{equation*}
		E = \frac{\hbar^{2}k^{2}}{2m} \implies \diff E = \frac{\hbar^{2}k}{m}\diff k \qquad \text{and} \qquad k^{2} = \frac{2mE}{\hbar^{2}}
	\end{equation*}
	
	\item Using the dispersion relation, we can write the integral over $ k $ as
	\begin{equation*}
		\frac{4\pi V}{(2\pi)^{3}}\int_{0}^{\infty}k^{2} \diff k = \frac{V}{2\pi^{2}} \int \sqrt{\frac{2mE}{\hbar^{2}}} \frac{m}{\hbar^{2}} \diff E \equiv \int g(E) \diff E
	\end{equation*}
	where we have defined the so-called \textit{density of states}
	\begin{equation*}
		g(E) \equiv \frac{V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \sqrt{E}
	\end{equation*}
	The quantity $ g(E)\diff E $ is dimensionless and counts the number of states with energy $ E $ between $ E $ and $ E + \diff E $ (note that $ g(E) $ has units $ \si{\joule^{-1}} $). 	
	
	Note that we haven't actually performed an integral over $ \int g(E) \diff E $. Instead, the density of states is used as a measure which we can integrate over any function $ f(E) $, in which case we would be interested in the quantity $ \int f(E) g(E) \diff E $. 
	
	\item The density of states is introduced to replace a summation of any quantity over the eigenstates $ \sum_{\bm{n}} $ with an integration of the same quantity over the density of states. 
	\begin{equation*}
		\sum_{n} \to \int g(E) \diff E 
	\end{equation*}
	Viewed pragmatically, the density of states is a way to avoid evaluating the difficult summation over eigenstates; the integral is usually easier to evaluate. In practice, for some quantity $ f(E) $, we would write
	\begin{equation*}
		\sum_{n} f(E) \approx \int f(E) g(E) \diff E 
	\end{equation*}

	\item For relativistic systems, the dispersion relation (using the relativistic equation for energy) is
	\begin{equation*}
		E = \sqrt{\hbar^{2}k^{2}c^{2} + m^{2}c^{4}}
	\end{equation*}
	An analogous (if slightly cumbersome) derivation to the one above produces the density of states
	\begin{equation*}
		g(E) = \frac{VE}{2\pi^{2}\hbar^{3}c^{3}} \sqrt{E^{2} - m^{2}c^{4}}
	\end{equation*}
	For massless particles, the density of states is
	\begin{equation*}
		g(E) = \frac{VE^{2}}{2\pi^{2}\hbar^{3}c^{3}}
	\end{equation*}
	Notice that for relativistic particles $ g(E) \sim E^{2} $. 
\end{itemize}

\subsection{Photons and Blackbody Radiation}
\begin{itemize}
	\item We will model light as a gas of photons, and use statistical mechanics to derive light's wavelength distribution, which in turn encodes the light's color.
	
	\item We analyze the color of light (i.e. a photon gas) at fixed temperature, but a similar analysis applies to the color of \textit{any} object at fixed temperature. Here is the justification: consider bathing an object in a gas of photons. In equilibrium, the object will have the same temperature as the photon gas and emit as many photons as it absorbs. This means that, in thermal equilibrium, the color of the object and the color the surrounding light will be the same, and we can find the object's color simply by finding the light's color.
	
	There is a caveat, however: any object will exhibit absorption and emission lines characteristic of its particular atomic makeup where it will exhibit deviations from the spectrum of a photon gas.
	
	\item A gas of photons is called \textit{blackbody radiation}, which is something of a strange name. If we neglect the details of a body's absorption and emission lines and consider only the general form of the emitted photon spectrum, we can work with an idealized body that absorbs photons of any wavelength and reflects none. At zero temperature, the body would appear black, hence the name blackbody. At any non-zero temperature, however, the body's photon emission spectrum will have indeed color.
	
\end{itemize}

\subsubsection{Some Review of Photons}
\begin{itemize}
	\item A photon's energy is determined by its wavelength $ \lambda $, or equivalently by its frequency $ \nu $. Wavelength and frequency are related by
	\begin{equation*}
		c = \lambda \nu
	\end{equation*}
	It is customary to work with the angular frequency $ \omega = 2\pi \nu $, in which case we have $ \omega = 2\pi \frac{c}{\lambda} $. A photon's energy is
	\begin{equation*}
		E = \hbar \omega = h \nu
	\end{equation*}
	Frequency is related to the magnitude $ k $ of the photon's wave vector by 
	\begin{equation*}
		\omega = ck
	\end{equation*}
	
	\item Photons have two polarization states, one for each of the two dimensions transverse to the direction of propagation. The density of states for photons is
	\begin{equation*}
		g(E)\diff E = \frac{VE^{2}}{\pi^{2}\hbar^{3}c^{3}} \diff E
	\end{equation*}
	This is the density of states for massless relativistic particles multiplied by a factor of two to account for the two polarization states.
	
	In terms of frequency, using $ E = \hbar \omega $ and $ \diff E = \hbar \diff \omega $, the number of energy states available to a photon with frequency between $ \omega  $ and $ \omega + \diff \omega $ is 
	\begin{equation*}
		g(\omega) \diff \omega = \frac{V \omega^{2}}{\pi^{2}c^{3}}\diff \omega
	\end{equation*}
	
	\item Note that photons are not conserved. For example, an object can absorb one photon and emit two. Because photon number is not conserved, we cannot define a chemical potential for a gas of photons. This means we must consider states with any number $ N $ of photons, and even in the canonical ensemble, we must sum over states with different numbers of photons, because these are all ``accessible states''. 

\end{itemize}

\subsubsection{Photon Partition Function and the Planck Distribution}

\begin{itemize}	
	\item A gas of $ N $ photons at a definite frequency $ \omega $ has energy $ E = N \hbar \omega $. Summing over $ N $ gives the partition function $ Z_{\omega} $ for photons at a fixed frequency $ \omega $. We evaluate the sum by recognizing the general form of the geometric series $ \sum x^{n} = \frac{1}{1 - x}$.
	\begin{equation*}
		Z_{\omega} = \sum_{N = 0}^{\infty} e^{-N\beta E} = 1 + e^{-\beta \hbar \omega} + e^{-2 \beta \hbar \omega}  + \cdots = \frac{1}{1 - e^{-\beta \hbar \omega}}
	\end{equation*}
	
	\item Next, we need to sum over all possible frequencies. We will take advantage of the fact that independent partition functions are multiplicative, which means that the logarithms of the partition functions add. In terms of $ Z_{\omega} $, the number of photon states at fixed frequency $ \omega $, the partition function for a photon gas is
	\begin{equation*}
		\ln Z = \int_{0}^{\infty} \ln Z_{\omega} g(\omega) \diff \omega = - \frac{V}{\pi^{2}c^{3}} \int_{0}^{\infty}\omega^{2} \ln (1-e^{-\beta \hbar \omega}) \diff \omega
	\end{equation*}
	We can calculate all the interesting quantities of a photon gas from its partition function using the familiar statistical mechanical formulas. Note that most of these formula use $ \ln Z $, so it is convenient to have defined the photon partition function in terms of $ \ln Z $ and not $ Z $.
	
	\item The energy stored in a photon gas is (using the identity $\frac{e^{-x}}{1-e^{-x}} = \frac{1}{e^{x} -1}$)
	\begin{equation*}
		E = - \pdv{}{\beta} \ln Z = \frac{V\hbar}{\pi^{2}c^{3}} \int_{0}^{\infty} \frac{\omega^{3}}{e^{\beta \hbar \omega} - 1}\diff \omega
	\end{equation*}
	There is important information contained in the integrand itself (before integrating over frequency): the integrand is the amount of energy carried by photons with frequency between $ \omega $ and $ \omega + \diff \omega$. 
	\begin{equation*}
		E(\omega) \diff \omega = \frac{V\hbar}{\pi^{2}c^{3}}\frac{\omega^{3}}{e^{\beta \hbar \omega} - 1}\diff \omega
	\end{equation*}
	This is called the \textit{Planck distribution}. 
	
	\item The maximum of the Planck distribution occurs at higher frequency (lower wavelength) for higher-temperature gases. The maximum is found by solving $ \dv{}{\omega}E(\omega) = 0$; the solution is 
	\begin{equation*}
		\omega_{max} = \chi \frac{k_{B}T}{\hbar}
	\end{equation*}
	where $ \chi \approx 2.822 $; this comes from the solution of $ 3 - \chi = 3e^{-\chi} $. The equation is called \textit{Wien's displacement law}, and gives maximum frequency of the Planck distribution for a photon gas (or blackbody) at temperature $ T $. Roughly speaking, the equation tells you the color of an object given its temperature, since the object will appear to have the color where its Planck distribution is largest.
	
	\item Returning to the energy stored in a photon gas, we must first evaluate the integral $ \int_{0}^{\infty} \frac{\omega^{3}}{e^{\beta \hbar \omega} - 1}\diff \omega $. To simplify the integral, we first rescale the expression for energy density using $ x = \beta \hbar \omega $ to get
	\begin{equation*}
		E = \frac{V}{\pi^{2}c^{3}} \frac{(k_{B}T)^{4}}{\hbar^{3}} \int_{0}^{\infty}\frac{x^{3}}{e^{x}-1}\diff x
	\end{equation*}
	The solution can be found in terms of the Gamma function and Riemann zeta function; the result is
	\begin{equation*}
		\int_{0}^{\infty}\frac{x^{3}}{e^{x}-1}\diff x = \Gamma(4) \zeta(4) = \frac{\pi^{4}}{15}
	\end{equation*}
	The energy density $ \mathcal{E} = \frac{E}{V} $ is thus
	\begin{equation*}
		\mathcal{E} = \frac{\pi^{2}k_{B}^{4}}{15 \hbar^{3}c^{3}}T^{4}
	\end{equation*}
	
	\item The expression for photon energy density is closely related to the energy emitted by an object at temperature $ T $. This energy flux $ j $ is defined as the energy transfer per unit area per unit time and is given by the \textit{Stefan-Boltzmann law}
	\begin{equation*}
		j = \frac{\mathcal{E}c}{4} = \sigma T^{4}
	\end{equation*}
	where the constant factors are conveniently contained in the \textit{Stefan constant} 
	\begin{equation*}
		\sigma = \frac{\pi^{4}k_{B}^{4}}{60\hbar^{3}c^{2}} = \SI{5.67e-8}{\joule\, \second^{-1}\meter^{-2}\kelvin^{-4}}
	\end{equation*}
	How to explain the factor $ \frac{1}{4} $?. It arises because the law describes not a point source, but a blackbody whose size is larger than the wavelength of the emitted photons. This means photons are emitted only away from the object and we consider only the emitted photon velocity perpendicular to the blackbody. Rather than filling out a sphere of area $ 4\pi $ surrounding the object, the actual flux of photons from any point on the object's surface is
	\begin{equation*}
		\frac{1}{4\pi} \int_{0}^{2\pi}\diff \phi \int_{0}^{\frac{\pi}{2}} (c \cos \theta)\sin \theta \diff \theta = \frac{c}{4}
	\end{equation*}
	where we integrate $ c \cos  $ instead of $ c $ because consider only emitted photon velocity perpendicular to the blackbody, which is $ c \cos \theta $.
	
\end{itemize}

\subsubsection{The Ultraviolet Catastrophe}
\begin{itemize}
	\item The Planck distribution rests on the concept of light existing in quanta---photons---with discrete energy $ E = \hbar \omega $. In fact, this is the first time quanta arose in theoretical physics and represents the beginning of quantum mechanics (in classical physics, light of frequency $ \omega $ can have arbitrarily low energy).
	
	\item Viewing light as packets of photons with discrete energy resolved the so-called ``ultraviolet catastrophe''. The classical-physics distribution for light energy at frequency $ \omega $ is 
	\begin{equation*}
		E(\omega) = \frac{V}{\pi^{2}c^{3}}\omega^{2}k_{B}T
	\end{equation*}
	The ultraviolet catastrophe was the classically predicted total energy $ E = \int_{0}^{\infty} E(\omega)\diff \omega $ diverging when we integrate over all frequencies. This result obviously does not agree with experiment---it essentially predicts the energy of light is infinite! 
	
	Because the Planck distribution contains an exponential suppression $ E(\omega) \sim e^{-\hbar \beta \omega} $, the integral over frequency converges and the ultraviolet catastrophe is resolved. 
	
	\item A physical interpretation: the exponential suppression of energy at high frequencies occurs because when $ \hbar \omega \gg k_{B}T $, the temperature $ T $ and thus thermal energy $ k_{B}T $ is not large enough to create even single photon of quantized energy $ \hbar \omega $. If photons could have arbitrarily low energy at frequency $ \omega $ the thermal energy $ k_{B}T $ would be large enough for light emission at any frequency and we would not see the exponential suppression necessary for finite $ E = \int_{0}^{\infty}E(\omega)\diff \omega$.
	
	\item Another cool note: In the classical world, light of frequency $ \omega $ can have arbitrarily low energy, which essentially corresponds to the classical limit $ \hbar \to 0 $ and the regime $ \hbar \omega \ll k_{B}T $ in the Planck distribution. If we approximate $ e^{\beta\hbar\omega} \approx 1 + \beta \hbar \omega$ for $ \hbar \omega \ll k_{B}T $, we get
	\begin{equation*}
		\frac{1}{e^{\beta \hbar \omega} - 1} \approx \frac{1}{\beta \hbar \omega}
	\end{equation*}
	In which case the Planck distribution reduces to the (incorrect) classical formula
	\begin{equation*}
		E(\omega) = \frac{V}{\pi^{2}c^{3}}\omega^{2}k_{B}T
	\end{equation*}
	In other words, the classical distribution can be though of as limiting case the Planck distribution as $ \hbar \to 0 $, corresponding to the transition from the quantum to the classical regime.
\end{itemize}

\subsection{Phonons}
Consider a solid modeled by a three-dimensional crystal lattice in which the constituent atoms are embedded in the lattice; they can vibrate about their lattice positions but cannot travel through the solid. The vibrations of the atoms in the solid (i.e. sound waves) can be analyzed with the same formalism used for photons.

\subsubsection{The Debye Model}
\begin{itemize}
	\item In the example of light, electromagnetic waves are viewed in the quantum world as discrete packets of energy called photons. Analogously, sound waves in solid can be viewed as discrete packets called \textit{phonons}. This conceptual steps forms the basis of the \textit{Debye model} of sound in solids. Taking the photon-phonon analogy further, we take the energy of a phonon to be of the form $ E = \hbar \omega $. If, as for light, we assume the dispersion relation $ \omega = k c_{s} $ we have
	\begin{equation*}
		E = \hbar \omega = \hbar k c_{s}
	\end{equation*}
	where $ c_{s} $ is the speed of sound.
	
	\item The density of the states for phonons is very similar to that of photons with two minor modifications. First, we replace the speed of light $ c $ with the speed of sound $ c_{s} $. Second, phonons have three polarization states (two for the two axes transverse to the direction of propagation and one longitudinal polarization) instead of two for photons. The correct density of states is
	\begin{equation*}
		g(\omega)\diff \omega = \frac{3V}{2\pi^{2}c_{s}^{3}} \omega^{2} \diff \omega
	\end{equation*}
	
	\item The largest difference between phonons and photons is not the density of states but the existence of a maximum phonon frequency (or a minimum phonon wavelength). A minimum possible wavelength should make intuitive sense. Recall that unlike electromagnetic waves, which can propagate through a vacuum, sound can only propagate as vibrations through a material medium; sound is essentially a material material medium vibrating back and forth. 
	
	Phonons with wavelength smaller than the spacing between atoms in a solid will encounter empty space in the inter-atomic regions. Such phonons have no material medium for sound propagation, so phonons are limited to wavelengths comparable to or larger than the inter-atomic spacing. 
	
	The inter-atomic spacing is proportional to $ \left(\frac{V}{N}\right)^{1/3} $, so we expect
	\begin{equation*}
		\lambda_{min} \sim \left(\frac{V}{N}\right)^{1/3} \qquad \text{and} \qquad \omega_{max} \equiv \omega_{D} \sim \left(\frac{V}{N}\right)^{1/3} c_{s}
	\end{equation*}
	The maximum frequency is called the \textit{Debye frequency} in honor of Debye and carries his initial.
	
	\item The Debye frequency is found by equating the number of single phonon states $ \int g(\omega)\diff \omega $ to the number of degrees of freedom in a solid. The number of single phonon states (bounded by the maximum frequency $ \omega_{D} $) is
	\begin{equation*}
		\int_{0}^{\omega_{D}} g(\omega)\diff \omega  = \int_{0}^{\omega_{D}}\frac{3V}{2\pi^{2}c_{s}^{3}} \omega^{2} \diff \omega = \frac{V\omega_{D}^{3}}{2\pi^{2}c_{s}^{3}}
	\end{equation*}
	The number of degrees of freedom in a crystal lattice of $ N $ atoms is $ 3N $ since each atom can move in three spatial dimensions. Equating the two gives
	\begin{equation*}
		\frac{V\omega_{D}^{3}}{2\pi^{2}c_{s}^{3}} = 3N \implies \omega_{D} = \left(\frac{6\pi^{2}N}{V}\right)^{\frac{1}{3}}c_{s}
	\end{equation*}
	Note the proportionality $ \omega_{D} \sim \left(\frac{V}{N}\right)^{1/3} c_{s} $, as expected. 
	
	The argument for equating the number of photon states to the lattice's degrees of freedom comes from solid state physics; I am leaving it out.
	
	\item The Debye frequency gives rise to an associated energy scale and temperature. Following the pattern $ E = \hbar \omega $, the Debye energy is simply $ \hbar \omega_{D} $. The Debye temperature comes from equating the thermal energy $ k_{B}T $ to the Debye energy. The result is
	\begin{equation*}
		T_{D} = \frac{\hbar \omega_{D}}{k_{B}}
	\end{equation*}
	The Debye temperature is the temperature at which the highest frequency phonon (i.e. phonon with frequency $ \omega_{D} $) becomes thermally excited. $ T_{D} $ is near room temperature for most solids. Toward the extreme ends of the spectrum are lead with $ T_{D} \approx \SI{100}{\kelvin} $ and diamond with $ T_{D} \approx \SI{2000}{\kelvin} $. 
	
\end{itemize}

\subsubsection{Heat Capacity in the Debye Model}
\begin{itemize}
	\item First, we construct the phonon partition function; the process is analogous to photon case. As before, we will first find the single-phonon partition function $ Z_{\omega} $ at fixed frequency, then integrate over all frequencies to get the complete partition function. 
	
	Phonon number is not conserved, so we must count over all possible phonon numbers when constructing $ Z_{\omega} $. As for photons (using the geometric series to evaluate the sum), the result is
	\begin{equation*}
		Z_{\omega} = \sum_{N=1}^{\infty}e^{-N \beta\hbar \omega} = 1 + e^{-\beta \hbar \omega} + \cdots = \frac{1}{1 - e^{-\beta \hbar \omega}}
	\end{equation*}
	
	\item Integrating the single-particle partition function over all possible frequencies (bounded by $ \omega_{D} $) and taking advantage of the fact that logarithms of partition functions add (just like for the photon derivation) the complete partition function for a phonon gas is given by
	\begin{equation*}
		\ln Z = \int_{0}^{\omega_{D}} \ln Z_{\omega}g(\omega)\diff \omega = - \frac{3V}{2\pi^{2}c_{s}^{3}}\int_{0}^{\omega_{D}} \omega^{2} \ln(1-e^{-\beta \hbar \omega})\diff \omega
	\end{equation*}
	
	\item The total energy in sound waves is (again using $\frac{e^{-x}}{1-e^{-x}} = \frac{1}{e^{x} -1}$)
	\begin{equation*}
		E = -\pdv{}{\beta} \ln Z = \frac{3V\hbar}{2\pi^{2}c_{s}^{3}} \int_{0}^{\omega_{D}}\frac{\omega^{3}}{e^{\beta\hbar\omega}-1}
	\end{equation*}
	As before, the integrand represents the phonon energy distribution. It has the same form as the Planck distribution for photons and reads
	\begin{equation*}
		E(\omega) \diff \omega = \frac{3V\hbar}{2\pi^{2}c_{s}^{3}}\frac{\omega^{3}}{e^{\beta \hbar \omega} - 1}\diff \omega
	\end{equation*}
	Next, we rescale the integral for phonon energy $ E $ using $ x = \beta \hbar \omega $, so the upper limit becomes $ x_{D} = \beta \hbar \omega_{D} = \frac{T_{D}}{T} $ where we use $ T_{D} = \frac{\hbar \omega_{D}}{k_{B}} $. The energy is
	\begin{equation*}
		E = \frac{3V(k_{B}T)^{4}}{2\pi^{2}(\hbar c_{s})^{3}} \int_{0}^{T_{D}/T} \frac{x^{3}}{e^{x}-1}\diff x
	\end{equation*}
	The resulting parameter-dependent integral has no analytic solution; it is a function of $ \frac{T_{D}}{T} $. (For photons, the upper limit was $ \infty $, so the integral was solvable.) 
	
	In the phonon case, we satisfy ourselves with the limit scenarios $ T\ll T_{D} $ and $ T \gg T_{D} $, for which we can find analytic solutions.
	
	\item In the low-temperature limit $ T \ll T_{D} $ we have $ \frac{T_{D}}{T} \to \infty $, and we can use the integral $ \int_{0}^{\infty}\frac{x^{3}}{e^{x}-1}\diff x = \frac{\pi^{4}}{15}$ to get
	\begin{equation*}
		E = \frac{\pi^{2}Vk_{B}^{4}}{10 \hbar^{3} c_{s}^{3}}T^{4}
	\end{equation*}
	The low-temperature heat capacity $ C_{V} = \pdv{E}{T} $  is
	\begin{equation*}
		C_{V} = \frac{2\pi^{2}Vk_{B}^{4}}{5\hbar^{3} c_{s}^{3}T^{3}} \equiv \frac{12\pi^{4}}{5}Nk_{B}\left(\frac{T}{T_{D}}\right)^{3}
	\end{equation*}
	where the last equality uses the identity
	\begin{equation*}
		T_{D} = \frac{\hbar \omega_{D}}{k_{B}} = \frac{\hbar}{k_{B}} \left(\frac{6\pi^{2}N}{V}\right)^{\frac{1}{3}}c_{s}
	\end{equation*}
	
	\item In the high-temperature limit $ T \gg T_{D} $, all $ x $ in the integral $  \int_{0}^{T_{D}/T}\frac{x^{3}}{e^{x}-1}\diff x  $ will be very small. We can safely the expand the integral into a Taylor series to get
	\begin{equation*}
		\frac{x^{3}}{e^{x} - 1} \approx \frac{x^{3}}{(1 + x + \cdots )-1} = x^{2}
	\end{equation*}
	In this case, the integral is simply
	\begin{equation*}
		 \int_{0}^{T_{D}/T}\frac{x^{3}}{e^{x}-1}\diff x  \approx \int_{0}^{T_{D}/T} x^{2}\diff x  = \frac{1}{3}\left(\frac{T_{D}}{T}\right)^{3}
	\end{equation*}
	The energy and heat capacity are then 
	\begin{equation*}
		E = \frac{Vk_{B}^{4}T_{D}^{3}}{2\pi^{2}\hbar^{3}c_{s}^{3}}T = 3Nk_{B}T \qquad \text{and}\quad C_{V} = \frac{V k_{B}^{4}T_{D}^{3}}{2\pi^{2}\hbar^{3}c_{s}^{3}} = 3 N k_{B}
	\end{equation*}
	where both equalities use $ T_{D} = \frac{\hbar}{k_{B}} \left(\frac{6\pi^{2}N}{V}\right)^{\frac{1}{3}}c_{s} $.
	
	Note that high-temperature heat capacity $ C_{V} = 3Nk_{B} $ depends only on the amount $ N $ and makes no reference to the material's specific properties. Indeed, the fact that high-temperature heat capacity is about the same for all solids had been known experimentally well before the Debye model. It is called the \textit{law of Dulong and Petit}. 
	
\end{itemize}

\subsubsection{Diatomic Gas with Quantum Effects}
\begin{itemize}
	\item Recall that the classical prediction for the heat capacity of a diatomic gas ($ C_{V} = \frac{7}{2}k_{B}T $) only agrees with experiment in the high temperature limit. As the temperature is lowered, experiment suggests the vibrational and then the rotational modes become frozen out and do not contribute to heat capacity.
	
	We can theoretically predict this behavior with a quantum approach, in which we expect degrees of freedom to be frozen out when the discrete minimum amount of energy needed to excite the degree of freedom exceeds the thermal energy $ k_{B}T $. For example, the freezing out of high-frequency photons in the Planck distribution in the case of blackbody radiation resolved the classical ultraviolet catastrophe and led to a reduced temperature heat capacity for phonons.
	
	\item We begin with rotational modes. Recall the classical Hamiltonian is 
	\begin{equation*}
		H_{rot} = \frac{1}{2I} \left(p_{\theta}^{2} +  \frac{p_{\phi}^{2}}{\sin^{2}\theta}\right)
	\end{equation*}
	The analogous quantum Hamiltonian has energy eigenvalues
	\begin{equation*}
		E_{j} = \frac{\hbar^{2}}{2I}j(j+1)
	\end{equation*}
	where $ j $ corresponds to total angular momentum and the degeneracy of each energy level is $ 2j + 1 $. Making sure to include the degeneracy factor, the quantum rotational partition function for a single molecule is
	\begin{equation*}
		Z_{rot} = \sum_{j} (2j + 1)e^{-\beta E_{j}} =  \sum_{j} (2j + 1) \exp(\frac{-\beta\hbar^{2}}{2I}j(j+1))
	\end{equation*}
	
	\item In the high temperature limit $ T \gg \frac{\hbar^{2}}{2Ik_{B}} $ the energy levels are densely populated and we can safely approximate the sum with an integral to get
	\begin{equation*}
		Z_{rot} \approx \int_{0}^{\infty}(2j + 1) \exp(\frac{-\beta\hbar^{2}}{2I}j(j+1)) \diff j = - \frac{2I}{\beta\hbar^{2}} \int_{0}^{\infty} e^{u}\diff u=  \frac{2I}{\beta \hbar^{2}}
	\end{equation*}
	This agrees with the result $ Z_{rot} = \frac{2I}{\beta \hbar^{2}} $ in the classical analysis.
	
	\item In the low temperature limit $ T \ll \frac{\hbar^{2}}{2Ik_{B}} $ the exponent $ e^{-\beta E_{j}} $ is very small for $ j \neq 0 $. Only the term with $ j = 0 $ contributes appreciably to the sum and we have $ Z_{rot} \approx 1 $.
	
	It follows that $ E_{rot} = -\pdv{}{\beta}\ln Z_{rot} \approx 0 $ at low temperature, so $ C_{V_{rot}} = \pdv{E}{T} \approx 0 $, in agreement with experiment.
	
	A cool note: the low-temperature analysis of rotational degrees of freedom also explains why there is no rotational contribution to the heat capacity of a monatomic gas. The reason is not because atoms are point particles and cannot rotate, even though this might make intuitive sense at first glance. The correct argument is that $ I $, although not zero, is so small for atoms that the exponent $ e^{-\beta E_{j}} \sim e^{I} $ in the rotational partition function is effectively zero for all $ j \neq 0 $, (similarly to how these terms were negligible for $ j \neq 0 $ for a diatomic gas at low temperature). The result is $ Z_{rot} \approx $ and thus $ C_{V_{rot}} = 0 $.
	
	\item Next, on to vibrational degrees of freedom, whose Hamiltonian is the quantum harmonic oscillator with energy eigenvalues
	\begin{equation*}
		E_{n} = \hbar \omega \left(n + \frac{1}{2}\right)
	\end{equation*}
	The corresponding partition function is
	\begin{equation*}
		Z_{vib} = \sum_{n} e^{-\beta E_{n}} = e^{-\beta \hbar \omega /2 } \sum_{n} e^{-\beta \hbar \omega n }
	\end{equation*}
	Recognizing the familiar sum of the geometric series $ \sum_{n}e^{-xn} = \frac{1}{1-e^{x}}$ we write
	\begin{equation*}
		Z_{vib} = \frac{e^{-\beta \hbar \omega /2 }}{1 - e^{-\beta \hbar \omega }} = \frac{1}{2 \sinh(\beta \hbar \omega / 2)}
	\end{equation*}
	
	\item In the high-temperature limit $ \beta \ll \hbar \omega \implies \beta \hbar \omega \ll 1$  we can approximate the hyperbolic sine with a Taylor series $ \sinh x \approx x + \cdots $ to get
	\begin{equation*}
		Z_{vib} \approx \frac{1}{2} \frac{2}{\beta \hbar \omega} = \frac{k_{B}T}{\hbar \omega} \qquad (\beta \hbar \omega \ll 1)
	\end{equation*}
	in agreement with the classical result.
	
	\item At low temperatures, $ \beta \hbar \omega \gg 1 $ and so $ e^{-\beta E_{n}} = e^{-\beta \hbar \omega /2} \approx 0 $ for $ n = 0 $ meaning only the term $ n = 0 $ contributes appreciably to the partition function. The result is
	\begin{equation*}
		Z_{vib} = e^{-\beta \hbar \omega /2 } \sum_{n} e^{-\beta \hbar \omega n } \approx  e^{-\beta \hbar \omega /2 }  \qquad (\beta \hbar \omega \gg 1)
	\end{equation*}
	In this case we have 
	\begin{equation*}
		E_{vib} = - \pdv{}{\beta} \ln Z_{vib}\approx - \pdv{}{\beta}e^{-\beta \hbar \omega /2 } = \frac{\hbar \omega}{2}
	\end{equation*}
	which is the familiar zero-point energy of the quantum harmonic oscillator. In agreement with experiment, the heat capacity is
	\begin{equation*}
		C_{V_{vib}} = \pdv{E}{T} \approx \pdv{}{T} \frac{\hbar \omega}{2} = 0 \qquad (\beta \hbar \omega \gg 1)
	\end{equation*}
\end{itemize}

\subsection{Boson Statistics}
For this section, we consider only the monatomic ideal gas. First, a quick reminder of the motivation for quantum statistics at low temperature. As temperature decreases, the thermal de Broglie wavelength, defined in the classical section as
\begin{equation*}
	\lambda = \sqrt{\frac{2\pi \hbar^{2}}{m k_{B}T}}
\end{equation*}
becomes larger and larger. For normal conditions $ \lambda $ is very small, but at low enough $ T $, $ \lambda $ will approach the inter-particle separation $ \left(\frac{V}{N}\right)^{1/3} $. Quantum effects become important once inter-particle separation is comparable to the particle's de Broglie wavelength.

\subsubsection{Quick Review of Bosons and Fermions}
\begin{itemize}
	\item The known quantum particles to occur in nature come in two classes: bosons and fermions. The two classes are characterized by their spin and behavior under space inversion. 
	
	\item Bosons have integer spin. The wave function $ \psi_{b} $ of a boson system is symmetric under the exchange of particles, i.e.
	\begin{equation*}
		\psi_{b}(\bm{r}_{1}, \bm{r}_{2}) = \psi_{b}(\bm{r}_{2}, \bm{r}_{1})
	\end{equation*}
	
	\item Fermions have half-integer spin. The wave function $ \psi_{f} $ of a fermion system is anti-symmetric under the exchange of particles, i.e.
	\begin{equation*}
		\psi_{f}(\bm{r}_{1}, \bm{r}_{2}) = -\psi_{f}(\bm{r}_{2}, \bm{r}_{1})
	\end{equation*}
	
	\item All familiar particles (such as the electron, proton and neutron) are fermions. Systems consisting of an even number of fermions act as bosons, since the even number of half-integer spins produces a cumulative integer spin.
\end{itemize}

\subsubsection{Bose-Einstein Distribution}
\begin{itemize}
	\item First, a change of notation: we will label our quantum systems' single particle quantum states with $ \ket{i} $. We had previously used $ \ket{n} $ to denote single particle states, but $ n $ will be used in this section to denote the number of particles in a state $ \ket{i} $.
	
	With this notation, the single particle energies are $ E_{i} $. We will assume our particles are ideal and thus non-interacting. 
	
	\item Next, we come to an important point: quantum particles of a given type (e.g. electrons, protons, etc...) are indistinguishable. This means we have completely described our system by naming how many particles are in state $ \ket{0} $, how many are in state $ \ket{1} $, and in general how many are in state $ \ket{i} $. 
	
	If the particles were distinguishable, we would need to be more specific and identify which state particle $ a $ is in, which state particle $ b $ is in, and so on for all $ N $ particles. But because quantum particles are indistinguishable, there is no basis for notion of particle $ a, b $, etc... and simply identify the number of particles in each state. 
	
	We will denote the number of particles in the state $ \ket{i} $  by $ n_{i} $. 
	
	\item If we chose to work in the canonical ensemble (which we won't), we would have to sum over all possible ways of partitioning the total system's $ N $ particles into sets $ \{n_{i}\} $ subject to the constraint $ \sum_{i}n_{i} = N $. The partition function would be
	\begin{equation*}
		Z = \sum_{\{n_{i}\}} e^{-\beta n_{i}E_{i}}
	\end{equation*}
	The factor $ n_{i} $ in the exponent accounts for the $ n_{i} $ particles in the state $ \ket{i} $. 
	
	The details of partitioning the $ N $ particles into sets $ \{n_{i}\} $ makes evaluating the partition function really hard! The complication is a consequence of the fact that we can at best describe our system by stating how many particles are in each state $ \ket{i} $, which occurs because quantum particles are indistinguishable.
	
	\item For indistinguishable particles, it turns out to be much easier to work in the grand canonical ensemble. We introduce a chemical potential $ \mu $ and allow the total number of particles $ N $ in our system of interest to fluctuate.  
	
	We consider each single particle state $ \ket{i} $ in turn. In the GCE, a given state $ \ket{i} $ can be occupied by an arbitrary number of particles $ n_{i} $. For the state $ \ket{i} $, the grand partition function is
	\begin{equation*}
		\mathcal{Z}_{i} = \sum_{n_{i}=0}^{\infty} e^{-\beta n_{i}(E_{i} - \mu)}
	\end{equation*}
	
	\item Evaluating the sum is not too difficult. If we assume $ (E_{r} - \mu) > 0 $, so that the terms $ e^{-\beta n_{i}(E_{i} - \mu)} $ decrease with increasing $ n_{i} $, we can write
	\begin{equation*}
		\mathcal{Z}_{i} = \sum_{n_{i}=0}^{\infty} \left(e^{-\beta (E_{i} - \mu)}\right)^{n_{i}} = \frac{1}{1 - e^{-\beta (E_{i} - \mu)}}
	\end{equation*}
	were we have used geometric series $ \sum_{n} x^{n} = \frac{1}{1 - x} $ with $ x = e^{-\beta (E_{i} - \mu)} $. 
	
	I stress that the boson partition function only converges under the assumption $ (E_{i} - \mu) > 0 $, which must hold for all single particle states $ \ket{i} $. If, as per convention, we set the ground state to have energy $ E_{0} = 0 $, the convergence condition implies 
	\begin{equation*}
		(E_{i} - \mu)\big |_{E_{0} = 0} > 0 \implies \mu < 0 
	\end{equation*}
	Since $ \mu $ is the same for all states $ \ket{i} $ \textit{the grand partition function for a Bose gas only makes sense if the system's chemical potential is negative}.
	
	\item Because the grand partition function is multiplicative for independent subsystems, the full grand partition function for a boson gas is simply the product over all $ \mathcal{Z}_{i} $
	\begin{equation*}
		\mathcal{Z} = \prod_{i} \mathcal{Z}_{i} = \prod_{i} \frac{1}{1 - e^{-\beta (E_{i} - \mu)}}
	\end{equation*}
	
	\item Using the familiar formula, the average number of particles in a Bose gas is
	\begin{equation*}
		\expval{N} = \frac{1}{\beta} \pdv{}{\mu} \ln \mathcal{Z} = \sum_{i}\frac{1}{e^{\beta(E_{i} - \mu)} - 1}
	\end{equation*}
	\textit{Derivation:} Plug in $ \mathcal{Z} $ and use $ \ln AB = \ln A + \ln B \implies  \ln \big(\prod_{i}x_{i} \big)= \sum_{i} \ln x_{i} $
	\begin{equation*}
		\ln \mathcal{Z} = \ln \prod_{i} \frac{1}{1 - e^{-\beta (E_{i} - \mu)}} = \sum_{i} \ln (1 - e^{-\beta (E_{i} - \mu)})^{-1}
	\end{equation*} 
	Differentiate with respect to $ \mu $ using $ \pdv{}{\mu}\ln u = \frac{1}{u} \pdv{u}{\mu} $ and simplify to get
	\begin{align*}
		\expval{N} = \frac{1}{\beta} \pdv{}{\mu} \left[\sum_{i} \ln (1 - e^{-\beta (E_{i} - \mu)})^{-1}\right] = \sum_{i} \frac{e^{-\beta(E_{i} - \mu)}}{1 - e^{-\beta(e_{i} - \mu)}}
	\end{align*}
	Multiplying the numerator and denominator by $ e^{\beta(E_{i} - \mu)} $ gives
	\begin{equation*}
		\expval{N} = \sum_{i}\frac{1}{e^{\beta(E_{i} - \mu)} - 1}
	\end{equation*}
	
	\item Naturally, the sum of all $ n_{i} $, the number of gas particles in each state $ \ket{i} $, should add up to the total number of particles $ N $ in the gas as a whole. We write this constraint as $ \expval{N} = \sum_{i}\expval{n_{i}} $. Comparing to the identical form in the definition of $ \expval{N} $, we can define
	\begin{equation*}
		\expval{n_{i}} = \frac{1}{e^{\beta(E_{i} - \mu)} - 1}
	\end{equation*}
	This important equation is the \textit{Bose-Einstein} distribution. It gives the average number of particles in the state $ \ket{i} $ for an ideal monatomic boson gas. 
	
	%TODO The shape of the Bose distribution arises because we sum over the number of particles in a given state rather than summing over the states for a single particle. (compare photon and phonon partition functions).
	
	For macroscopic systems we have $ N \sim 10^{23} $ which is essentially thermodynamic limit $ N \to \infty $ where $ \frac{\Delta N}{\expval{N}} \to 0$ and $ \frac{\Delta n_{i}}{\expval{n_{i}}} \to 0 $. This means fluctuations of particle number around the average value are negligible and we can safely replace $ \expval{N} $ with $ N $ and $ \expval{n_{i}} $ with $ n_{i} $ for most applications.
	
	\subsubsection{Ideal Bose Gas} 
	\textbf{TODO} There is a great deal of interesting information on Bose gases I am currently leaving out, e.g. the convergence of the boson equation of state to the ideal gas law at high temperature, Bose-Einstein condensation, the low-temperature equation of state for a boson gas, etc...
\end{itemize}

\subsection{Fermions}
Fermions are particles with half-integer spin. The wave function of a fermion system is antisymmetric under the exchange of a particle. For a two particle system, we have
\begin{equation*}
	\psi_{f}(\bm{r}_{1}, \bm{r}_{2}) = -\psi_{f}(\bm{r}_{2}, \bm{r}_{1})
\end{equation*}
Unlike bosons, fermions obey the \textit{Pauli exclusion principle}, which states that two identical fermions in a fermion system cannot occupy the same quantum state. Note that the behavior of the fermion wave function under particle exchange agrees with the exclusion principle: If we put two identical fermions in the same place, the wave function vanishes, e.g. if $ \bm{r}_{1} \equiv \bm{r}_{2} $ we have
\begin{equation*}
	\psi_{f}(\bm{r}_{1}, \bm{r}_{1}) = - \psi_{f}(\bm{r}_{1}, \bm{r}_{1})
\end{equation*}
under particle exchange which implies $ \psi_{f} = 0 $.

\subsubsection{Fermion Partition Function}
Like bosons, fermions are indistinguishable, so it is best to work in the grand canonical ensemble.
\begin{itemize}
	\item The exclusion principle makes the grand partition function for single particle state $ \ket{i} $ very simple. Since at most one fermion can occupy a given state, each state $ \ket{i} $ is either occupied or it is not; there is no other option.
	\begin{equation*}
		\mathcal{Z}_{i} = \sum_{n=0}^{1}e^{-\beta n(E_{i}-\mu)} = 1 + e^{-\beta(E_{i} - \mu)}
	\end{equation*}
	
	\item The grand partition function for the whole system is the product of $ \mathcal{Z}_{i} $ over all states $ \ket{i} $:
	\begin{equation*}
		\mathcal{Z} = \prod_{i} \mathcal{Z}_{i} = \prod_{i} (1 + e^{-\beta(E_{i} - \mu)})
	\end{equation*}
	
	\item Using the familiar formula, the average number of particles is
	\begin{equation*}
		\expval{N} = \frac{1}{\beta}\pdv{}{\mu} \ln \mathcal{Z} = \sum_{i} \frac{1}{1 + e^{\beta (E_{i} - \mu)}}
	\end{equation*}
	The derivation reads
	\begin{align*}
		\expval{N} &= \frac{1}{\beta}\pdv{}{\mu} \ln \left[\prod_{i} (1 + e^{-\beta(E_{i} - \mu)})\right] = \frac{1}{\beta} \pdv{}{\mu} \sum_{i} \ln(1 + e^{-\beta(E_{i} - \mu)})\\
		&=\frac{1}{\beta} \sum_{i} \frac{\beta e^{-\beta(E_{i}-\mu)}}{1 + e^{-\beta(E_{i} - \mu)}} = \sum_{i} \frac{e^{-\beta(E_{i}-\mu)}}{1 + e^{-\beta(E_{i} - \mu)}} = \sum_{i} \frac{1}{1 + e^{\beta (E_{i} - \mu)}}
	\end{align*}
	
	\item Just like for bosons, the average number of particles in each state $ \ket{i} $ must add up to the total number of particles $ N $ in the system, leading to $ \sum_{i}\expval{n_{i}} = N $. Comparing this constraint to the expression for $ \expval{N} $, we define
	\begin{equation*}
		\expval{n_{i}} =  \frac{1}{1 + e^{\beta (E_{i} - \mu)}}
	\end{equation*}
	This is the \textit{Fermi-Dirac distribution} and gives the average number of particles in the state $ \ket{i} $ in an ideal monatomic Fermi gas. It differs from the Bose-Einstein distribution only in the sign of the one in the denominator, but this difference is enough to produce very different behavior between bosons and fermions.
	
	Finally, as for bosons, we note that typical values of particle number $ N \sim 10^{23} $ are close enough to the thermodynamic limit that we can safely neglect fluctuations of particle number about the average value and simply write $ N $ and $ n_{i} $.
\end{itemize}

\subsubsection{Ideal Fermi Gas}
We consider the simple case of non-interacting, non-relativistic particles with the dispersion relation $ E = \frac{\hbar^{2}k^{2}}{2m} $.

Note that the sum over $ i $ in the definition of $ \expval{N} $ and other quantities is impractical to work with. Instead, we tend to analyze fermion systems by changing to an integration over the density of states $ g(E) $, as discussed in the introduction to quantum gases. 
\begin{itemize}
	\item We first find the density of states for a Fermi gas. In general, fermion systems with spin $ s $ have a degeneracy factor
	\begin{equation*}
		g_{s} = 2s + 1
	\end{equation*}
	corresponding to projections of spin onto an external $ z $ axis. For an electron (spin $\frac{1}{2}$) gas, we have $ g_{s} = 2 $, corresponding to the spin up and spin down states.
	
	When we account for $ g_{s} $, the density of states for non-relativistic fermions is
	\begin{equation*}
		g(E) =  \frac{g_{s}V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \sqrt{E}
	\end{equation*}
	
	
	\item Recall that it is generally more convenient to replace a summation over eigenstates with an integration over the density of states with a procedure of the form
	\begin{equation*}
		\sum_{n} (\ldots) \to \int (\ldots) g(E) \diff E 
	\end{equation*}
	Applying this trick to the average number of particles $ \expval{N} $ in a Fermi gas we get
	\begin{equation*}
		\expval{N} = \sum_{i} \frac{1}{1 + e^{\beta (E_{i} - \mu)}} \approx \int  \frac{g(E)}{1 + e^{\beta (E - \mu)}}  \diff E 
	\end{equation*}
	
	\item The Fermi gas's average energy is (using the basic definition of an average quantity)
	\begin{equation*}
		 \expval{E} = \sum_{i} \expval{n_{i}} E_{i}
	\end{equation*}
	where $ n_{i} $, the average number of particles in the state $ \ket{i} $, is given by the Fermi-Dirac distribution and $ E_{i} $ is the energy of the state $ \ket{i} $. In terms of the density of states, we have
	\begin{equation*}
		\expval{E} = \sum_{i}  \frac{E_{i}}{1 + e^{\beta (E_{i} - \mu)}} \\\approx \int  \frac{E }{1 + e^{\beta (E - \mu)}}  g(E) \diff E 
	\end{equation*}
	
	\item Using the convenient grand canonical potential $ pV = - \Phi = k_{B}T \ln \mathcal{Z} $ and the grand canonical partition function for a Fermi gas, the gas's pressure is
	\begin{equation*}
		pV = k_{B}T \ln \mathcal{Z} = k_{B}T \ln \left[\prod_{i} (1 + e^{-\beta(E_{i} - \mu)})\right] = k_{B}T \sum_{i} \ln(1 + e^{-\beta(E_{i} - \mu)})
	\end{equation*}
	In terms of the density of states, we have
	\begin{equation*}
		pV = k_{B}T \int \ln(1 + e^{-\beta(E - \mu)}) g(E)\diff E
	\end{equation*}
	In fact, using integration by parts, the last equation can be written
	\begin{equation*}
		pV = \frac{2}{3} \expval{E}
	\end{equation*}
	\textit{Derivation:} Choosing $ u = \ln(1 + e^{-\beta(E_{i} - \mu)}) g(E) $ and $ \diff V = \diff E $ we have
	\begin{equation*}
		\diff u = \frac{- \beta g(E) e^{-\beta(E - \mu)}}{1 + e^{-\beta(E - \mu)}} + \frac{g(E)}{2E}\ln(1 + e^{-\beta(E - \mu)})  \qquad \text{and} \qquad v = E
	\end{equation*}
	The second term in the expression for $ \diff u $ comes uses the identity $ g'(E) = \frac{g(E)}{2E} $. 
	
	First, we define for shorthand 
	\begin{equation*}
		I \equiv \int \ln(1 + e^{-\beta(E - \mu)}) g(E)\diff E
	\end{equation*}
	Applying the integration by parts formula $ \int u \diff v = uv - \int v\diff u $ and integrating $ E $ from  $ 0 $ to $ \infty $ we get
	\begin{equation*}
		pV \equiv k_{B}T I = k_{B}T \left[\ln(1 + e^{-\beta(E - \mu)})Eg(E)\right]_{0}^{\infty} + \int \frac{e^{-\beta(E - \mu)} E g(E)}{1 + e^{-\beta(E - \mu)}} \diff E - \frac{k_{B}T}{2}I
	\end{equation*}
	Note that the integral $ I $ conveniently appears on both sides of the equation. Just as conveniently, the $ uv $ term $  k_{B}T[\cdots]_{0}^{\infty} $ is zero at both $ 0 $ and $ \infty $ and vanishes from the equality (try checking the limit yourself). Rearranging the equation and multiplying the numerator and denominator by $ e^{\beta(E - \mu)} $ gives
	\begin{equation*}
		\frac{3k_{B}T}{2}I = \int \frac{e^{-\beta(E - \mu)} E g(E)}{1 + e^{-\beta(E - \mu)}} \diff E = \int \frac{E g(E)}{1 + e^{\beta(E - \mu)}} \diff E
	\end{equation*}
	The last integral is none other than the average energy $ \expval{E} $. The equality shows $ I = \frac{2}{3} \frac{\expval{E}}{k_{B}T} $ from which follows
	\begin{equation*}
		pV = k_{B}T I = \frac{2}{3} \expval{E}
	\end{equation*}	
	
\end{itemize}

\subsubsection{Fermi Sphere and Fermi Energy}
\begin{itemize}
	\item In the low-temperature limit $ T \to 0 $, the Fermi-Dirac distribution simplifies to
	\begin{equation*}
		\lim_{T \to 0} \expval{n_{i}} = \lim_{T \to 0} \frac{1}{e^{\beta(E - \mu)} + 1} = 
		\begin{cases}
			1 & E < \mu\\
			0 & E > \mu
		\end{cases}
	\end{equation*}
	All states with energy less than $ \mu $ are filled and all states with energy less than $ \mu $ are empty. At $ T \to 0 $, all fermions we add to the system settle into the lowest available (unoccupied) energy state. The energy of the last filled state is called the \textit{Fermi energy} and is denoted by $ E_{F} $. Mathematically, $ E_{F} $ is the value of the chemical potential at $ T = 0 $. 
	
	\item When we fill energy states in an ideal fermion system with fermions, the energy states of free particles are localized in momentum space. Successive fermions occupy states of ever-increasing momentum, and the fermion system forms a sphere in momentum space called the \textit{Fermi sphere}. 
	
	The momentum of the last (highest energy) fermion is called the \textit{Fermi momentum}. The Fermi momentum is usually given as a wave vector via the relationship $ p = \hbar k_{F} $:
	\begin{equation*}
		p_{F} = \hbar k_{F} = \sqrt{2mE_{F}}
	\end{equation*}
	For quantum systems we typically work with the wave vector $ k $ instead of momentum. In this convention, all states in a fermion system with $ \abs{k} \leq k_{F} $  are filled an lie in the Fermi sphere. The states with $ \abs{k} = k_{F} $ lie on the edge of the Fermi sphere and form the Fermi surface. Keep in mind that the Fermi surface place a central role in many areas of quantum statistical and solid states physics, but most of these topics are beyond the scope of these notes.
	
	\item It best to define the Fermi energy in terms of the number of particles $ N $ in the fermion system. To do this, we take advantage of the fact that at $ T \to 0 $, only states with $ E \leq E_{F} $ are occupied. As mentioned in the first bullet, this means the Fermi-Dirac distribution $ \expval{n_{i}} = \frac{1}{e^{\beta(E - \mu)}+1}$ is one for $ E \in [0, E_{F}] $, greatly simplifying the integration. Using the density of states definition of $ N $ we have
	\begin{align*}
		N &= \int_{0}^{\infty}\expval{n_{i}}g(E) \diff E = \int_{0}^{E_{F}} (1) \cdot g(E) \diff E = \int_{0}^{E_{F}} \frac{g_{s}V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \sqrt{E} \diff E \\
		&= \frac{g_{s}V}{6\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} E_{F}^{3/2}
	\end{align*}
	Solving the equality for $ E_{F} $ gives
	\begin{equation*}
		E_{F} = \frac{\hbar^{2}}{2m} \left(\frac{6\pi^{2}N}{g_{s}V}\right)^{2/3}
	\end{equation*}
	
	\item The Fermi energy sets the energy scale for a Fermion system. We can define an corresponding temperature scale in terms of the \textit{Fermi temperature}
	\begin{equation*}
		T_{F} = \frac{E_{F}}{k_{B}}
	\end{equation*}
	Temperatures with $ T > T_{F} $ are considered high-temperature for fermion systems, while temperatures $ T < T_{F} $ are considered low-temperature. Note that Fermi temperatures are often quite high by everyday standards. For instance, $ T_{F} \sim \SI{e4}{\kelvin} $ for electrons in a metal.
	
	\item In the low temperature limit $ T \to 0 $ where $ E_{F} $ is the energy of the last occupied state the average energy of a fermion system is
	\begin{equation*}
		\expval{E} = \int_{0}^{E_{F}} E g(E) \diff E =  \frac{g_{s}V}{10\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} E_{F}^{5/2} = \frac{3}{5}NE_{F}
	\end{equation*}
	Using the relationship $ pV = \frac{2}{3}\expval{E} $, we can  write $ pV  = \frac{2}{5} N E_{F} $ for a fermion gas at $ T \to 0 $. This deserves a second look: even at zero temperature a Fermi gas has non-zero pressure! This remarkable result, sometimes called \textit{degeneracy pressure} is the result of the Pauli exclusion principle and is at completely at odds with the behavior of a classical ideal gas.
	
\end{itemize}

\subsubsection{A Few Important Results Given Without a Full Derivation}
\begin{itemize}
	\item At high temperature, the equation of state for a Fermi gas, including the first quantum correction, is
	\begin{equation*}
		pV = Nk_{B}T\left(1 + \frac{\lambda^{3}N}{4\sqrt{2}g_{s}V} + \cdots \right)
	\end{equation*}
	where $ \lambda $ is the thermal de Broglie wavelength.	Note that in the limit $ T \to \infty $ we have $ \lambda \to 0 $, and the equation of state reduces to the ideal gas law $ pV = Nk_{B}T $. The first quantum correction increases the pressure and can be thought of as a result of the Pauli exclusion principle.
	
	\item The heat capacity of a Fermi gas is linearly proportional to $ T $ and is often written
	\begin{equation*}
		C_{V} \sim N k_{B} \frac{T}{T_{F}}
	\end{equation*}
	Note that $ C_{V} $ is also proportional to $ N $, as expected, and that the Fermi temperature $ T_{F} $ sets the temperature scale for the system. 
	
	\item Fermion contribution to heat capacity plays an important role in the heat capacity of metals, where we have two contributions: a phonon contribution (see earlier) scaling as $ T^{3} $ and a conduction electron (fermion) contribution scaling as $ T $. Conduction electrons are those that are free to move through the metal's crystal lattice; surprisingly, even though these electrons feel a Coulomb interaction, the treating them as an ideal Fermi gas gives an accurate result. 
	
	Considering both the phonon and conduction electron term, the heat capacity of a metal can be written
	\begin{equation*}
		C_{V} = \alpha T + \beta T^{3}
	\end{equation*}
	where $ \alpha T $ is the electron (Fermi gas) contribution. In practice, the phonon and electron contributions are comparable only at temperatures below $ \sim \SI{1}{\kelvin} $.
	
	\item Mathematical aside: we will use something called a \textit{polylogarithm function} function to concisely express the $ \expval{N} $ and $ \expval{E} $ for a Fermi gas. The polylogarithm functions are a family of parameter-dependent integrals that occur frequently in quantum statistics. The polylogarithm are given by
	\begin{equation*}
		\mathrm{Li}_{n}(z) = \frac{1}{\Gamma(n)} \int_{0}^{\infty} \frac{x^{n-1}}{z^{-1}e^{x}+1} \diff x
	\end{equation*}
	where $ \Gamma(n) $ is the \textit{Gamma function} 
	\begin{equation*}
		\Gamma(z) = \int_{0}^{\infty} x^{z-1}e^{-z}\diff z
	\end{equation*}
	For the purpose of this section, all we need to know about the Gamma function is $ \Gamma(\frac{3}{2}) = \frac{\sqrt{\pi}}{2} $ and $ \Gamma(\frac{5}{2}) = \frac{3\sqrt{\pi}}{4} $.

	Later, in the discussion of magnetism, we will use the small argument approximation
	\begin{equation*}
		\mathrm{Li}_{3/2}(z) \approx z \qquad (z \ll 1)
	\end{equation*}
	and also the large-argument approximation
	\begin{equation*}
		\mathrm{Li}_{n}(z) \approx \frac{(\ln z)^{n}}{\Gamma(n+1)} 
	\end{equation*}
	Both are given without derivation.
	
	\item Recall the average occupation number for a low-temperature Fermi gas is
	\begin{equation*}
		\expval{N} = \int  \frac{g(E)}{1 + e^{\beta (E - \mu)}}  \diff E = \frac{g_{s}V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \int_{0}^{\infty}\frac{E^{1/2}}{e^{-\beta \mu} e^{\beta E} + 1} \diff E 
	\end{equation*}
	The last integral can be conveniently expressed as polylogarithm function
	\begin{equation*}
		\expval{N} = \frac{g_{s}V}{\lambda^{3}}\mathrm{Li}_{3/2}(e^{\beta \mu})
	\end{equation*}
	where $ \lambda $ is the familiar thermal de Broglie wavelength $ \lambda = \sqrt{\frac{2\pi \hbar^{2}}{mk_{B}T}} $.
	
	\textit{Derivation}: checking the definition of $ \mathrm{Li}_{n}(z) $ for reference, we choose $ n = \frac{3}{2} $ and $ z = e^{\beta E} $. To create the necessary $ x = \beta E $, we multiply the integrand above and below by $ \beta^{1/2} $ and change integration via $ \diff E =  \frac{\diff[\beta E]}{\beta}  $, which generates the $ (k_{B}T)^{3/2} $ term needed for the thermal de Broglie wavelength $ \lambda $.
	
	Manipulating the outside constants from the density of states into the thermal de Broglie wavelength $ \lambda^{3} $ conveniently generates the necessary $ \frac{1}{\Gamma(3/2)} = \frac{2}{\sqrt{\pi}} $ to complete $ \mathrm{Li}_{3/2}(e^{\beta \mu}) $ with $ x = \beta E $.
	
	\item Following an analogous procedure, the average energy can be written
	\begin{equation*}
		\expval{E} = \frac{g_{s}V}{4\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} \int_{0}^{\infty}\frac{E^{3/2}}{z^{-1}e^{\beta (E - \mu)} + 1} \diff E = \frac{3}{2}\frac{g_{s}V}{\lambda^{3}\beta}\mathrm{Li}_{5/2}(e^{\beta \mu})
	\end{equation*}
		

\end{itemize}

\subsubsection{Brief Section of Magnetism via Fermion Statistics}
Following is a brief introduction to magnetism from a quantum statistical approach. Keep in mind that this section barely scratches the surface of what is out there!
\begin{itemize}
	\item Next, we consider an electron gas in an external magnetic field. The magnetic field introduces two important effects: the Lorentz force $ \bm{F} = \bm{v} \cross \bm{B} $ and the coupling of electron spin to the magnetic field. We will only discuss spin coupling in this section.
	
	\item An electron (a spin $ \frac{1}{2} $ fermion) can only have two spin states, spin up or spin down. We will label the spin up state with $ s = 1 $ and the spin down state with $ s = -1 $. In an external magnetic field, the electron's energy picks up the additional term
	\begin{equation*}
		E_{B} = \mu_{B} s B
	\end{equation*}
	from the interaction of the electron's magnetic moment with the magnetic field. The constant $ \mu_{B} = \frac{e_{0}\hbar}{2m} $ is the Bohr magneton and is unrelated to chemical potential.
	
	\item An important note: because the spin up and spin down electrons have different energies, their partition functions, which involve the term $ e^{-\beta E} $ will also be different, as will all the quantities derived from the partition function. 
	
	We separately consider the number of spin up and spin down particles, which we will label $ N_{\uparrow} $ and $ N_{\downarrow} $ and describe in terms of the polylogarithm, as in the earlier expression
	\begin{equation*}
		\expval{N} = \frac{g_{s}V}{\lambda^{3}}\mathrm{Li}_{3/2}(e^{\beta \mu})
	\end{equation*}
	We have to simply account for the extra energy term and replace $ e^{\beta(E - \mu)} $ with $ e^{\beta(E + \mu_{B}sB) - \mu} $ in the Fermi-Dirac distribution. We get
	\begin{align*}
		&\expval{N_{\uparrow}} = \int  \frac{g(E)}{1 + e^{\beta (E + \mu_{B}B - \mu)}}  \diff E = \frac{V}{\lambda^{3}}\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right )\\
		&\expval{N_{\downarrow}} = \int  \frac{g(E)}{1 + e^{\beta (E - \mu_{B}B - \mu)}}  \diff E =  \frac{V}{\lambda^{3}}\mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )
	\end{align*}
	Note the I have plugged in $ g_{s} = 1 $ for the spin up and spin down states (an electron that can be in \textit{either} spin up or spin down has $ g_{s} = 2$, but the fixed up or down states themselves have $ g_{s} = 1 $).
	
	\item Next, we turn to the system's magnetization, which measures how the system's energy responds to an external magnetic field. Magnetization $ M $ is defined as
	\begin{equation*}
		M = - \pdv{E}{B}
	\end{equation*}
	Inserting the magnetic field energy $ E_{B} $ leads to $ M = - \mu_{B}s $ where $ s $ is the system's spin. For a system of $ N $ particles, $ s $ is the difference between the number of states with spin up and spin down, so the total magnetization is
	\begin{equation*}
		M = - \mu_{B}(N_{\uparrow} - N_{\downarrow}) = - \frac{\mu_{B}V}{\lambda^{3}} \left[\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right ) - \mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )\right]
	\end{equation*}
	 
	\item In the high temperature limit (for suitably low magnetic fields) $ e^{\beta(\mu \pm \mu_{B}B)} \ll 1$. We can then make the small-argument approximation $ \mathrm{Li}_{3/2}(z) \approx z $ to get
	\begin{equation*}
		M \approx - \frac{\mu_{B}V}{\lambda^{3}} \left(e^{\beta(\mu - \mu_{B}B)} - e^{\beta(\mu + \mu_{B}B)}\right) =  \frac{2\mu_{B}V}{\lambda^{3}}e^{\beta \mu} \sinh(\beta \mu_{B}B)
	\end{equation*}
	To get to an important result, we will re-write this equality in terms of total particle number $ N $. Making use of $ \mathrm{Li}_{3/2}(z) \approx z $, we have
	\begin{align*}
		N &= N_{\uparrow} + N_{\downarrow} = \frac{V}{\lambda^{3}}\left[\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right ) + \mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )\right]\\
		&\approx \frac{V}{\lambda^{3}}\left(e^{\beta(\mu - \mu_{B}B)} + e^{\beta(\mu + \mu_{B}B)} \right) = \frac{2V}{\lambda^{3}}e^{\beta \mu} \cosh(\beta\mu_{B}B)
	\end{align*}
	We can invert the equality to get $ e^{\beta \mu} = \frac{N\lambda^{3}}{2V} \frac{1}{\cosh(\beta\mu_{B}B)} $ from which follows
	\begin{equation*}
		M \approx \mu_{B} N \tanh(\beta\mu_{B}B)
	\end{equation*}
	
	\item As $ T \to \infty $ we have $ \beta \mu_{B}B \to 0 $, and we can safely make the small-argument approximation $ \tanh x \approx x $, which leads to
	\begin{equation*}
		M \approx \frac{N\mu_{B}^{2}}{k_{B}} \frac{B}{T} \qquad (\text{high } T)
	\end{equation*}
	The inverse proportionality of magnetization to temperature is called \textit{Curie's law} and is valid in the high temperature limit where quantum effects are not important. 
	
	\item Curie's law is often given in terms of magnetic susceptibility $ \chi $, defined as
	\begin{equation*}
		\chi = \pdv{M}{B}
	\end{equation*}
 	Magnetic susceptibility measures how easy it is to magnetize a substance. In terms of susceptibility, Curie's law reads
 	\begin{equation*}
	 	\chi \approx \frac{N\mu_{B}^{2}}{k_{B}T}
 	\end{equation*}
 	where the $ \frac{1}{T} $ proportionality is the characteristic feature.
 	
 	\item In the low temperature limit, quantum effects become important in the analysis of fermion magnetism. Recall the general expression for magnetization:
	\begin{equation*}
		M = - \frac{\mu_{B}V}{\lambda^{3}} \left[\mathrm{Li}_{3/2}\left (e^{\beta(\mu - \mu_{B}B)}\right ) - \mathrm{Li}_{3/2}\left (e^{\beta(\mu + \mu_{B}B)}\right )\right]
	\end{equation*}
	Using the first-order large-argument approximation $ f_{n}(z) \approx \frac{(\ln z)^{n}}{\Gamma(n+1)} $ with $ n = \frac{3}{2} $ and $ z =  e^{\beta(\mu \pm \mu_{B}B)} $ and making the low-temperature approximation $ \mu \approx E_{F} $ leads to
	\begin{equation*}
		M \approx \frac{\mu_{B}V}{6\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2}\left[(E_{F} + \mu_{B}B)^{3/2} - (E_{F} - \mu_{B}B)^{3/2}  \right]
	\end{equation*}
	In the low magnetic field regime $ \mu_{B}B \ll E_{F} $, we can expand $ (E_{F} \pm \mu_{B}B)^{3/2} $ to first order in $ B $ using general first-order approximation $ f(x + \delta x) \approx x + f'(x) \delta x $ with $ x = E_{F} $ and $ \delta x = \pm \mu_{B}B $. This gives
	\begin{equation*}
		(E_{F} \pm \mu_{B} B)^{3/2} \approx E_{F} \pm \frac{3}{2} E_{F}^{1/2} \mu_{B}B
	\end{equation*}
	From which follows
	\begin{equation*}
		M \approx \frac{\mu_{B}^{2}V}{2\pi^{2}} \left(\frac{2m}{\hbar^{2}}\right)^{3/2} B E_{F}^{1/2}
	\end{equation*} 
	In terms of the electron density of states $ g(E) = \frac{V}{2\pi^{2}}\left(\frac{2m}{\hbar^{2}}\right)^{3/2}E^{1/2}$ (we've plugged in $ g_{s} = 2 $) the magnetization reads
	\begin{equation*}
			M \approx \mu_{B}^{2} g(E_{F}) B
	\end{equation*}
	Note that at low temperatures the Curie's law $ \frac{1}{T} $ proportionality is gone. Likewise, magnetic susceptibility saturates to a constant value
	\begin{equation*}
		\chi = \pdv{M}{B} = \mu_{B}^{2} g(E_{F})
	\end{equation*}
	As a final note, materials with $ \chi > 0 $ are called \textit{paramagnetic}; their magnetism increases in the presence of a magnetic field. 
\end{itemize}

\section{Classical Thermodynamics}
\textit{Warning: the quality of the writing and formulation drops off a bit in this section. I was statistical physic-ed out by the time I got to here.}
\subsection{Temperature and the Zeroth Law}
\subsubsection{Some Definitions}
\begin{itemize}
	\item An \textit{adiabatic} process is a process that runs without the flow of heat (a mode of energy transfer, to be defined shortly).
	
	\item A system that is completely isolated from all outside influences is said to be \textit{isolated}. Such a system is said to be contained in \textit{adiabatic walls}.
	
	\item A system is \textit{thermally isolated} if heat cannot flow between between the system and its surroundings. 
	
	\item A system that is mechanically but not thermally isolated is said to be contained in \textit{diathermal walls}. A diathermal wall cannot move and does not allow the transfer of particles between systems, but it does allow heat to flow between systems. A good real-life example is a thin sheet of metal.
	
	\item Classical thermodynamics (which predates a theory of microscopic particles) use macroscopic variables to describe a system. The macroscopic variables need to completely specify a system's state are called \textit{state variables}. If a system's state variables are specified, all other relevant quantities can be determined from the values of the state variables.
	
	For a gas, only two state variables are needed to completely describe the system: pressure and volume. State variables commonly come in intensive-extensive pairs. For gas, pressure is intensive and volume is extensive. For a magnet, the state variables are magnetic field (intensive) and magnetization (extensive); for a dielectric, the state variables are electric field (intensive) and electric polarization (extensive); for a film: surface tension (intensive) and area (extensive).  
	
	\item Secondary quantities that are functions of the state variables are called \textit{functions of state}. Some common functions of state we will encounter shortly are temperature, internal energy and entropy.
	
	\item An isolated system left alone for a suitably long time will relax into a constant state in which no further change is noticeable. This state is called \textit{equilibrium}; it is the steady state of an isolated system. 
	
	A system is in equilibrium when its state variables are constant with respect to time.
\end{itemize}

\subsubsection{The Zeroth Law}
The zeroth, first, and second law all lead to the definitions for new functions of state. We begin with the zeroth law, which forms a basis for the definition of temperature.
\begin{itemize}
	\item The zeroth law states:
	\begin{quote}
		If two systems $ A $ and $ B $ are both in equilibrium with a third system $ C $, then $ A $ and $ B $ are also in equilibrium with each other.
	\end{quote}
	Essentially, the zeroth law states that equilibrium is a transitive property.
	
	\item  How does the zeroth law lead to temperature? 
	
	We will assume $ A $ and $ B $ are both separately in equilibrium with $ C $, and apply the zeroth law to know that $ A $ and $ B $ are also in equilibrium with each other.
	
	To be concrete, we will assume our system is an ideal gas, so our state variables are $ (p, V) $. Suppose system $ A $ is in state $ (p_{A}, V_{A}) $ and system $ C $ in state $ p_{C}, V_{C} $.
	
	\item First, we consider $ A $ and $ C $: If we fix the variables $ p_{A}, V_{A} $ and $ p_{C} $, there is a particular value of $ V_{C} $ for which nothing happens when systems $ A $ and $ C $ are brought together, i.e. both systems remain in equilibrium. 
	
	We can introduce a constraint $ F $ on systems $ A $ and $ C $ that determines when $ A $ and $ C $ are in equilibrium:
	\begin{equation*}
		F_{AC}(p_{A}, V_{A}; p_{C}, V_{C}) = 0
	\end{equation*}
	Given $ p_{A}, V_{A} $ and $ p_{C} $, we can solve this constraint for the equilibrium volume $ V_{C} $ to get
	\begin{equation*}
		V_{C} = f_{AC}(p_{A}, V_{A}; p_{C})
	\end{equation*} for some function $ f_{AC} $.
	
	\item We apply an analogous argument to systems $ B $ and $ C $, which are also in equilibrium. We get the equilibrium constraint
	\begin{equation*}
		F_{BC}(p_{B}, V_{B}; p_{C}, V_{C}) = 0
	\end{equation*}
	If we fix $ p_{B}, V_{B} $ and $ p_{C} $, we can solve for
	\begin{equation*}
		V_{C} = f_{BC}(p_{B}, V_{B}; p_{C})
	\end{equation*}
	Equating the two expressions for volume $ V_{C} $ gives
	\begin{equation*}
		f_{AC}(p_{A}, V_{A}; p_{C}) = f_{BC}(p_{B}, V_{B}; p_{C})
	\end{equation*}
	Next, we apply the zeroth law, which tells us that $ A $ and $ B $ are also in equilibrium with each other. We get the familiar constraint
	\begin{equation*}
		F_{AB}(p_{A}, V_{A}; p_{B}, V_{B}) = 0
	\end{equation*}
	and the transitive nature of equilibrium from the zeroth law implies this must be equivalent to the earlier equality
	\begin{equation*}
		f_{AC}(p_{A}, V_{A}; p_{C}) = f_{BC}(p_{B}, V_{B}; p_{C})
	\end{equation*}
	The equivalence of the last two constraints via the zeroth law gives the important result. Because $ p_{C} $ does not appear anywhere in $ F_{AB}(p_{A}, V_{A}; p_{B}, V_{B}) = 0 $, it must occur in $ f_{AC}(p_{A}, V_{A}; p_{C}) = f_{BC}(p_{B}, V_{B}; p_{C}) $ in such a way that it cancels from both sides of the equality. In this case (once $ p_{C} $ is canceled), all trace of system $ C $ disappears from both $ f_{AC} $ and $ f_{BC} $. This means that (as long as we have equilibrium) there is a relationship between $ A $ and $ B $ of the form
	\begin{equation*}
		\theta_{A}(p_{A}, V_{A}) = \theta_{B}(p_{B}, V_{B})
	\end{equation*}
	The value of $ \theta(p, V) $ is called the \textit{temperature} of the system, and the function $ T = \theta(p, V) $ is the corresponding equation of state.
	
	The zeroth law basically states that every system has a well-defined property $ \theta(p, V) $ that is a function of the state, which we call temperature.
	
	\item Note that the zeroth law does not define the specific form of the temperature $ T = \theta(p, V) $; it just says that the quantity exists. In the section on the second law of thermodynamics, we will see there is a canonical choice for the exact form of temperature. For now, we will simply pick a reference system to define temperature: the ideal gas, from which we get
	\begin{equation*}
		T = \frac{pV}{Nk_{B}}
	\end{equation*}
\end{itemize}

\subsection{The First Law}
\subsubsection{Statement of the First Law}
\begin{itemize}
	\item The first law of thermodynamics is a statement of the conservation of energy acknowledging that there is more than one way besides work (i.e. heat) to change the energy of a system. 
	
	Here is one formulation: 
	\begin{quote}
		The amount of work required to change an \textit{isolated} system between two states is independent of how the work is performed.
	\end{quote}
	The first law (in particular that energy change is \textit{independent} of how work is performed) tells us the system has another function of state, which we call energy. For a gas with state variables $ (p, V) $ we have $ E = E(p, V) $. By the first law, when we perform work $ W $ on an \textit{isolated} system, the change in energy $ \Delta E $ is the same regardless of how the work is performed.
	
	\item If the system is \textit{not} isolated, it can pass between states with different amounts of work done on the system, and the change in the system's energy is general not equal to the amount of work done. A simple example is heating a pot of water with a gas flame: the flame does not do work on the water, but the water's energy increases nonetheless.  
	
	We allow for the additional (non-work) mode of energy transfer by defining \textit{heat}. Heat is a mode of energy transfer that arises from temperature differences.  
	
	This leads to the equation form of the first law:
	\begin{equation*}
		\Delta E = Q + W
	\end{equation*}
	where $ \Delta E $ is the system's change in energy during some thermodynamic process and $ Q $ and $ W $ are the amount of energy transferred to the system as heat and the amount of work done on the system during that process, respectively.
	
	The first law links two state of a system by specifying the change in energy and modes of energy transfer. 
	
	Note that heat, like work, is not a \textit{type} of energy, like potential or kinetic energy, and a system cannot ``contain'' heat. Instead, heat is a mode of energy transfer. Because they are modes of energy \textit{transfer} and not properties of a system itself, neither heat nor work are state variables (while energy is).
	
	Often, in these notes and in the thermodynamics literature, you will find borderline correct  phrases like ``a system absorbs heat from its surroundings''. Formally, this means a system absorbs energy from its surroundings, with heat being the mode of energy transfer. But this is such a mouthful that we often just speak of absorbing or emitting heat with the tacit knowledge that heat is a form of energy transfer.
	

	
	\iffalse
	\item Fermi: The only assumption underlying the above empirical definition of the energy is that the total amount of work performed by the system during any transformation depends only on the initial and final states of the transformation. We have already noticed that if this assumption is contradicted by experiment, and if we still do not wish to discard the principle of the conservation of energy, then we must admit the existence of other methods, besides mechanical work, by means of which energy can be ex- changed between the system and its environment.
	
	\item We can take a system from state $ A $ to state $ B $ in multiple ways: water heating example. We thus see that the work performed by the system in going from the state A to the state B depends on whether we go by means of the first way or by means of the second way.
	
	If we assume that the principle of the conservation of energy holds for our system, then we must admit that the energy that is transmitted to the water in the form of the mechanical work of the rotating paddles in the second way is transmitted to the water in the first way in a non-mechanical form called heat.
	
	$ Q $ can be interpreted physically as the amount of energy that is received by the system in forms other than work.
	\fi
\end{itemize}

\subsubsection{Definitions; Quasi-Static Process and Improper Differentials}
\begin{itemize}
	\item A \textit{quasi-static} process is a thermodynamic process in which we transfer energy to or from a system slowly and gently enough that the system effectively remains in equilibrium and is well defined by its state variables throughout the process.
	
	\item It is useful to write first law equation $ \Delta E = Q + W $ in infinitesimal form for quasi-static processes. Recall that neither heat nor work are functions of state, while energy $ E(p, V) $ is. An infinitesimal change in energy is the total derivative
	\begin{equation*}
		\diff E = \pdv{E}{p}\diff p + \pdv{E}{V}\diff V
	\end{equation*}
	
	\item Meanwhile, an infinitesimal amount of work or heat is simply a small quantity, and has no interpretation as a total differential, since work and heat aren't functions of state. To make the distinction, we denote an infinitesimal quantities of work and heat, which aren't functions of state, by $ \dbar W $ and $ \dbar Q$ to distinguish them from energy, which is a function of state. 
	
	In this notation the first law reads
	\begin{equation*}
		\diff E = \dbar Q + \dbar W
	\end{equation*}
	
	\item In general, the first law of thermodynamics applies to all types of work (e.g. mechanical, electric, magnetic, etc...), which makes the law relevant across a wide range of physical systems.
	
	However, by focusing on a particular system we can be more specific about the form of work. By convention, the system of choice for (introductory) thermodynamics is an ideal gas, so that's what we'll deal with here. For an ideal gas, only one type of work is really relevant: contraction (squeezing) or expanding. An infinitesimal amount of work done on the system by only squeezing or expanding can be written
	\begin{equation*}
		\dbar W = - p \diff V
	\end{equation*}
	The negative sign is there to ensure that when the system is squeezed (i.e. decreasing its volume $ \implies \diff V < 0 $ ) the work done on the system is positive. If the system expands, $ \diff V > 0 $ and we expect the system to be doing work on its surroundings (equivalent to negative work $ \dbar W < 0 $ done on the system).
	
	\item Note that the definition of work $ \dbar W = - p \diff V $ underlines the fact that work is not a function of state, since it is impossible to construct a function of two variables $ W = W(p, V) $ whose total derivative is the single term $ \diff W = - p \diff V $.
	
	\item More on the function of state theme: If take our system between two states through two different quasi-static processes, the change in energy is independent of the path taken. This is because energy is a function of state and depends only on the initial and final state; we have simply $ \Delta E = E(p_{2}, V_{2}) - E(p_{1}, V_{1})  $.
	
	In contrast, the work done in general depends on the path taken, since $ - \int p \diff V $ is different for different paths. This is because $ W $ is not a function of state.
	
\end{itemize}

\subsection{The Second Law}
\subsubsection{Reversible Processes}
\begin{itemize}
	\item \textit{Reversible processes} are a special class of quasi-static process in which no energy is lost to friction. 
	
	A system undergoing a reversible process between two states must be in equilibrium at each point in the process (the quasi-static condition) and no energy can be lost to friction.
	
	\item Consider a reversible process taking a system on a round trip between two thermodynamic states, e.g. from state $ A $ to $ B $ back to $ A $. Because energy is a function of state and the initial and final states are the same (and no energy is lost to friction by definition of a reversible process), the total change in the system's energy during the complete cycle is zero: 
	\begin{equation*}
		\oint \diff E = E_{f} - E_{i} = E(p_{A}, V_{A}) - E(p_{A}, V_{A}) = 0
	\end{equation*}
	The first law $ \Delta E = Q + W $ then gives $ Q = - W $ or more formally $ \oint \dbar Q = \oint - \dbar W $: the heat absorbed by the system during the complete cycle equals the work done by the system  (i.e. the negative of the work done on the system) during the cycle.
	
	\item Note that the first law does not impose a preferred direction for the cycle; as far as conservation of energy is concerned, the system could absorb heat and do work, or have work done on it emit heat, depending on the direction of the cycle. 
	
	Processes that running in cycles of absorbing heat and performing work form the basis for e.g. combustion engines and a plethora of stuff that makes the industrial world work.
\end{itemize}

\subsubsection{Statement of the Second Law}
\begin{itemize}
	\item The second law of thermodynamics is usually given in two equivalent forms:
	\begin{quote}
		\textbf{Kelvin formulation:} No process is possible whose \textit{only} effect is to extract heat from a heat reservoir and convert it entirely into work.
	\end{quote}
	\begin{quote}
		\textbf{Clausius formulation:} No process is possible whose \textit{only} effect is heat transfer from a colder to a hotter body. Equivalently, heat flows from hotter to colder bodies.
	\end{quote}
	The goal of this section is to explain how these two statements of the second law allow the definition of a function of state with the same properties as entropy.
	
	\item How to prove the statements are equivalent? We start by showing that if the Kelvin formulation were invalid the Clausius postulate would be invalid as well.
	
	If the Kelvin formulation were invalid, we could perform a process who only result would be to transfer heat from a heat reservoir completely into work. We could then (e.g. with friction) completely transform this work back into heat and use the heat to arbitrarily increase the temperature of some given body. We could keep this up until the temperature of the body increased beyond the temperature of the heat reservoir. Once the $ T_{2} > T_{1} $ sole effect of the process is the transfer of heat from a colder body to a hotter body, in violation of Clausius's formulation. We will prove the converse after discussing the Carnot cycle.
	

\end{itemize}

\subsubsection{The Carnot Cycle}
\begin{itemize}
	\item First, let's clear something up. Recall that a reversible cycle allows the conversion of heat to work. This does \textit{not} violate the Kelvin statement; the key is in the small print ``only effect''. There is no reversible process whose \textit{sole} effect is the conversion of heat to work. That would violate the Kelvin statement. But its fine for a reversible process to convert heat to work and something else. That something else is heat: Every reversible process, besides performing work, must also emit some heat. 
	
	\begin{quote}
		Reversible processes take heat from a hot reservoir and do two things: perform some work and dump some waste heat somewhere else. 
	\end{quote} 
	
	\item By conservation of energy, the energy available to do work in a reversible process is the difference between the heat absorbed and the heat emitted. 
	
	We're now ready to discuss the famous \textit{Carnot cycle}. The Carnot cycle is a theoretical series of four reversible process running in a continuous cycle. The system operates between two heat reservoirs: a hot reservoir with temperature $ T_{H} $ and a cold reservoir with temperature $ T_{C} $. The four steps are
	\begin{enumerate}
		\item Isothermal (constant temperature) expansion at the hot temperature $ T_{H} $. The gas pushes against the sides of its container and is allowed to slowly (quasi-statically) expand, performing usable work in the process. To keep the temperature constant on account of increasing volume, the system absorbs heat $ Q_{H} $ from its surroundings. This is the main work step!
		
		\item Adiabatic (without heat flow) expansion: the gas continues to expand and because the gas is adiabatically isolated (e.g. can't absorb heat) pressure and temperature decrease on account of increasing volume. Temperature decreases from $ T_{H} $ to $ T_{C} $. The gas performs work, but not as much as in step one.
		
		\item Isothermal contraction at the cold temperature $ T_{C} $. We do work on the system and the volume decreases. The system emits heat $ Q_{H} $ to its surroundings to account for decreasing volume at constant temperature.
		
		\item Adiabatic contraction from $ T_{C} $ to $ T_{H} $, bringing the system back to its original state. We continue to do work on the system, the volume decreases. To account for decreasing volume without heat flow, the system's temperature and pressure increase. We return to our original state and repeat the process.
	\end{enumerate}
	More work is performed in the isothermal processes than in the adiabatic processes. All the steps are assumed to be performed reversibly.

	\item \textbf{TODO: EDIT} If you're dubious (as I first was), here's an excellent explanation of why the Carnot cycle is indeed reversible. I took it directly from \href{https://www.quora.com/Why-do-we-say-that-the-Carnot-cycle-is-reversible}{this Quora link}.
	
	Carnot deduced that the most efficient cycle possible would have to be reversible - and that meant that any heat transfer would have to occur at constant temperature, since heat only flows naturally from higher to lower temperatures. But if you could simulate a transfer of heat from a high temperature reservoir into the working fluid of the engine at the same temperature, it could in effect be reversible. And any heat transfer out of the heat engine would also have to be at constant temperature. The problem then became how do you get the system from the high temperature where energy is absorbed to the lower temperature where it is exhausted. His conclusion was that to get from the higher to lower temperature and then from the lower back to the higher temperature, those two steps would have to be in the absence of any heat transfer - otherwise, they could never be reversible. But a thermodynamic process that does not involve a heat transfer is called adiabatic.
	
	So the Carnot cycle process involves four steps:
	
	\begin{enumerate}
		\item 	Heat is absorbed isothermally from the higher T reservoir while the piston expands and does work.
		
		\item A further adiabatic expansion occurs (which also does work) to lower the temperature of the gas to that of the low temperature reservoir.
		
		\item The gas then is compressed isothermally by the piston which also rejects heat into the low temperature reservoir (and is reversible).
		
		\item And finally, another adiabatic compression occurs to raise the temperature back to that of the high temperature reservoir (but without any heat transfer during that step.
	\end{enumerate}

 
	
	\item At the end of the four steps, the system has returned to its original state. The net heat absorbed is $ Q_{H} - Q_{C} $ and the net work performed by the system is $ W $. As mentioned at the end of the first law section, conservation of energy implies
	\begin{equation*}
		Q_{H} - Q_{C} = W
	\end{equation*}
	(Since we're back in the original state and energy is a function of state, so $ \Delta E = 0 $).
	
	Naturally, we'd want to get as much work as possible with a minimum input of heat. With this in mind, we define the efficiency $ \eta $ of the engine as the ratio of the work done to the heat absorbed from the hot reservoir. Applying $ Q_{H} - Q_{C} = W $ we have
	\begin{equation*}
		\eta \equiv \frac{W}{Q_{H}} = \frac{Q_{H}-Q_{C}}{Q_{H}} = 1 - \frac{Q_{C}}{Q_{H}}
	\end{equation*}

	
	\item Conservation of energy and the first law imply a limit of $ \eta \leq 1$. $ \eta > 1 \implies W > Q_{H} $ would mean we get more energy out of the cycle from work than we inputted as heat. That's an obvious violation of conservation of energy! 
	
	 Ideally we would want to convert all the absorbed heat $ Q_{H} $ into work. Such a cycle would have $ \eta = 1 $, and its sole result would be the conversion of heat from a hot reservoir into work. This directly violates the Kelvin formulation of the second law. 
	 
	 We must content ourselves with depositing some heat $ Q_{C} $ back to the cold reservoir during each cycle. 
	 
	 \item In fact, the Carnot cycle is the most efficient way to convert heat to work. This is formally stated in \textit{Carnot's theorem:}
	\begin{quote}
	 	Of all engines operating between two heat reservoirs, a reversible Carnot engine is the most efficient. More so, all reversible engines have the same efficiency $ \eta = \eta (T_{H}, T_{C}) $, which depends only on the temperatures of the reservoirs $ T_{H} $ and $ T_{C} $.
	\end{quote}
	We can prove this theorem using the Clausius formulation of the second law. 
	\begin{itemize}
		\item Consider Carnot engine and a second engine, both operating between the same two heat reservoirs at temperatures $ T_{H} $ and $ T_{C} $. We run the Carnot and the second engine so they both perform the work $ W $ and assume the Carnot engine is reversible while the second engine is not; we'll call the non-reversible engine ``NRE''. 
		
		\item 	Suppose the NRE absorbs heat $ Q_{H}' $  from the hot reservoir and deposits heat $ Q_{C}' $ into the cold reservoir. Next, assume we use the NRE to run the Carnot engine in reverse: instead of taking in heat $ Q_{H} $ from the hot reservoir and performing work $ W $ while emitting heat $ Q_{C} $ to the cold reservoir, the Carnot engine run in reverse takes in work $ W $ from the NRE and heat $ Q_{C} $ from the cold reservoir and emits heat $ Q_{H} $ to the hot reservoir. The quantities $ W, Q_{H} $ and $ Q_{C} $ have the same values as if the Carnot engine were running normally, but their direction is reverse.
		
		\item The effect of the two engines with the surroundings is to extract the net heat $ Q_{H}' - Q_{H} $ from the hot reservoir and emit net heat $ Q_{C}' - Q_{C} $ to the cold reservoir. 
		
		The first law (conservation of energy) tells us that $ Q_{C}' - Q_{C} = Q_{H}' - Q_{H}$, i.e. the energy leaving the system equals the energy coming in, while the Clausius formulation of the second law tells us that $  Q_{H}' \geq Q_{H}  $, i.e. net heat must move from the hotter reservoir to the colder reservoir, and not in reverse. 
		
		\item Applying $ Q_{C}' - Q_{C} = Q_{H}' - Q_{H} $ and then $ Q_{H}' \geq Q_{H} $ give us
		\begin{equation*}
			\eta_{\text{NRE}} \equiv 1 - \frac{Q_{C}'}{Q_{H}'} = \frac{Q_{H} - Q_{C}}{Q_{H}'} \leq \frac{Q_{H} - Q_{C}}{Q_{H}} \equiv \eta_{\text{Carnot}}
		\end{equation*}
		which implies $ \eta_{\text{NRE}} \leq \eta_{\text{Carnot}} $
		
		\item For the second part of the theorem: If the NRE were reversible, we could repeat the argument to imply $ \eta_{\text{NRE}} \geq \eta_{\text{Carnot}} $, which together with the result $ \eta_{\text{NRE}} \leq \eta_{\text{Carnot}}  $ implies $ \eta_{\text{NRE}} = \eta_{\text{Carnot}}  $. This means that all reversible engines operating between $ T_{H} $ and $ T_{C} $ have the same efficiency, or that the ratio $ \frac{Q_{H}}{Q_{C}} $ is the same for all reversible engines  operating between $ T_{H} $ and $ T_{C} $. Because $ T_{H} $ and $ T_{C} $ are the only variables involved in the proof, if follows that the efficiency of a reversible Carnot engine is a function of only $ T_{H} $ and $ T_{C} $, i.e. $ \eta_{Carnot} = \eta(T_{H}, T_{C}) $.
		
	\end{itemize}
	

\end{itemize}

\subsubsection{Thermodynamic Temperature Scale}
\begin{itemize}
	\item Recall that zeroth law of thermodynamics forms a basis for the function of state we call temperature which has the same value for any two systems in equilibrium. The zeroth law did not, however give the form temperature function; it only stated that such a function of state exists. We then turned to the ideal gas a reference system to define $ T = \frac{pV}{Nk_{B}} $.
	
	\item Because the Carnot cycle depends only the reservoir temperatures $ T_{H} $ and $ T_{C} $, we can use the Carnot cycle to define a more universal temperature scale independent of any specific material.
	
	\item The key is to consider two Carnot engines. The first operates between two reservoirs with temperatures $ T_{1} > T_{2} $ and the second operates between reservoirs with temperatures $ T_{2} > T_{3} $. 
	
	If the first engine extracts heat $ Q_{1} $ from the hot reservoir it must emit heat $ Q_{2} $. From the conservation of energy $ W = Q_{1} - Q_{2} $ and definition of efficiency $ \eta = \frac{W}{Q_{1}} $ we have
	\begin{equation*}
		Q_{2} = Q_{1}(1 - \eta(T_{1}, T_{2}))
	\end{equation*}
	where we stress that efficiency is a function only the reservoir temperatures $ T_{1} $ and $ T_{2} $.
	
	\item The second engine operating between $ T_{2} $ and $ T_{3} $ absorbs the heat $ Q_{2} $ and emits the heat $ Q_{3} $; we have
	\begin{equation*}
		Q_{3} = Q_{2}(1 - \eta(T_{2}, T_{3})) = \left[Q_{1}(1 - \eta(T_{1}, T_{2}))\right](1 - \eta(T_{2}, T_{3}))
	\end{equation*}
	where we have plugged in $ Q_{2} = Q_{1}(1 - \eta(T_{1}, T_{2})) $ in the second equality.
	
	\item We can also consider both engines together as a combined Carnot engine operating between $ T_{1} $ and $ T_{3} $. Such an engine has $ W = Q_{1} - Q_{3} $ and efficiency $ \eta(T_{1}, T_{3}) $ so that
	\begin{equation*}
		Q_{3} = Q_{1}(1 - \eta(T_{1}, T_{3}))
	\end{equation*}
	Equating the two expressions for $ Q_{3} $ and canceling $ Q_{1} $ 
	\begin{equation*}
		1 - \eta(T_{1}, T_{3}) = (1 - \eta(T_{1}, T_{2}))(1 - \eta(T_{2}, T_{3}))
	\end{equation*}
	The key is recognizing that the left hand side is a function of only $ T_{1} $ and $ T_{3} $ (and not $ T_{2} $), so $ T_{2} $ must cancel in the right hand side. 
	
	\item To get $ T_{2} $ to cancel from the right side, the first and second terms must be functions of the form 
	\begin{equation*}
		1 - \eta(T_{1}, T_{2}) = \frac{f(T_{2})}{f(T_{1})} \qquad \text{and} \qquad 1 - \eta(T_{2}, T_{3}) = \frac{f(T_{3})}{f(T_{2})} 
	\end{equation*}
	for some function $ f(T) $ so that their product $ \frac{f(T_{2})}{f(T_{1})}  \frac{f(T_{3})}{f(T_{2})} = \frac{f(T_{3})}{f(T_{1})}$ is not a function of $ T_{2} $, as desired. The function $ f(T) $ is the temperature of a thermodynamic system. We could choose any monotonic function, but simplest and most natural choice is $ f(T) = T $. We have thus defined the thermodynamic temperature so that efficiency of the Carnot cycle is 
	\begin{equation*}
		\eta = 1 - \frac{T_{C}}{T_{H}} \implies  \frac{T_{C}}{T_{H}} = \frac{Q_{C}}{Q_{H}}
	\end{equation*}
	The scale $ \frac{T_{C}}{T_{H}} = \frac{Q_{C}}{Q_{H}} $ is sometimes called the \textit{thermodynamic temperature scale}.
\end{itemize}

\subsubsection{Comparing the Thermodynamic and Ideal Gas Temperature Scales}
\begin{itemize}
	\item Recall the ideal gas temperature scale arises from the equation of state of an ideal gas; temperature is defined as $ T = \frac{pV}{k_{B}T} $. It turns out the ideal gas definition is equivalent to the Carnot cycle definition formulated above. To show this, we will take an ideal gas through a complete Carnot cycle using the ideal gas temperature $ T = \frac{pV}{k_{B}T}  $ the whole way through, then show that using the ideal gas temperature scale recovers the Carnot engine efficiency $ \eta = 1 - \frac{T_{C}}{T_{H}} $, meaning the ideal gas and Carnot temperature scales are equivalent.
	
	\item We first analyze the isothermal processes in the Carnot cycle. From statistical physics, the energy of an three-dimensional ideal gas is $ E \frac{3}{2}N k_{B}T $. (In fact, all we need to know is that energy is a function of temperature only, i.e. $ E = E(T) $.)
	
	Because $ E = E(T) $, any isothermal process (for which $ \diff T = 0 $) has $ \diff E = 0 $. Plugging $ \diff E = 0 $ into the differential form of the first law gives $ \dbar Q = - \dbar W $ for the two isotherms in the Carnot cycle. We will label the four states of the Carnot cycle $ A, B, C $ and $ D $ (in the order described in the earlier list) to give labels for the limits of integration.
	
	\item For isothermal expansion in step 1 between states $ A $ and $ B $ at constant $ T_{H} $, using the ideal gas equation of state $ p =  \frac{Nk_{B}T}{V} $ we have
	\begin{equation*}
		Q_{H} \equiv \int_{A}^{B} \dbar Q = - \int_{A}^{B} \dbar W = \int_{A}^{B} p \diff V = \int_{V_{A}}^{V_{B}} \frac{Nk_{B}T_{H}}{V} \diff V = N k_{B} T_{H} \ln(\frac{V_{B}}{V_{A}})
	\end{equation*}
	Analogously, contraction in step three between states $ C $ and $ D $ at constant $ T_{C} $
	\begin{equation*}
		Q_{C} =  \int_{V_{C}}^{V_{D}} \frac{Nk_{B}T_{C}}{V} \diff V = - N k_{B} T_{C} \ln(\frac{V_{D}}{V_{C}})
	\end{equation*}
	The negative sign comes from volume decreasing (i.e. $ \diff V < 0 $) during contraction.
	
	\item For the two adiabatic processes (adiabatic means $ \dbar Q  = 0$) the first law gives $ \diff E = - p \diff V $. To make the integration easier we write $ \diff E $ in terms of heat capacity as $ \diff E = C_{V} \diff T $. Plugging in $ p = \frac{Nk_{B}T}{V} $ and $ C_{V} = \frac{3}{2}Nk_{B} $ for an ideal gas we have 
	\begin{equation*}
		\diff E = - p \diff V \iff C_{V} \diff T = - \frac{Nk_{B}T}{V} \diff V \implies \frac{\diff T}{T} = -\frac{2}{3}  \frac{\diff V}{V}
	\end{equation*}
	Integrating both sides of the equation gives
	\begin{equation*}
		\ln \frac{T_{2}}{T_{1}} = - \frac{2}{3} \ln \frac{V_{2}}{V_{1}} = \ln \left(\frac{V_{1}}{V_{2}}\right)^{2/3} \implies \frac{T_{2}}{T_{1}} = \left(\frac{V_{1}}{V_{2}}\right)^{2/3}
	\end{equation*}
	Applied to the two adiabatic processes (step 2 from $ B $ to $ C $ and step 3 from $ D $ to $ A $) gives
	\begin{equation*}
		T_{H}V_{B}^{2/3} = T_{C}V_{C}^{2/3} \qquad \text{and} \qquad T_{C}V_{D}^{2/3} = T_{H}V_{A}^{2/3}
	\end{equation*}
	Solving both equations for $ \frac{T_{C}}{T_{H}} $ shows that $\frac{V_{B}}{V_{A}} = \frac{V_{C}}{V_{D}}$. 
	
	\item Because $ \frac{V_{B}}{V_{A}} = \frac{V_{C}}{V_{D}} $, the logarithms cancel when we take the ratio of heats $ Q_{C} $ and $ Q_{H} $:
	\begin{equation*}
		\frac{Q_{C}}{Q_{H}} = \frac{- N k_{B} T_{C} \ln(\frac{V_{D}}{V_{C}})}{N k_{B} T_{H} \ln(\frac{V_{B}}{V_{A}})} =  \frac{+ N k_{B} T_{C} \ln(\frac{V_{C}}{V_{D}})}{N k_{B} T_{H} \ln(\frac{V_{B}}{V_{A}})} = \frac{T_{C}}{T_{H}}
	\end{equation*}
	The efficiency of the Carnot cycle for an ideal gas is thus
	\begin{equation*}
		\eta = 1 - \frac{Q_{C}}{Q_{H}} = 1 - \frac{T_{C}}{T_{H}}
	\end{equation*}
	in agreement with the thermodynamic temperatures scale.
	
\end{itemize}

\subsubsection{Entropy}
\begin{itemize}
	\item Applied to the Carnot cycle, the second law states that we cannot turn all the heat from the hot reservoir into work; we must emit some heat to the cold reservoir. From the Carnot cycle and thermodynamic temperature scale $ \frac{Q_{H}}{T_{H}} = \frac{Q_{C}}{T_{C}} $ we have
	\begin{equation*}
		\frac{Q_{H}}{T_{H}} - \frac{Q_{C}}{T_{C}} = 0
	\end{equation*}
	
	\item We now generalize our notation, so that heat $ Q $ is negative when the system releases heat and positive when the system absorbs heat, and we replace the indices $ H $ and $ C $ with $ 1 $ and $ 2 $. In this new convention we have $ Q_{2} = - Q_{C} $ (the system releases heat $ Q_{C} $) and the Carnot cycle is succinctly written
	\begin{equation*}
		\sum_{i=1}^{2} \frac{Q_{i}}{T_{i}} = 0
	\end{equation*}
	
	\item In fact, any reversible cycle, no matter the shape, can be equivalently constructed as sequence of (infinitesimally) small isothermal and adiabatic segments (basically mini Carnot cycles), for which we have
	\begin{equation*}
		\sum_{i=1} \frac{Q_{i}}{T_{i}} = \frac{Q_{1}}{T_{1}} + \frac{Q_{2}}{T_{2}} + \cdots = 0
	\end{equation*}
	Summing all the infinitesimal contributions $\frac{Q}{T}$ from each mini Carnot cycle along the path, we see the total heat absorbed an any reversible cyclic process must obey the relation
	\begin{equation*}
		\oint \frac{\dbar Q}{T} = 0
	\end{equation*}
	
	\item The relationship $ \oint \frac{\dbar Q}{T} = 0 $ for reversible cycles is a powerful statement: it states that if we reversibly change a system from state $ A $ to state $ B $, the quantity $ \int_{A}^{B} \frac{\dbar Q}{T} $ is independent of the path taken; it depends only on the initial and final state. In other words, the quantity $ \int \frac{\dbar Q}{T} $ is a new function of state. We call it entropy:
	\begin{equation*}
		S(A) = \int_{0}^{A} \frac{\dbar Q}{T} \eqtext{and} \diff S = \frac{\dbar Q}{T}
	\end{equation*}
	I stress again that entropy, like any function of state, depends only on the state variables and not on the path we took to get to a given state. In fact, we don't even have to take a reversible path to have well-defined entropy: as long as our system is in a state of equilibrium its entropy is well defined (at least relative to a reference state). However, we will have to slightly generalize our formulation of entropy for reversible processes.
	
	\item How can we be sure this definition of entropy agrees with the one we met in statistical mechanics? To do this, we first find the differential $ \diff S = \frac{\dbar Q}{T} $ and solve for $ \dbar Q = T \diff S $. Plugged into the first law of thermodynamics for an ideal gas $ \diff E = \dbar Q - p \diff V $ we have 
	\begin{equation*}
		\diff E = T \diff S - p \diff V
	\end{equation*}
	This form of the first law (and in particular the role of $ S $) is identical to the version from statistical mechanics, so $ S $ must be the same quantity.
\end{itemize}

\subsubsection{Irreversible Processes}
\begin{itemize}
	\item What can we say about irreversible processes? We know by Carnot's theorem that an irreversible engine operating between two temperatures $ T_{H} $ and $ T_{C} $ is less efficient than a reversible Carnot engine operating between the same temperature.
	
	\item We will now return to the Carnot theorem situation and use the same notation the Carnot engine absorbs heat $ Q_{H} $ and emits heat $ Q_{C} $; the irreversible engine absorbs heat $ Q_{H}' $ and emits heat $ Q_{C}' $. We run the engines so they both do the same amount of work $ W = Q_{H} - Q_{C} = Q_{H}' - Q_{C}' $. 
	
	\item Recall from the first bullet of the entropy section that any Carnot cycle can be written
	\begin{equation*}
		\frac{Q_{H}}{T_{H}} - \frac{Q_{C}}{T_{C}} = 0
	\end{equation*}
	However, the irreversible engine does not run in a Carnot cycle. Instead, for the reversible engine we have the inequality
	\begin{equation*}
		\frac{Q_{H}'}{T_{H}} - \frac{Q_{C}'}{T_{C}} \leq 0
	\end{equation*}
	We can prove this with the Carnot identity $ \frac{Q_{H}}{T_{H}} - \frac{Q_{C}}{T_{C}} = 0 $ and the Clausius formulation of the second law. There's probably a better way to do this by the way. Here's my long version; at least it will be clear to other slow learners like myself ;)
	\begin{itemize}
		\item Both engines do the same amount of work; by conservation of energy we have $ W = Q_{H} - Q_{C} = Q_{H}' - Q_{C}' $. Dividing through by $ T_{C} $ and rearranging gives
		\begin{equation*}
			\frac{Q_{C}}{T_{C}} - \frac{Q_{C}'}{T_{C}} = \frac{Q_{H}}{T_{C}} - \frac{Q_{H}'}{T_{C}}
		\end{equation*}
		
		\item Adding $ \frac{Q_{H}'}{T_{H}} $ to both sides of the equality and rearranging gives
		\begin{equation*}
			\frac{Q_{H}'}{T_{H}} - \frac{Q_{C}'}{T_{C}} = \frac{Q_{H}}{T_{C}} - \frac{Q_{H}'}{T_{C}} - \frac{Q_{C}}{T_{C}} + \frac{Q_{H}'}{T_{H}}
		\end{equation*}
		
		\item We add and subtract $  \frac{Q_{H}}{T_{H}} $ from the right hand side to get
		\begin{equation*}
			\frac{Q_{H}'}{T_{H}} - \frac{Q_{C}'}{T_{C}} =  \frac{Q_{H}}{T_{H}} - \frac{Q_{C}}{T_{C}} + \frac{Q_{H}'}{T_{H}} -  \frac{Q_{H}}{T_{H}} + \frac{Q_{H}}{T_{C}} - \frac{Q_{H}'}{T_{C}}
		\end{equation*}
		
		\item Things get simpler now; the first two terms on the right side are zero (the Carnot identity $ \frac{Q_{H}}{T_{H}} - \frac{Q_{C}}{T_{C}} = 0 $). The left four terms conveniently factor. We are left with
		\begin{equation*}
			\frac{Q_{H}'}{T_{H}} - \frac{Q_{C}'}{T_{C}} = (0) + (Q_{H}' - Q_{H})\left(\frac{1}{T_{H}} - \frac{1}{T_{C}}\right)
		\end{equation*}
		
		\item Finally, we invoke the Clausius statement of the second law, which tells us that $ T_{H} > T_{C} $ and $ Q_{H}' - Q_{H} \geq 0 $ (i.e. heat flows from hotter to colder bodies; see the proof of Carnot's theorem for a similar use of Clausius's formulation). The temperature term $ \frac{1}{T_{H}} - \frac{1}{T_{C}} $ is thus negative while $ (Q_{H}' - Q_{H}) \geq 0 $, leading to
		\begin{equation*}
			\frac{Q_{H}'}{T_{H}} - \frac{Q_{C}'}{Q_{H}} =(Q_{H}' - Q_{H})\left(\frac{1}{T_{H}} - \frac{1}{T_{C}}\right) \leq 0
		\end{equation*}
	\end{itemize}

	
	\item The resulting inequality is important. I'll write it again:
	\begin{equation*}
		\frac{Q_{H}'}{T_{H}} - \frac{Q_{C}'}{T_{C}} \leq 0
	\end{equation*}
	Why is it important? Because we did not require reversibility, the inequality holds for \textit{any} engine operating between the two temperatures $ T_{H} $ and $ T_{C} $, reversible or not. 
	
	\item We can now repeat the process by which we derived entropy, i.e. breaking up a larger cycle into infinitesimal reversible cycles, each contributing $ \frac{Q_{i}}{T_{i}} $. This time, instead of an equality, we have
	\begin{equation*}
		\sum_{i=1} \frac{Q_{i}}{T_{i}} = \frac{Q_{1}}{T_{1}} + \frac{Q_{2}}{T_{2}} + \cdots \leq 0
	\end{equation*}
	In the infinitesimal limit, this generalizes to
	\begin{equation*}
		\oint \frac{\dbar Q}{T} \leq 0
	\end{equation*}
	This result holds for \textit{any} cyclic thermodynamic process, reversible or not, returning to its original state. The result is called the \textit{Clausius inequality}. It allows us to generalize the equation for entropy to 
	\begin{equation*}
		\diff S \geq \frac{\dbar Q}{T}
	\end{equation*}
	which holds for all processes, reversible or not.
	
	\item Consider a system with two possible paths from states $ A $ to $ B $; path $\mathcal{IR}$ is irreversible and path $\mathcal{R}$ is reversible. If take the irreversible path $ \mathcal{IR} $ from $ A $ to $ B $ and return from $ B $ to $ A $ by the reversible path $\mathcal{R}$, Clausius's inequality reads
	\begin{equation*}
		\oint \frac{\dbar Q}{T} = \int_{\mathcal{IR}}\frac{\dbar Q}{T} - \int_{\mathcal{R}} \frac{\dbar Q}{T} \leq 0 \implies \int_{\mathcal{IR}}\frac{\dbar Q}{T} \leq \int_{\mathcal{R}}\frac{\dbar Q}{T} 
	\end{equation*}
	There is a negative sign in front of the reversible path integral because $ \mathcal{R} $ is defined from $ A $ to $ B $, the negative of going from $ B $ to $ A $.
	
	But we know from earlier that the reversible path integral $ \int_{\mathcal{R}} \frac{\dbar Q}{T} $ between states $ A $ and $ B $ equals the entropy difference $ S(B) - S(A)$. The inequality $ \int_{\mathcal{IR}}\frac{\dbar Q}{T} \leq \int_{\mathcal{R}}\frac{\dbar Q}{T}  $ then reads
	\begin{equation*}
		S(B) - S(A) \geq \int_{\mathcal{IR}}\frac{\dbar Q}{T}  \eqtext{and} \diff S \geq \frac{\dbar Q}{T}
	\end{equation*}
	This definition hold for all processes, reversible or not. Reversible processes get an equals sign $ \diff S = \frac{\dbar Q}{T} $, while irreversible processes read $ \diff S > \frac{\dbar Q}{T} $.
	
	
	\item Next, we suppose the system is isolated from its environment during the path $ \mathcal{IR} $. This means no heat can flow into or out of the system, so $ \dbar Q = 0 $. We then have
	\begin{equation*}
		S(B) \geq S(A)
	\end{equation*}
	In other words, in any process from state $ A $ to $ B $, the entropy of an isolated system can never decrease. If the path $ \mathcal{IR} $ were a reversible process, Clausius's inequality would take the earlier form $ \oint \frac{\dbar Q}{T} = 0 $; we would repeat the analysis with an equality to get
	\begin{equation*}
		S(B) = S(A)
	\end{equation*}
	for any \textit{reversible} adiabatic process. 
	
	Note that the processes being adiabatic is equivalent to the system being thermally isolated from its surroundings; in both cases their is no heat exchange.
	
	\item Most of us are introduced to the second law with the result $ S(B) \geq S(A) $ or the equivalent statement ``entropy never decreases''. But if we are picky, that result is really a consequence of the second law, not the second law itself!
	
	Note also that entropy does not always decrease. It's true that the entropy of an isolated system (or a system undergoing an adiabatic process) will always increase, but if we allow for heat exchange between system and surroundings, it is also possible for entropy to decrease.
	
	For example, if we put a hot bowl into a refrigerator, the bowl's entropy will decrease as it cools off while exchanging heat with its surroundings. But once we view the combined refrigerator-bowl system as a whole, we'll see once again that entropy has increased.
	
\end{itemize}


\subsection{Thermodynamic Potentials}
Starting with the state variables pressure $ p $ and volume $ V $, we have defined three functions of state: temperature $ T $, energy $ E $ and entropy $ S $, giving a total of five quantities with which to describe a thermodynamic system. 

But there is no reason we couldn't invert one of the functions of state and instead label the state of our system by specifying e.g $ T $ and $ V $, $ S $ and $ V $, $ E $ and $ p $, etc...

It turns out that certain pairs of quantities are particularly suited to certain types of systems. 

\subsubsection{Helmholtz Free Energy}
\begin{itemize}
	\item \textit{Helmholtz free energy} $ F $ is the quantity best suited to describing a system at constant temperature and volume. Helmholtz free energy is defined as
	\begin{equation*}
		F = E - TS
	\end{equation*}
	
	\item Why is the $ F $ the natural quantity for an isothermal system? Consider a primary system in contact with a large heat reservoir with fixed temperature and volume. The energy both the system and reservoir can individually fluctuate, but the total energy $ E_{tot} $ of the primary system and reservoir remains fixed. The primary system has energy $ E $ and the reservoir has energy $ E_{R} = E_{tot} - E $. The entropy of the combined system is
	\begin{equation*}
		S_{tot}(E_{tot}) = S_{R}(E_{tot} - E) + S(E)
	\end{equation*}
	Because $ E_{tot} \gg E $, we can Taylor expand $ S_{R} $ about $ E_{tot} $ using $ f(x + \delta x) \approx x + \delta x f'(x)$ to get
	\begin{align*}
		S_{tot}(E_{tot}) &\approx S_{R}(E_{tot}) - E \pdv{S_{R}}{E_{tot}} + S(E) = S_{R}(E_{tot}) - \frac{E}{T} + S(E) \\
		&= S_{R}(E_{tot}) - \frac{F}{T}
	\end{align*}
	where we have recognized $ \pdv{S_{R}}{E_{tot}} = \frac{1}{T} $ (known from statistical mechanics, or from the partial derivative of the first law $ \diff E = T \diff S - p \diff V $ with respect to $ E $, which gives $ 1 = T \pdv{S}{E} $).
	
	\item The equality $ S_{tot}(E_{tot}) = S_{R}(E_{tot}) - \frac{F}{T} $ leads to an important insight: the second law tells us that total entropy $ S_{tot} $ can never decrease. This means the free energy $ F $ of our isothermal system can never increase (this would cause $ S_{tot} $ to decrease).
	
	In summary, an isolated system's total entropy can never decrease while its free energy can never increase. A system at constant temperature and volume will always settle into the equilibrium state minimizing its free energy.
	
	\item As mentioned in the context of statistical mechanics, free energy is a Legendre transformation of internal energy $ E $. Its total derivative (using the first law of thermodynamics $ \diff E = T \diff S - p \diff V $) is
	\begin{equation*}
		\diff F  = - S \diff T - p \diff V 
	\end{equation*}
	We see free energy is most naturally a function of $ T $ and $ V $, and we can immediately deduce the relationships
	\begin{equation*}
		\eval{\pdv{F}{S}}_{V} = - S \qquad \text{and} \qquad 	\eval{\pdv{F}{S}}_{T} = - p
	\end{equation*}
	
	\item Consider taking a system through a reversible isothermal process from state $ A $ to state $ B $. For an isothermal process $ \diff T = 0 $. The free energy differential is $ \diff F = - p \diff V $ and the change in free energy during the process is
	\begin{equation*}
		F(B) - F(A) = \int_{A}^{B} \diff F = - \int_{A}^{B} p \diff V = -W_{sys}
	\end{equation*}
	where $ W_{sys} $ is the work \textit{by} the system during the process (the work done \textit{on} the system is $ W = -\int p \diff V$).
	
	\item The expression $ F(B) - F(A) = -W_{sys} $ is important. In the form $ F_{B} = F_{A} - W_{sys} $, we see a system's free energy decreases in any isothermal process in which the system performs work. This leads to the physical meaning of free energy: it is the amount of the system's energy available to perform work at the fixed temperature $ T $.
	
	\item As a final note, $ F $ is implicitly also a function of particle number $ N $. If we allow $ N $ to fluctuate, the total differential of $ F $ becomes
	\begin{equation*}
		\diff F  = - S \diff T - p \diff V + \mu \diff N
	\end{equation*}
	where $ \mu $ is the familiar chemical potential from the grand canonical ensemble, measuring the energy required to add more particles to the system.
\end{itemize}

\subsubsection{Gibbs Free Energy}
\begin{itemize}
	\item The most natural quantity to describe a system at constant pressure whose volume can fluctuate is \textit{Gibbs free energy} $ G $. Gibbs free energy is defined as
	\begin{equation*}
		G = F + pV = E + pV - TS
	\end{equation*}
	
	\item More on Gibbs free energy and entropy: Consider a primary system in contact with a large reservoir at temperature $ T $. The volume of each system can fluctuate but the total volume of the combined system and reservoir remains fixed. The system has energy $ E $ and volume  $ V $; the reservoir energy $ E_{R} = E_{tot} - E $ and $ V_{R} = V_{tot} - V $. The total entropy is
	\begin{equation*}
		S_{tot}(E_{tot}, V_{tot}) = S_{R}(E_{tot} - E, V_{tot} - V) + S(E, V)
	\end{equation*}
	Performing the usual differential approximation we get
	\begin{equation*}
		S_{tot}(E_{tot}, V_{tot}) \approx S_{R}(E_{tot}, V_{tot}) - E\pdv{S_{R}}{E_{tot}} - V\pdv{S_{R}}{V_{tot}} + S(E, V)
	\end{equation*}
	Recognizing $ \pdv{S}{E} = \frac{1}{T}  $ and $ \pdv{S}{V} = \frac{p}{T} $ we get
	\begin{equation*}
		S_{tot}(E_{tot}, V_{tot}) = S_{R}(E_{tot}, V_{tot}) - \frac{E + pV - TS}{T} = S_{R}(E_{tot}, V_{tot}) - \frac{G}{T}
	\end{equation*}
	
	So: why is Gibbs used at fixed temperature and pressure? Because it is the quantity naturally arising in the entropy expression with $ T $ and $ p $ fixed. Specifically, we will minimize $ G $, so that $ S_{tot} $ is maximized in accordance with statistical physics arguments and the second law.
	
	
	\item Gibbs free energy is a Legendre transform of $ F $. Its total derivative is
	\begin{equation*}
		\diff G = - S \diff T + V \diff p
	\end{equation*}
	so $ G $ is most naturally a function of $ T $ and $ p $.
	
	\item Finally, if we make allowance for variation in particle number we have
	\begin{equation*}
		\diff G = - \S \diff T + V \diff p + \mu \diff N
	\end{equation*}
	In this case $ G = G(T, p, N) $ is a function of only a single extensive quantity: particle number $ N $. From the partial derivative $ \pdv{G}{N} = \mu $ we then have the simple relationship on $ N $
	\begin{equation*}
		G(T, p, N) = \mu(p, T) N
	\end{equation*}
	This simplification is not completely for free: we have packaged temperature and pressure dependence into the chemical potential $ \mu $. Nonetheless, the simple equality $ G = \mu N $ is very useful if $ \mu $ is known.
	
	Note that this simple dependence on $ N $ a special quality of $ G $, arising form the fact that $ G $ depends only on one extensive quantity. For example, free energy $ F = F(T, V, N) $ is a function of two extensive quantities, $ V $ and $ N $, so does not have a simple dependence on $ N $.
	
	Note also the similarity to how the grand canonical potential gave us the simple volume dependence $ \Phi = - pV $ with temperature and chemical potential dependence packaged into $ p = p(T, \mu) $. 
	
\end{itemize}
	
\subsubsection{Enthalpy}
\begin{itemize}
	\item We have one more combination to consider. Systems at fixed energy and pressure are most naturally described by \textit{enthalpy}, defined as
	\begin{equation*}
		H = E + p V
	\end{equation*}
	
	\item The total derivative of enthalpy is 
	\begin{equation*}
		\diff H = T \diff S + V \diff p
	\end{equation*}
	so enthalpy is best thought of as function of entropy and pressure.
	
	\item The quantities $ E, F, G $ and $ H $ are called the \textit{thermodynamic potentials}.
\end{itemize}

\subsubsection{Maxwell's Relations}
Maxwell's relations give us some very useful relations between the thermodynamic potentials $ E, F, G $ and $ H $.
\begin{itemize}
	\item We start with energy: like any function of state, energy can be views a function of any two other quantities specifying the state of the system. However, the first law 
	\begin{equation*}
		 \diff E = T \diff S - p \diff V 
	\end{equation*}
	suggests energy is most naturally viewed as a function of energy and volume. In the interpretation $ E = E(S, V) $, the partial derivatives are the familiar quantities
	\begin{equation*}
		\eval{\pdv{E}{S}}_{V} = T \qquad \text{and} \qquad \eval{\pdv{E}{V}}_{S} = - p
	\end{equation*}
	The interesting part comes from considering the mixed second partial derivative
	\begin{equation*}
		\pdv{E}{S}{V} \equiv \pdv{E}{V}{S} \implies \eval{\pdv{T}{V}}_{S} = - \eval{\pdv{p}{S}}_{V}
	\end{equation*}
	The resulting equality 
	\begin{equation*}
		\eval{\pdv{T}{V}}_{S} = - \eval{\pdv{p}{S}}_{V}
	\end{equation*}
	is called \textit{Maxwell's first relation}. The remaining Maxwell's relations come from considering each of the remaining thermodynamic potentials in turn.
	
	\item On to (Helmholtz) free energy $ F $. The total differential
	\begin{equation*}
		\diff F = - p \diff V - S \diff T
	\end{equation*}
	tells us free energy is naturally viewed as a function of $ V $ and $ T $; the interpretation $ F = F(V, T) $ gives
	\begin{equation*}
		\eval{\pdv{F}{V}}_{T} = - p \qquad \text{and} \qquad \eval{\pdv{F}{T}}_{V} = - S
	\end{equation*}
	Equating the mixed second partial derivatives $ \pdv{F}{V}{T} \equiv \pdv{F}{T}{V} $ produces the \textit{second Maxwell relation}
	\begin{equation*}
		\eval{\pdv{p}{T}}_{V} = \eval{\pdv{S}{V}}_{T}
	\end{equation*}
	
	\item Next, Gibbs free energy: the total differential 
	\begin{equation*}
		\diff G = V\diff p - S \diff T
	\end{equation*}
	tells us Gibbs free energy is most naturally a function of $ p $ and $ T $. The interpretation $ G = G(p, T) $ gives
	\begin{equation*}
		\eval{\pdv{G}{T}}_{p} = - S \qquad \text{and} \qquad \eval{\pdv{G}{p}}_{T} = V
	\end{equation*}
	Equating the mixed second partial derivatives $ \pdv{G}{T}{p} \equiv \pdv{G}{p}{T} $ produces the \textit{third Maxwell relation}
	\begin{equation*}
		\eval{\pdv{S}{p}}_{T} = -\eval{\pdv{V}{T}}_{p}
	\end{equation*}
	
	\item Finally, enthalpy: the total differential is
	\begin{equation*}
		\diff H = T \diff S + V \diff p 
	\end{equation*}
	so enthalpy is most naturally a function of $ S $ and $ p $; writing $ H = H(S, p) $ gives
	\begin{equation*}
		\eval{\pdv{H}{S}}_{p} = T \qquad \text{and} \qquad \eval{\pdv{H}{p}}_{S} = V
	\end{equation*}
	Equating the mixed second partial derivatives $ \pdv{H}{S}{p} \equiv \pdv{H}{p}{S} $ produces the \textit{fourth Maxwell relation}
	\begin{align*}
		\eval{\pdv{T}{p}}_{S} = \eval{\pdv{V}{S}}_{p} 
	\end{align*}
	
	\item Maxwell's relations are remarkable because they mathematically trivial but not physically obvious. They are mathematical identities that hold for any physical system. 	Note that cross-multiplication of Maxwell's relations always yields $ pV = \pm TS $
	
	Maxwell's relations are particularly useful because they equate entropy, which is difficult to measure experimentally, to the  state variables $ (p, V) $ and temperature $ T $, which are easy to measure or determine from the system's equation of state.
	
\end{itemize}

\subsubsection{Applications of Thermodynamic Potentials}
Difference of heat capacities, adiabatic compressibility and the Joule-Kelvin coefficient.

\href{https://faculty.uca.edu/saddison/ThermalPhysics/Heat\%20Capacity.pdf}{Heat Capacity Link}

\begin{itemize}
	\item Recall heat capacity at constant volume and constant pressure are defined as
	\begin{equation*}
		C_{V} \equiv \eval{\pdv{E}{T}}_{V} = T \eval{\pdv{S}{T}}_{V} \qquad \text{and} \qquad C_{p} \equiv \eval{\pdv{H}{T}}_{p} = T \eval{\pdv{S}{T}}_{p}
	\end{equation*}
	where the entropy definition comes from the total differentials $ \diff E = T \diff S - p \diff V $ and $ \diff H = T \diff S + V \diff p $.
	
	\item The difference of heat capacities $ C_{p} $ and $ C_{V} $ can be written
	\begin{equation*}
		C_{p} - C_{V} = T \eval{\pdv{V}{T}}_{p}\eval{\pdv{p}{T}}_{V}
	\end{equation*}
	The derivation is basically an exercise in manipulating partial derivatives: I'll put it in the next bullet point. The equality conceals some interesting information about an ideal gas; we just need the equation of state $ pV = Nk_{B}T $. Evaluating the derivatives gives
	\begin{equation*}
		C_{p} - C_{V} = T \left(\frac{Nk_{B}}{p}\right) \left(\frac{Nk_{B}}{V}\right) = Nk_{B}
	\end{equation*}
	In other words, $ C_{p} $ is larger than $ C_{V} $ by a factor of $ Nk_{B} $. The fact that $ C_{p} > C_{V} $ makes sense: at constant volume there is no work, so all heat added to a system goes into increasing its internal energy. At constant pressure at least some heat added to a system goes into work; not all goes into increasing internal energy. This means there is less energy available to raise the system's temperature, so more heat is needed for a given temperature increase in the constant pressure situation than in the constant volume situation. This means $ C_{p} > C_{V} $. This basically means we have to add more heat to a constant pressure system than to a constant volume system for a given increase in temperature.
	
	\item Aside: deriving the result $ C_{p} - C_{V} = T \eval{\pdv{V}{T}}_{p}\eval{\pdv{p}{T}}_{V} $ (feel free to skip). Assuming $ N $ is constant, entropy is $ S = S(T, V) $ and the total derivative is
	\begin{equation*}
		\diff S = \eval{\pdv{S}{T}}_{V} \diff T + \eval{\pdv{S}{V}}_{T} \diff V
	\end{equation*}
	Meanwhile, viewing volume as a function of state $ V = V(p, T) $ we have
	\begin{equation*}
		\diff V = \eval{\pdv{V}{p}}_{T} \diff p + \eval{\pdv{V}{T}}_{p} \diff T
	\end{equation*}
	Plugging $ \diff V $ into the equation for $ \diff S $ at grouping the $ \diff T $ and $ \diff p $ terms gives
	\begin{equation*}
	 \diff S = \left[\eval{\pdv{S}{T}}_{V} + \eval{\pdv{S}{V}}_{T} \eval{\pdv{V}{T}}_{p}  \right] \diff T + \eval{\pdv{S}{V}}_{T}\eval{\pdv{V}{p}}_{T}  \diff p
	\end{equation*}
	For constant pressure process we set $ \diff p = 0 $, differentiate with respect to $ T $ at constant $ p $ and recognize the expression $ C_{p} = T \eval{\pdv{S}{T}}_{p} $
	\begin{equation*}
		\eval{\pdv{S}{T}}_{p} = \eval{\pdv{S}{T}}_{V} + \eval{\pdv{S}{V}}_{T} \eval{\pdv{V}{T}}_{p} = \frac{C_{p}}{T}
	\end{equation*}
	Subtracting $ \frac{C_{p}}{T} $ and $ \frac{C_{V}}{T} = \eval{\pdv{S}{T}}_{V} $ and multiplying by $ T $ gives the desired result
	\begin{equation*}
		 C_{p} - C_{V} = T \eval{\pdv{V}{T}}_{p}\eval{\pdv{p}{T}}_{V}
	\end{equation*}
\end{itemize}

\subsection{The Liquid-Gas Phase Transition}
A phase transition is an abrupt, discontinuous change in the properties of a system. Classic examples are liquid water freezing to ice or steam condensing to water. Phase transitions are a rich field of study, and much of the advanced work in statistical mechanics and thermodynamics concerns phase transitions. This section, which discusses the basics of the liquid-gas phase transition, barely scratches the surface of this important field of study.

\subsubsection{Quick Review of the van der Waals Equation}
We will begin our analysis of the liquid-gas transition with the van der Waals equation, derived in the section on interacting gases. Following is a quick review.
\begin{itemize}
	\item In terms of volume per particle $ v = \frac{V}{N} $, the equation reads
	\begin{equation*}
		p = \frac{k_{B}T}{v-b} - \frac{a}{v^{2}}
	\end{equation*}
	The equation is often written in terms of particle density $ \rho = \frac{N}{V} $ in the form
	\begin{equation*}
		p = \frac{\rho k_{B}T}{1 - \rho b} - \rho^{2}a
	\end{equation*}
	We will use the first form in terms of volume per particle $ v $.

	
	\item The equation is derived by expanding the partition function in terms of the Mayer function $ f = e^{-\beta U(r)} - 1 $ in the high-temperature, low density limit using the hard core potential.
	\begin{equation*}
		U(r) = \begin{cases}
			\infty & r < r_{0}\\
			-U_{0}\left(\frac{r}{r_{0}}\right)^{6} & r \geq r_{0}
		\end{cases}
	\end{equation*}
	Even though the equation is derived with the hard core potential, it holds for similar inter-atomic potentials that are highly repulsive at short range ($ r \lesssim r_{0}  $) and attractive at short range, with the attractive term falling as $ \frac{1}{r^{n}}, n \geq 4$. The point is that the van der Waals has farther-reaching applicability than just the hard core potential and successfully models a wide range of gases.
	
	\item The parameters are $ a $ and $ b $ are
	\begin{equation*}
		 a = \frac{2\pi r_{0}^{3}U_{0}}{3} \qquad \text{and} \qquad b = \frac{2\pi r_{0}^{3}}{3} 
	\end{equation*}
	$ a $ (which contains the factor $ U_{0} $ representing the depth of the potential well) models the attractive interaction between particles that takes effect at large distances. $ a $ reduces the pressure of the interacting gas compared to an ideal gas.

	$ b $ contains only $ r_{0} $ (the minimum distance two particles can approach) and models hard-core repulsion at short distances. $ b $ reduces the effective volume of the interacting gas compared to an ideal gas; more volume is taken up by the finite-size particles, so the gas's effective volume (the ``empty'' space) decreases. 
	
\end{itemize}

\subsubsection{Isotherms and Critical Temperature}
\begin{itemize}
	\item For the van der Waals equation, \textit{isotherms} are $ p $ vs $ v $ curves ($ p $ on the ordinate and $ v $ on the abscissa) evaluated fixed values of temperature $ T $; think of temperature parametrizing the $ (v, p) $ curves. (In general, isotherms are curves of a system's state variables at constant temperature $ T $; for the van der Waals equation the state variables are $ p $ and $ v $).
	
	\item Isotherms are an excellent tool for analyzing the liquid-gas phase transition. If we plot isotherms of the van der Waals equation across a wide range of temperatures, we discover an interesting behavior. 
	\begin{itemize}
		\item First, we see that $ v $ is always larger than the parameter $ b = \frac{2\pi r_{0}^{3}}{3} $ and pressure approaches infinity as $ v $ approaches $ b $ from the left. This makes sense; $ b $ represents the volume of a particle, and the volume per particle $ v $ cannot be smaller than the volume of each particle.
		
		\item For very high temperatures the isotherms are monotonically decreasing functions scaling approximately as $ p \sim \frac{1}{v} $, just like the isotherms of an ideal gas. This is expected---most interacting gases begin to behave like an ideal gas in the high temperature regime.
		
		\item At very low temperatures (when both terms in the van der Waals equation contribute comparably, roughly for temperatures where $ k_{B}T \sim \frac{a}{v} $), the $ (v, p) $ curve has a wiggle. You basically need to see an actual graph to picture this. The wiggle represents an highly unstable, non-physical state; we will discuss it more detail shortly.
		
		\item At an intermediate \textit{critical temperature}  $ T = T_{c} $, the maximum and minimum of the wiggle meet, and the wiggle flattens out to form an inflection point. This occurs where $ \dv{p}{v} = \dv[2]{p}{v} = 0 $ and is satisfied only for $ T = T_{c} $ given by
		\begin{equation*}
			k_{B}T_{c} = \frac{8a}{27b}
		\end{equation*}
		Along the way, we also get the critical volume $ v_{c} $ and pressure $ p_{c} $; these are
		\begin{equation*}
			v_{c} = 3b \qquad \text{and} \qquad p_{c} = \frac{a}{27b^{2}}
		\end{equation*}
		
	\end{itemize}
	
	\item Derivation of $ v_{c}, T_{c}, p_{c} $: (it's mostly algebra manipulations; feel free to skip). 
	\begin{itemize}
		\item Calculating $  \dv{p}{v} $ and $ \dv[2]{p}{v} $ and setting both equal to zero gives the system of equations
		\begin{equation*}
			\dv{p}{v} = -\frac{k_{B}T}{(v-b)^{2}} + \frac{2a}{v^{3}} = 0 \qquad \text{and} \qquad  \dv[2]{p}{v} = \frac{2k_{B}T}{(v-b)^{3}} - \frac{6a}{v^{4}} = 0
		\end{equation*}
		\item Multiplying $ \dv{p}{v} $ by $ \frac{2}{v-b} $ and adding  equations (the $ k_{B}T $ term cancels) gives 
		\begin{equation*}
			\frac{4a}{v^{3}(v-b)} = \frac{6a}{v^{4}} \implies v = v_{c} = 3b
		\end{equation*}
		
		\item Plugging the critical volume into e.g. $  \dv{p}{v} $ ($ \dv[2]{p}{v} $ also works) gives
		\begin{equation*}
			\frac{k_{B}T}{4b^{2}} = \frac{2a}{27b^{3}} \implies k_{B}T_{c} = \frac{8a}{27b}
		\end{equation*}
		
		\item Plugging $ v_{c} $ and $ T_{c} $ into the van der Waals equation $ p = \frac{k_{B}T}{v-b} - \frac{a}{v^{2}}$ gives
		\begin{equation*}
			p_{c} = \frac{8a}{27b}\frac{1}{2b} - \frac{a}{9b^{2}} = \frac{a}{27b^{2}}
		\end{equation*}
	
	\end{itemize}

	\item Next, we examine the low temperature $ T < T_{c} $ isotherm in more detail. Pressures between the minimum and maximum of the wiggle can mathematically occur at three different $ v $. But how to physically interpret the system existing at three different densities $ \rho = \frac{1}{v} $ at the same pressure? We examine the three solutions in turn, first ruling out a non-physical solution.
	\begin{itemize}
		\item The middle solution for $ v $ has strange properties. In particular, the point occurs where the isotherm's slope $ \eval{\dv{p}{v}}_{T} $ is positive. This means that if we increase the gas's volume, the pressure increases, or alternatively, if we decrease the gas's volume (e.g. by compression) the pressure decreases! This is not physical behavior; gases as we know them display the opposite behavior. Clearly, solutions in the region $ \eval{\dv{p}{v}}_{T} $ represent highly unstable, non-physical states. Even if we were able to create such a state, any small perturbation from the surroundings would blow into a large change in density from the $ \eval{\dv{p}{v}}_{T}  $ behavior. We do not expect to find the middle solution in Nature.
		
		\item Next, the solution on the left with small volume per particle, comparable to the particle volume: $ v \sim b $. This means the particles are very densely packed. More so, we see the graph is very steep in the region $ v \sim b $, i.e. $ \dv{p}{v} $ is very large and negative, meaning a small change in volume requires a great deal of pressure; the fluid is very difficult to compress. This type of state is familiar to all of us: it is a \textit{liquid}.
		
		Note that our derivation of the van der Waals equation assumed low-density situations with densities well above those of a liquid. This means we cannot completely trust our conclusions about the high-density solution. Nonetheless (fortunately) the behavior predicted by the van der Waals equation near the liquid regime actually turns out to be qualitatively correct in many respects, and the equation remains a useful tools for studying the liquid-gas phase transition.
		
		\item The last solution on the right of the graph with large $ v $ remains. This state has large spacing per particle $ v \gg b $ and small, negative $ \dv{p}{v} $, meaning only a small amount of pressure is required for an appreciable change in volume. This state is also familiar: it is a \textit{gas}.
		
	\end{itemize}
\end{itemize}

\subsubsection{Phase Equilibrium}

\begin{itemize}
	\item Our next goal is to better understand what happens between the liquid and gas phase. We have seen the middle solution of the $ T < T_{c} $ isotherm between the liquid and gas phases \`{a} la van der Waals exhibits non-physical behavior because $ \eval{\dv{p}{v}}_{T} > 0 $. We cannot trust the van der Waals equation in this region. How to proceed?
	
	\item We start with a conceptual step and assume the possibility of two solutions in the transition regime: part of the system could be in the liquid phase and part of the system could be in the gas phase. 
		
	How to confirm the stable coexistence of two phases in the same system is allowed? 
	
	\item For a system to be stable, it must be in thermal and mechanical equilibrium. Recall two systems placed in contact are thermal and mechanical equilibrium if they have the same uniform values of temperature $ T $ and pressure $ p $. For our hypothetical liquid-gas combination these conditions are satisfied by default: the two solutions occur on the same isotherm (constant $ T $) and at the same value of $ p $. 
	
	\item There is one more equilibrium condition concerning particle exchange between the systems: both systems must have the same value of chemical potential $ \mu $, in which case particle number $ N $ is stable in both phases. This requirement is called \textit{chemical equilibrium} and calls for
	\begin{equation*}
		\mu_{gas} = \mu_{liquid}
	\end{equation*}
	Because the convenient relationship between chemical potential and Gibbs free energy $ G = \mu N $, the condition for chemical equilibrium is often written
	\begin{equation*}
		\frac{G_{gas}}{N_{gas}} = \frac{G_{liquid}}{N_{liquid}} \implies g_{gas} = g_{liquid}
	\end{equation*}
	where $ g = \frac{G}{N} $ is the Gibbs free energy per particle.
	
	Finally, note the equilibrium conditions involve only intensive quantities: $ T $, $ p $, and $ \mu $, which essentially means that our equilibrium arguments apply to any size system.
	
	\item How to ensure the chemical equilibrium condition $ \mu_{gas} = \mu_{liquid} $ is satisfied? We will assume $ \mu = \mu(p, T) $; our goal is show the existence of a unique solution $ \mu_{gas} = \mu_{liquid} $. To do this, we will show that if $ T $ is fixed (an isotherm situation) then $ \mu_{gas} = \mu_{liquid} $ is solved only for a unique value of pressure $ p $.
	
	\item We start in the liquid state at some fixed value of $ T $ travel along the isotherm. Because $ \diff T = 0 $, the infinitesimal change in $ \mu $ is 
	\begin{equation*}
		\diff \mu = \eval{\pdv{\mu}{p}}_{T} \diff p
	\end{equation*}
	We can get an expression for $ \pdv{\mu}{p} $ from the convenient relationship $ G(p, T, N) = \mu(p, T)N $ we see that
	\begin{equation*}
		\eval{\pdv{G}{p}}_{T, N} = \eval{\pdv{\mu}{p}}_{T} N
	\end{equation*}
	while from $ \diff G = -S \diff T + V \diff p + \mu \diff N $ we have
	\begin{equation*}
		\eval{\pdv{G}{p}}_{T, N} = V \implies \eval{\pdv{\mu}{p}}_{T} N = V(p, T)
	\end{equation*}
	
	\item Starting in the liquid phase and integrating the last equality along an isotherm with $ \diff T = 0 $ to solve for $ \mu $ gives
	\begin{equation*}
		\mu(p, T) = \mu_{liquid} = \int_{p_{liquid}}^{p} \frac{V(\tilde{p}, T)}{N} \diff \tilde{p}
	\end{equation*}
	Here's where things get interesting. First, recall the van der Waals equation predicts three possible volumes for a given pressure $ p $ in the $ T < T_{c} $ regime, meaning a line of fixed $ p $ can intersect the $ p $ vs. $ v $ curve in three places. (Again, you basically have to have a graph of the van der Waals isotherms in front of you for this to make sense).
	
	If we start in the liquid phase at the pressure $ p_{liquid} $ and travel along the isotherm towards the gas phase, you will reach a value $ p = p_{gas} = p_{liquid} $. In this case the integral $ \int_{p}^{p} \frac{V}{N} \diff \tilde{p} $ vanishes (the two limits are equal) and we are left with the desired 
	\begin{equation*}
		\mu(p_{gas}, T) = \mu(p_{liquid}, T) \implies \mu_{gas} = \mu_{liquid}
	\end{equation*}
	
	\item Clearly, the condition $ \mu_{gas} = \mu_{liquid} $ is satisfied if the integral $ \int_{p_{liquid}}^{p} \frac{V(\tilde{p}, T)}{N} \diff \tilde{p} $ vanishes, which occurs if we integrate to the unique gaseous pressure $ p_{gas} = p_{liquid} $. Geometrically, the equilibrium pressure $ p_{gas} = p_{liquid} $ is such that the areas above and below the wiggle in the isotherm have equal areas.  This equal area condition is called the \textit{Maxwell construction} for chemical equilibrium and implicitly defines the pressure at which the gaseous and liquid phases can coexist.
	
	As a side note and disclaimer, the construction involves integrating along the non-physical region with $ \dv{p}{v} > 0 $, so we should take our derivation of the Maxwell construction with a grain of salt. Supposedly there are more rigorous arguments than ours that give the same result, so we can indeed trust the Maxwell construction for equilibrium pressure.
	
	\item We've shown the wiggly region in the $ T < T_{c}$ van der Waals isotherms is non-physical, and we know for each isotherm there is a corresponding equilibrium pressure satisfying the Maxwell construction at which the liquid and gas phases can coexist in equilibrium. 
	
	Importantly, there is no condition on how much gas and how much liquid occurs in the coexistence state: particles can move between the liquid and gas states. Such a situation means that the average density of the system can vary as the amount of liquid and gas (which have different densities) fluctuates. This means that inside the coexistence region the isotherms are simply flat lines, reflecting the fact that the system's density can take on any value between the gas and liquid densities. We now have our answer: the wiggly van der Waals region is replaced by straight lines of constant pressure, representing the coexistence of the gas and liquid phases.
	
\end{itemize}

\subsubsection{The Clausius Clapeyron Equation}
\begin{itemize}
	\item Next, we will analyze the liquid-gas phase diagrams in the temperature-pressure plane, with temperature on the abscissa and pressure on the ordinate. Recall how in the volume-pressure plane at fixed $ T < T_{c} $ the coexistence region between liquid and gas phases was a line of constant pressure. If we just slightly increase the pressure above the equilibrium pressure we enter the liquid regime, and vice versa for the gas regime. 
	
	\item In the $ (T, p) $ plane, the liquid and gas phases are separated by a line plotting the equilibrium pressure as a function of temperature. If we begin in the gas phase at fixed $ T < T_{c} $ and cross this phase transition line with even a small change in pressure, we jump immediately into the liquid phase (and vice versa going from liquid to gas). The rapid transition between phases manifests as a rapid, discontinuous change in the system's volume; such discontinuities are the sign of a phase transition. 
	
	\item The line line plotting equilibrium pressure versus temperature in the $ (T, p) $ plane separating the liquid and gas phases marks the presence of a phase transition. On either side of the line, all particles are either in the liquid or gas phase. Recall the chemical equilibrium condition $ \mu_{gas} = \mu_{liquid} $ means the Gibbs free energies per particle of both states are equal, i.e.
	\begin{equation*}
		 g_{gas} = g_{liquid} 
	\end{equation*}
	This means $ G $ is continuous as we cross the line of phase transitions. 
	
	\item Next, instead of crossing the phase transition line, suppose we move along the line itself. How does $ g $ change? From the total differential $ \diff G = -S \diff T + V\diff p  $ we have
	\begin{equation*}
		\diff g = - s \diff T + v \diff p
	\end{equation*}
	where $ s = \frac{S}{N}$ and $ v = \frac{V}{N} $ are the entropy and volume per particle, respectively (both are intensive quantities).
	Applying the chemical equilibrium condition $ g_{gas} = g_{liquid}  $ we have
	\begin{equation*}
		g_{gas} = g_{liquid} \implies - s_{liquid} \diff T + v_{liquid} \diff p = - s_{gas} \diff T + v_{gas} \diff p
	\end{equation*}
	Rearranging and solving for $ \dv{p}{T} $ gives the slope of the line of phase transitions in the $ (T, p) $ plane:
	\begin{equation*}
		\dv{p}{T} = \frac{s_{gas} - s_{liquid}}{v_{gas} - v_{liquid}}
	\end{equation*}
	
	\item A quick aside: \textit{specific latent heat} $ L $, defined as
	\begin{equation*}
		L = T(s_{gas} - s_{liquid})
	\end{equation*}
	is a convenient quantity measuring the energy released per particle when passing through a phase transition. Latent heat is commonly used when describing phase transitions.
	
	\item In terms of specific latent heat $ L = T(s_{gas} - s_{liquid}) $, the expression for the slope of the phase transition line reads
	\begin{equation*}
		\dv{p}{T} = \frac{L}{T(v_{gas} - v_{liquid})}
	\end{equation*}
	This result is the \textit{Clausius-Clapeyron equation}. It tells use that the slope of the phase transition line in the $ (T, p) $ plane is determined by the ratio of specific latent heat released in the transition to the change in volume.
	
	\item We can easily find an approximate solution to the Clausius-Clapeyron equation with a few simplifications. We assume:
	\begin{itemize}
		\item The latent heat $ L $ is constant throughout the transition
		\item $ v_{gas} \gg v_{liquid} \implies v_{gas} - v_{liquid} \approx v_{gas}$ (a safe approximation for many substances; e.g. for water the error is $ \sim 0.1 \% $)
		\item We assume the gas obeys the ideal gas equation $ pv = k_{B}T $
	\end{itemize}
	Using the ideal gas law to solve for $ v_{gas} $, the equation reduces to
	\begin{equation*}
		\dv{p}{T} = \frac{L}{Tv_{gas}} = \frac{Lp}{k_{B}T^{2}} \implies p(T) = p_{0} e^{-L/k_{B}T}
	\end{equation*}
\end{itemize}

\subsubsection{A Quick Note on Order of Phase Transitions}
\begin{itemize}
	\item The \textit{order} of a phase transition is classified by the continuity of the relevant thermodynamic potentials involved in the phase change. In an $ n $-th order phase transition, the $ n $th derivative of a relevant thermodynamic potential is discontinuous over the transition. For example, in a first-order phase transition the first derivative of a thermodynamic is discontinuous; in a second-order transition the second derivative is discontinuous, etc..
	
	\item The liquid-gas transition releases latent heat which means the entropy $ S = -\pdv{F}{T} $ is discontinuous. Alternatively, we can say the system's volume $ V = \pdv{G}{p} $ is discontinuous in the transition from liquid to gas. In either case, a first derivative of a thermodynamic potential ($ F $ or $ G $) is discontinuous, so the liquid-gas transition is a first-order transition. 
	
	In fact, the Clausius-Clapeyron equation applies to \textit{any} first order transition.
	
	\item Recall from the volume-pressure $ (v, p) $ plane discussion that the wiggle in the van der Waals isotherm (which represents the region of phase transition) only occurs for $ T < T_{c} $ and as we approach $ T_{c} $ the wiggle disappears into a single inflection point. Physically, this means the phase transition disappears as we approach the critical temperature $ T_{c} $. 
	
	In the limit $ T \to T_{c} $, the discontinuity associated with the phase transition diminishes and $ S_{liquid} \to S_{gas} $. Precisely at the critical point there is a second order phase transition (because there is a point of inflection at the critical point), and above the critical point there is no discontinuous transition at all. The liquid and gaseous phases coexist and there is no sharp distinction between the phases.
	
\end{itemize}

\subsubsection{The Law of Corresponding States}
\begin{itemize}
	\item We will continue to focus on the critical point of phase transitions. Previously we derived the critical point of the van der Waals equation by looking for simultaneous solutions to the equations $ \pdv{p}{v} = 0 $ and $ \pdv[2]{p}{v} = 0 $ knowing the maximum and minimum of the characteristic van der Waals wiggle must meet at an inflection point. The values of $ T, v $ and $ p $ at the critical point were
	\begin{equation*}
		k_{B}T_{c} = \frac{8a}{27b} \qquad v_{c} = 3b \qquad p_{c} = \frac{a}{27b^{2}}
	\end{equation*}
	
	\item Next, we define the dimensionless \textit{reduced variables} 
	\begin{equation*}
		\tilde{T} = \frac{T}{T_{c}} \qquad \tilde{v} = \frac{v}{v_{c}} \qquad \tilde{p} = \frac{p}{p_{c}}
	\end{equation*}
	In terms of the reduced variables, the van der Waals equation reads
	\begin{equation*}
		p \tilde{p} = \frac{k_{B}T_{c}}{v_{c}} \frac{3 \tilde{T}}{(3 \tilde{v} -1)} - \frac{3 p_{c}}{\tilde{v}^{2}}
	\end{equation*}
	After dividing through by $ p_{c} $ and plugging the expressions for $ k_{B}T_{c} $, $ v_{c} $ and $ p_{c} $ in terms of $ a $ and $ b $, the parameters cancel and we get
	\begin{equation*}
		\frac{k_{B}T}{p_{c}v_{c}} = \frac{3}{8} \eqtext{and} \tilde{p} = \frac{8 \tilde{T}}{3\tilde{v}-1} - \frac{3}{\tilde{v}^{2}}
	\end{equation*}
	
	\item We have found something interesting here. In terms of the reduced variables the van der Waals equation is free of units and parameters. This means the reduced van der Waals equation \textit{applies universally to all gases}. In the form $ \tilde{p} = \frac{8 \tilde{T}}{3\tilde{v}-1} - \frac{3}{\tilde{v}^{2}} $ the equation is usually called the \textit{law of corresponding states}.
	
	Meanwhile, the ratio $ 	\frac{k_{B}T}{p_{c}v_{c}} = \frac{3}{8}  $ is called the \textit{universal compressibility ratio}; it is a dimensionless combination of the critical parameters that applies to any van der Waals gas. Typical compressibility ratios for real gases fall in the range $ 0.28 $ to $ 0.3 $, so the van der Waals equation doesn't exactly agree with experiment here.
	
	Instead, the lesson is (the brief introduction to) the idea that critical parameters and reduced variables are a powerful way to model phase transitions because they apply \textit{universally} to all systems in a way that standard quantities with units to not.

\end{itemize}


\end{document}






















